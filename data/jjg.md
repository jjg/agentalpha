---
title: 2014
date: Fri, 3 Jan 2014 00:35:57 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
At the end of each year some people try to predict what will happen over the
course of the new year.  I'm less inclined to try and map out what the rest of
the world will do in 2014, but I'll take a stab at what I expect to do myself:

  

  * Finish writing my Reprap book   

  * Complete the third version of the Murfie API   

  * Release major-version-number version updates of the Murfie mobile apps   

  * Add support for Murfie to a few more streaming playback devices   

  * Build a prototype Recyclotron   

  * Explore creating a maker/hackerspace with Jamie   

  * More time camping   

  * More time hacking on off-the-grid living at Kamp Kratz   

  * Teaching Libby to hack her Chromebook   

  * Developing collaborative CAD tools & environments   

  * Experiments with me2 and other exocortex-related development projects   

  * Home repairs   

  * More time paddling   

  * Some time making music   

  * Return to experimental filmmaking   

  * 3D Printing and Desktop Manufacturing teaching and ambassadorship   

  * Experiments in the design and construction of electric vehicles 
  * Release a compelling electronic publishing platform 

  

That's a pretty good sample of what seems likely to happen.  Of course theres
much more, and some of these may drop off, but at the moment they all seem
like good candidates to get some attention this year.

  

Can't wait to see what you do with the new year!

  

  

\- Jason

  

---
title: 3dna mentioned in IT World
date: Wed, 14 Jan 2015 06:33:25 -0600
author: jjg
draft: false
tags:
  - preposterous
---
My [ 3dna ](https://github.com/jjg/3dna) project ( [
https://github.com/jjg/3dna ](https://github.com/jjg/3dna) ) mentioned in this
[ IT World article ](http://www.itworld.com/article/2695826/big-data/where-to-
open-source-your-dna.html) about how best to open-source your genetic data:

[ http://www.itworld.com/article/2695826/big-data/where-to-open-source-your-
dna.html ](http://www.itworld.com/article/2695826/big-data/where-to-open-
source-your-dna.html)

This was a cool read because I wasn't aware that there are people seeking
developers to come up with cool applications for this data.  When I wrote 3dna
I had hoped to encourage people to think about how they could explore their
genetic map in new and interesting ways, and it's encouraging to know that
there's other people out there thinking about the same thing (and even seeking
help doing so on Github).

![](/preposterous/https://github.com/jjg/3dna/raw/master/examples/photos/jjg.JPG)

If I can find the time I may follow-up on some of the links from the article
and see what I can contribute to those efforts.

\- Jason

---
title: 3D Printing Bed Adhesion Solution
date: Fri, 07 Aug 2015 12:12:03 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I've been printing for a few years with a Reprap I've built and one of the
ongoing struggles has been keeping parts stuck to the bed long enough to
print.  I've tried many, many things, but along with proper tram adjustment,
nothing has worked as well as a thick piece of glass coated with Elmer's Craft
Bond (the specific flavor in the photo below):

![bedglue.jpg](/preposterous/assets/45-bedglue.jpg)

I have three pieces of glass cut for printing (not sure the exact thickness
but it's pretty heavy glass) which I spray with a light coating of this
adhesive and it is the perfect balance between enough stick to keep parts in
place but not so much it's impossible to remove them.

One application lasts a very long time, in fact I can't say for sure that I've
ever needed to reapply it (I have in some cases just to be cautious).  Over
time the surface becomes less sticky to the touch, but keeps working just as
well (perhaps a function of the heat coming from the plastic during printing).

It also creates a little roughness to the surface of the glass that might play
a role as well.

Using this setup I've been printing PLA on unheated glass for a number of
months with only one or two failed prints due to adhesion issues, and in all
cases I was pushing my luck (tall parts or circular overhangs without brim,
etc.).  I've also been able to use the entire printable area of my bed for the
first time without curling, etc.

I'm not sure how well this works with other plastics because I haven't tried
many of them yet.  I did do a little printing with [ Talman t-glase
](http://taulman3d.com/t-glase-features.html) and it stayed put as well.

The best part is that both of these things can be had at most local art/craft
supply stores.  The adhesive is relatively inexpensive and goes a long way (be
sure to follow the directions on clearing the nozzle after use!) and custom
cut glass can be had from anywhere that makes picture frames (ask them to
grind the edges if possible, much safer when handling the glass).

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Adafruit Blog
date: Fri, 20 Dec 2013 13:50:05 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Preposterous got a mention on the [ #piday
](http://www.adafruit.com/blog/category/raspberry-pi/) episode of the lovely [
Adafruit blog ](http://www.adafruit.com/blog/) today.

  

[ http://www.adafruit.com/blog/2013/12/20/raspberry_pi-piday-raspberrypi-84/
](http://www.adafruit.com/blog/2013/12/20/raspberry_pi-piday-raspberrypi-84/)

  

We can always use the encouragement, thanks [ Adafruit
](http://www.adafruit.com) !

  

  

\- Jason

---
title: A dog must be the main character
date: Thu, 26 Jun 2014 20:16:22 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---


---
title: Ajax index test
date: Sun, 15 Dec 2013 12:45:50 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Let's test what happens when we make a new post using the ajax index stuff...

---
title: A "Killer App" for Transcranial Magnetic Stimulation
date: Sat, 31 Jan 2015 08:09:31 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Transcranial Magnetic Stimulation  is the process of inducing an electrical
current in the brain using [ electromagnetic induction
](https://en.wikipedia.org/wiki/Electromagnetic_induction) .  This works using
the same mechanism that creates a current of electricity in one wire when it
is placed near another wire passing alternating current, which is used in
electrical transformers, AC electric motors, inductive cooktops, etc.

What's exciting about this technology is that it allows you to stimulate areas
of the brain in ways that previously required cutting holes in the skull.
There have been many interesting experiments where electrodes are implanted
directly ( [ cortical implants
](https://en.wikipedia.org/wiki/Cortical_implants) ) into the brain which can
then be used to trigger emotional reactions or even transmit information.
Many people think that TMS may allow us to achieve the same results without
surgery, which makes the whole thing a lot more practical.  It also makes it
more feasible for experimentation outside of the medical field.

![X-ray of early implanted
electrodes](/preposterous/http://www.pages.drexel.edu/~dh329/bmes212/Images/implants/pic03.jpg)

_X-ray of early experimental implanted electrodes (more info at[
http://www.pages.drexel.edu/~dh329/bmes212/cortical.html
](http://www.pages.drexel.edu/~dh329/bmes212/cortical.html) ) _

While the idea of sending images directly to the brain is enticing, and
certainly something we want to explore, doing so with TMS is complicated
because experiments that have done this so far with traditional electrodes
have involved arrays of individual probes arranged in grids (similar to pixels
on the screen) and this sort of array is difficult to achieve at with TMS (at
least so far).  Other senses may be simpler to trigger, but we don't know as
much about simulating them because traditionally computer simulations have
only stimulated two human senses: vision and sound.

I've been listening to a lot of music on my [ iPod
](http://blog.murfie.com/2014/12/18/opinion-save-the-ipod/) lately and have
become more aware of how uncomfortable headphones, earbuds, etc. can get after
several hours of wearing them.  I've also noticed that there are distinct
"gaps" forming the frequency range of my hearing (I'm sure it has nothing to
do with practicing Metallica songs at concert volume in [ Derek
](https://www.facebook.com/derek.schyvinck?fref=ts) 's basement).  This got me
thinking about ways to reduce the physical components of listening hardware as
well as ways to compensate for partial hearing loss, and then a light went on.
Audio reaches our ears in waves, and while creating arrays of stimulation with
TMS is hard, transmitting a wave to a particular spot of the brain is much
simpler.  Perhaps TMS could be used to transmit audio directly to the [
auditory cortex ](https://en.wikipedia.org/wiki/Auditory_cortex) of the brain?

If so, there's a few immediately interesting applications:

  * Overcoming hearing loss that is due to damage of the ear 
  * Listening to music and other audio with less intrusive physical apparatus 
  * Hearing sounds that are beyond the range capable with human ears 

The reason I think this is a "killer app" is that it has the traits of being
achievable within the limits of imaginable technology, and the work to carry
out this application will uncover information necessary for additional and
more ambitious future projects.  Especially when you consider the market for
high-end headphones, it's not hard to imagine a device like this with a cost
that might seem reasonable in this range.

Like anything else that deals directly with the brain, the process of testing
and proving whether or not this is possible isn't simple, but a path for the
experiment is conceivable, and the cost of the equipment necessary is rather
low in comparison to the potential value of any application imaginable.  I
plan to carry out some of the research and experiments myself, but if you find
this interesting and would like to participate, or have something you'd like
to contribute to the work, please let me know.

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: AlphaSmart
date: Sun, 2 Aug 2015 06:53:35 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Pieter mentioned an interesting device called **AlphaSmart** ( [
https://en.m.wikipedia.org/wiki/AlphaSmart
](https://en.m.wikipedia.org/wiki/AlphaSmart) ) on Twitter the other day.
It's sort of an electronic typewriter/typing trainer and was apparently
popular in the education market.

  

![image1.JPG](/preposterous/assets/40-image1.jpg)  

  

It's a neat device.  Small, rugged and with a long battery life, it would make
a nice tool for writing in the field.  

  

Among other things it had an interesting "synchronization" mode where it could
be attached to a computer via the keyboard port and then dump the contents of
its memory into an editor on the computer as if it were a person typing the
data in.

  

This seems like something that could be recreated using an Arduino, so I'm
noodling on this and collecting some of the pertinent information here for
reference.

  

* This board may be able to do most of the heavy lifting (character LCD with integrated AVR processor): [ https://www.sparkfun.com/products/10097 ](https://www.sparkfun.com/products/10097)

* Arduino library to act as a keyboard: [ https://www.arduino.cc/en/Reference/MouseKeyboard ](https://www.arduino.cc/en/Reference/MouseKeyboard)

  

  

// jjg

---
title: "A New Analog Audio Format"
date: 2019-05-05T13:50:34Z
draft: false 
tags:
  - music
---

Let’s start by agreeing that analog formats like vinyl do a better job of capturing music than digital formats.

That said digital (and especially electronic digital) formats offer a lot of features and convinience to both the listener and the artist.  You can't buy a car stereo today that can play records, and while you *can* make your own records at home, [it’s pretty hard](https://thevinylfactory.com/features/a-beginners-guide-to-lathe-cutting-your-own-records/).  


Technology has come a long way since the LP was “perfected”.  Could there be a new way of producing analog recordings that are as good or better than vinyl?  Or if not *better*, at least more convinient to listen to and easier to make at home if we apply technology that would have been impractical back when vinyl LP’s set the standard?


I had a faint memory that video [LaserDisc](https://en.wikipedia.org/wiki/LaserDisc)s were analog, so I started my research there.  As it turns out this is true, and while eventually LaserDisc added digital audio tracks, the video tracks were always analog.  Physically the format is similar to [Compact Disc](https://en.wikipedia.org/wiki/Compact_disc) (CD), but how the signal is encoded on the disc is different.


*“However, while the encoding is of a binary nature, the information is encoded as analog pulse-width modulation with a 50% duty cycle, where the information is contained in the lengths and spacing of the pits. In true digital media the pits, or their edges, directly represent 1s and 0s of a binary digital information stream”*

 - [https://en.m.wikipedia.org/wiki/LaserDisc](https://en.m.wikipedia.org/wiki/LaserDisc)


Further reading revealed that something akin to this may already have been done.  The [Super Audio Compact Disc](https://en.wikipedia.org/wiki/Super_Audio_CD) (SACD) used [pulse-density modulation](https://en.wikipedia.org/wiki/Pulse-density_modulation) to record audio on an optical disc.  They don't call this *analog* however, and use a [1-bit analog-to-digital converter](https://en.wikipedia.org/wiki/1-bit_DAC) to turn the analog signal into a [bitstream](https://en.wikipedia.org/wiki/Bitstream) that is then written to the disc.

Another related technology is the development of the [Class-D amplifier](https://en.wikipedia.org/wiki/Class-D_amplifier).  These amplifiers use a binary electrical signal (essentially a [switching power supply](https://en.wikipedia.org/wiki/Switched-mode_power_supply)) to drive a speaker which, due to its mechanical nature, converts the digital signal to analog automatically.

The SACD format isn't very common now, like most physical media formats (I'm not sure if it was ever possible to create your own SACD's at home).  Even so it's encouraging to know that it's at least possible to use these techniques to create an optical audio disc.

This makes me wonder: could a typical CD burner be modified to record an *analog* audio using an encoding similar to LaserDisc on standard CD-R media?

I don't think this can be done with software alone.  Most CD drives use a [ATAPI](https://en.wikipedia.org/wiki/ATA_Packet_Interface) or [SCSI](https://en.wikipedia.org/wiki/SCSI) interface which [AFAIK](https://en.wikipedia.org/wiki/List_of_acronyms:_A#AF) doesn't provide low-level commands necissary to control how and when the laser etches pits into the disc. It might be possible to modify the [firmware](https://en.wikipedia.org/wiki/Firmware) of a CD-ROM drive to accomplish this, but I'm not currently aware of any way to do it.

I have however sucessfully driven CD-ROM transports using [g-code](https://en.wikipedia.org/wiki/G-code).  In fact the [first CNC machine I built](https://www.youtube.com/watch?v=VNuxN39d9xs) was a laser engraver made out of two CD-ROM drives.  This is a pretty brute-force way to go about it, and it's not the most "repeatable" (as it involves a lot of cutting and soldering) but it might be the fastest way for me to begin experimenting and developing this format.  Once I have a proof-of-concept, it might worth the effort to find a drive that can be more easily modified to support the format and make the whole system more accessible.

 

## References

* https://web.archive.org/web/20080513083110/http://www.access-one.com/rjn/laser/legacy/ld96.html
* https://www.allaboutcircuits.com/textbook/digital/chpt-13/delta-sigma-adc/
* https://en.m.wikipedia.org/wiki/Pulse-density_modulation
* https://en.m.wikipedia.org/wiki/Super_Audio_CD

---
title: A Page-Oriented UI
date: Sat, 01 Aug 2015 20:50:40 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Since the first GUI's scrolling has been a stable of the genre.  Today it is a
given that a graphical interface will involve scrolling windows of content, as
if there is an inherent value in displaying information this way.  In reality,
scrolling was a compromise driven by the limits of graphical displays in the
1980's.  These limitations has long since been overcome, and now even
inexpensive screens are capable of displaying full print-quality pages of
information.  This opens up the possibility of returning the graphical
interface to one that is page-oriented, and the elimination of scrolling.

If you're already questioning the feasibility of this consider the fact that
until about 1984 all information was conveyed via page-oriented interfaces,
notably books and periodicals.  This was not only sufficient, but centuries of
refinement had reduced the process to a science, and even today many web
designers attempt to emulate the practices of the printmakers of old.  By
eliminating scrolling views and applying traditional page layout techniques we
can leverage this knowledge to produce not only higher-quality information,
but a more pleasant experience as well.

For example, long lists are well known to be the bane of performance-oriented
interfaces.  Interestingly enough this has been addressed by providing a
"paging" interface (you'll still see this in some places where search results
have a "next page" button at the bottom).  However, these "pages" are often
presented inside a scrolling container.  This is logical only from the
programmer's perspective, who is trying to relieve stress on the back-end
system by only returning a subset of the information at a time, but oblivious
to how this information is rendered to the reader.  A better system would
produce results in pages whose contents could be displayed in full, therefore
taking advantage of the human capacity to take in information a whole page at
a time, and eliminating the unnecessary interaction (and associated processing
overhead) of scrolling the page around.

If you don't think these are real problems consider the currently trendy
"infinite scroll" which is replacing these "paginated scrolls" in list
results.  This technique eliminates the pagination by creating the illusion of
an infinitely long scrolling page.  Aside from the fact that most
implementations fail to preserve the illusion (often displaying a "loading"
indicator if you scroll too fast), they create a new problem of orienting
yourself within the list, and in particular quickly navigating the space.  The
result is the introduction of "back to top" buttons and the like that are
essentially admitting "yeah, we know we broke the scrollbar, but we don't know
what to do about it".

There are of course computer applications that don't lend themselves to a
page-oriented interface.  Many applications that did not exist before
graphical computers (applications that were "born into" the digital world)
have canonical interface elements that define the application as much as the
other way around.  However the are many other applications which emulate
information systems that existed before graphical computers that would benefit
from this approach, and not just old fashioned things like "desktop
publishing", but the web itself.

Remember that the web was born as a document publishing system, and all
cleverness aside, one of it's most valuable applications is providing a
distributed, networked and indexed store of textural and visual information.
The web could become a significantly more useful, efficient and pleasant means
of conveying information if presented in a page-oriented interface.

This may seem like a lot of abstract ideas and hand-waving and I accept that.
I do plan to write more on the subject and provide more concrete examples that
may make the benefits of this approach more clear.  In the meantime, imagine
how systems might be different if you were to simply make a rule that
eliminated any sort of scrolling.  Imagine not only how existing applications
would change, but how new applications might be designed under these
constraints.

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: A peek at my next Pebble project
date: Fri, 9 Jan 2015 17:39:01 -0600
author: jjg
draft: false
tags:
  - preposterous
---
\- Jason

[ ![](/preposterous/assets/18-image1.jpg) ](assets/18-image1.jpg)

---
title: A Post a Day
date: Sun, 2 Aug 2015 05:52:06 -0500
author: jjg
draft: false
tags:
  - preposterous
---
As an experiment in motivation (and perhaps gumption generation) I'm going to
try to post here at least once a day for the month of August. I'm not
promising anything epic (although it's hard to write a little) but each day
I'll be posting something in an effort to make Preposter.us part of my daily
thoughts. This should make it clear to me if Preposter.us is valuable enough
to invest more time sanding down the edges, or of the time has come to work on
its successor. \- Jason

---
title: A potential new heart for OFFGRiD
date: Fri, 28 Aug 2015 16:03:01 -0500
author: jjg
draft: false
tags:
  - preposterous
---
As I've mentioned I've been playing with the [ Mixtile GENA
](https://www.tindie.com/products/Mixtile/mixtile-gena/) , a neat little
device that's a lot like the heart of a Pebble watch.  They've been enjoying
my work with the device and have offered to send me another one of their
products to experiment, the [ LOFT-Q ](https://www.tindie.com/products/Mixtile
/mixtile-loft-q/) .

I hadn't heard of the LOFT-Q before but checking it out, it looks like
potential candidate for the brains of [ OFFGRiD
](http://wiki.2soc.net/doku.php?id=offgrid) .  There's a bit more to it in
terms of size and cost than the other SBC's I've been considering, but it
packs a lot of things in that I'd like to have inside the OFFGRiD.

The question will be whether or not it can operate in the constraints I've
imposed on the laptop, in particular the physical size and power consumption.
On the other hand there is a possibility that it might allow me to eliminate
some of the hardware I'd otherwise need for the other SBC's I've been
considering, and that would be a big win.

If you're looking for something a bit beefier than a Raspberry Pi or and
Odroid the LOFT-Q is worth a look.  I can't vouch for it's quality yet, but if
it's anything like the GENA I'm sure it will be well made.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Archives
date: Sun, 20 Sep 2015 09:42:51 -0500
author: jjg
draft: false
tags:
  - preposterous
---
This morning I was digging through my Evernote archive to find something I
originally posted on gullicksonlaboratories.com about an old iOS project, [
DashApp ](https://github.com/jjg/dashapp) .

A few years ago, when I decided to move off Wordpress I wrote a [ script
](https://github.com/jjg/wp2evernote) that could import a Wordpress site into
Evernote.  The original idea was to then use a service called postach.io (the
site is gone, but I found some repositories: [ https://github.com/postachio
](https://github.com/postachio) ) to publish the blog, straight out of
Evernote, which sounded pretty sweet at the time.  However it took postach.io
too long to get things to a usable state and I decided to change course.  That
was probably around the time I started experimenting with what ultimately
became preposter.us.

Anyway, I thought I had a nice archive of everything from
gullicksonlaboratories.com in Evernote, but it seems like some things are
missing.  Furthermore, I don't think I preserved the post dates correctly (at
least it doesn't look that way in the Evernote UI) and that's a bit of a
problem.

You see, even more years back I scoured the Internet Archive for my earliest
"blog" posts (from a time before the word blog was really a word).  I found
many of these and carefully imported them into the Wordpress database and
correctly back-dated them to their original post date.  This made
gullicksonlaboratories.com a single repository of almost everything I ever
published on the web, which was pretty cool (and at times, embarrassing).

Eventually I'd like to get this content back on-line again, but if I torched
the dates when pulling them in to Evernote, I'll have to go back to the old
Wordpress database again (assuming it still exists).  The other problem with
bringing that stuff back is where to do it.  Preposter.us isn't really
designed to "fake" publishing dates; it could be done (I think), but it would
be a lot of screwing around.  In light of that, I might just come up with some
kind of "museum" for this content, since anyone coming across it should be
warned that it's far from fresh.

All this makes me think about how we should approach our information legacy in
a time where there will be an entire generation whose personal history is
likely to exist in only electronic formats, and stored on systems they have no
control over.  The world today might be a very different place if we were not
able to discover the unpublished writings and correspondence of our ancestors,
and that is a very real possibility for the near future.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Arc Reactor Mark I
date: Fri, 10 Jan 2014 08:11:10 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/180-photo.jpg) ](assets/180-photo.jpg)

---
title: A standard for quality in consumer goods
date: Thu, 26 Dec 2013 09:15:48 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Henry Ford said of the Model T:

  

_"I will build a car for the great multitude. It will be large enough for the
family, but small enough for the individual to run and care for. It will be
constructed of the best materials, by the best men to be hired, after the
simplest designs that modern engineering can devise. But it will be so low in
price that no man making a good salary will be unable to own one – and enjoy
with his family the blessing of hours of pleasure in God's great open
spaces."_

  

A good baseline to hold any consumer product to (adjusted for cultural
evolution of course).  It would be an interesting experiment to envision a
world where the merchantability and regulation of consumer goods were based on
statements like these as opposed to the primatial and easily circumvented rule
languages used by American law.  Let's try re-writing Ford's words updated for
a modern mobile phone company:

  

_"I will build a phone for everyone. It will be flexible enough for the
family, but small enough for the individual to use and care for. It will be
constructed of the best materials, by the best people to be hired, after the
simplest designs that modern engineering can devise. But it will be so low in
price that no one making a good salary will be unable to own one – and enjoy
with their family the blessing of hours of pleasure in communicating with the
world."_

  

Have you ever seen a phone that lives up to that standard? I haven't.  If you
think you have, remember that a phone isn't a phone without a network, so be
sure to apply the policies and behavior of a wireless provider to your
evaluation.  Also remember that everyone who bought a Model T owned it, so
when considering "so low in price" that you're looking at the unlocked, no-
contract version of the phone.

  

This is fun, let's try it with something else; how about a 3D printer?

  

_"I will build a printer for the student, the teacher and the stay-at-home
tweaker; a printer for anyone who has something to build. It will have the
build and materials capacity to print anything that could be needed by the
average family, but small enough for the individual to operate and care for.
It will be constructed of the best materials, by the best people to be hired,
after the simplest designs that modern engineering can devise. But it will be
so low in price that no one making a good salary will be unable to own one –
and enjoy with their family the blessing of hours of pleasure in bringing new
things into the world."_

  

It's interesting to see what changes and what stays the same.  In particular
the idea of using good materials and labor were important to Ford and those
values hold up today as well.  Interestingly enough, these are often the first
thing to be compromised in modern products.

  

The other thing that stands out to me is the family.  In modern terms I think
this can be expanded beyond the biological family (that's another blog post)
but I think the important thing to take away from this is the idea that a good
product is good not only for the individual, but for those around them as
well.  This is another area many modern products fail to address or overlook
altogether.

  

These two examples are focused on technology only because that is my area of
expertise; I encourage you to carry out the same experience with products you
know about and share the results.

  

  

\- Jason

---
title: Audio test 1
date: Sat, 14 Dec 2013 13:49:22 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
First test of audio embedding.

  

  

---
title: Audio test 2
date: Sat, 14 Dec 2013 13:51:01 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Second test of audio attachments.

  

  

---
title: Authoring
date: Sat, 15 Aug 2015 12:27:59 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Planned on spending an hour incorporating Jamie's edits into the Reprap book,
ended up more like four.  Writing is hard, and easy to underestimate.

I think there are shortcuts, and that's why bad writing exists.  I'm not
trying to imply that what we're working on will be perfect, but I can see how
under normal "business" conditions compromises would get made that would
result in a book not living up to its potential.

I'm also not saying it's easy to do the best job you can possibly do.  There
is constant temptation to just finish the thing, and that the result of doing
so might be good enough.  There's also the real fear that not finishing could
be worse than releasing something you know isn't as good as it could be,
because the pursuit of perfection has no time limit.

I don't think we're in danger of that yet.  3D printing is still a very fast-
moving target, but I've tried hard to strike a balance between abstract and
concrete content that will be useful and practical but not bolted to firmly to
contemporary tech.  We'll see, I could be wrong, but I feel good about where
this is and where it's going.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Automated processing issues test 1
date: Sun, 15 Dec 2013 22:47:00 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Having some trouble on the server, this is test 1.

---
title: automated processing test 2
date: Sun, 15 Dec 2013 23:17:27 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Think I got this now, but let's  test a second time.

---
title: Autonomous research agent
date: Sun, 22 Dec 2013 10:16:45 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Something I'd like to have is an agent that I can submit ideas to that will
run around and dig up relevant information, similar ideas and conversations
that might shed light on pursuing the work.  Sort of like a compiler or linker
for new ideas or projects.

  

Today I'm going to spend some time (ironically) researching if such a thing
already exists, or could be cobbled together using existing search API's, etc.

  

  

\- Jason

---
title: A Week Without Records
date: Sun, 17 May 2015 10:23:06 -0500
author: jjg
draft: false
tags:
  - preposterous
---
_Note: This is a re-post revived from way back in 2011, re-posted here because
the original is no longer accessible. AWWR 2015 will be held June 15 - 21. -
jjg_

"A  Week  Without  Records  " is an exercise in musical appreciation. The idea
is simple, for one  week  avoid listening to mechanically-reproduced music.
The benefits to you are twofold:

  
First off you will be encouraged to seek out non-mechanical sources of music
to listen to. The most obvious source of this is live performances of
professional musicians, however you may find other sources of music as well in
street performances, friends and family who sing or play an instrument or
perhaps you will be motivated to learn to make music of your own?

  
The second benefit you may experience is a heightened satisfaction with the
music you are able to experience. By making music "scarce", it is easy to
savor the music you have and encourage listening deeply instead of passively.
I believe that this "mindful" listening will allow you to see the quality in
forms of music you may otherwise have overlooked (and perhaps see the opposite
in music that is easily available otherwise).

  
AWWR 2011 will be observed July 3rd - July 9th. This  week  was selected to
coincide with Independence Day celebrations in the United States to increase
the likelihood of live music being available in most US communities (not that
AWWR is US-only, but that is the area I am most familiar with). Also there
just seems to be more live music in the summer :)

  
Background:  
The idea for AWWR first came to me while spending a day at Old World
Wisconsin. In this re-creation of 1800's America, the prevailing soundtrack is
one of nature and some human voice. In the afternoon we happened across a
musician playing an instrument on the porch of a shop and the music was as
sweet as any I had ever heard. Even thought the style of the music was outside
of my normal "taste", the combination of the context in which it was presented
and in the void of musical sound leading up to the experience, it allowed me
to hear the music in a deeper way, seeing past genre and style and feeling
only the quality of the musicians work and the instrument's creators
craftsmanship.

  
I originally set out to create AWWR with detailed descriptions and complexity
in 2010 and never completed the project. So instead of all that I'm starting
it in 2011 with this simple post and if others find the idea enriching we can
work together to make the experience more accessible to others in the
following years.

  
If you'd like to discuss AWWR join me on Twitter (I'm @jasonbot2000) or maybe
we'll run into each other at a concert next month. If you choose to
participate I'd love to hear about your experiences!

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: "Back to the Future"
date: 2019-10-10T16:03:31Z
draft: false 
---

I've found that my writing has suffered since switching to Hugo.  I spent a lot of time reflecting on this, and realized that the most productive tool for me was one of my own design, [Preposter.us](https://codeberg.org/jjg/preposter.us).

That being the case I've resumed work on that project, and revived the domain as well.  All future posts will be made to my Preposter.us blog, located at:

[https://preposter.us/single-michigan-helium-network/](https://preposter.us/single-michigan-helium-network/)

Eventually that blog will reside here, at [jasongullickson.com](https://jasongullickson.com/), but there is work yet to do around using custom domains with Preposter.us and I don't want that work to impede the writing, so I'm just going to leave this little breadcrumb here for now.

I'm also going to need to find a reasonable way to host [the archive](https://jasongullickson.com/about/) of posts that currently live here.  I have a few ideas but that another thing I'll need to figure out before redirecting this domain name.

Looking forward to getting back in the groove, catch you on the flip-side!
---
title: Barcamp Fond du Lac 2014
date: Sat, 20 Sep 2014 05:10:24 -0700 (PDT)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Today: at Barcamp

Tomorrow: post about Barcamp

—  
Sent from [ Mailbox ](https://www.dropbox.com/mailbox)

---
title: Behind the scenes of Preposterous's development
date: Sun, 15 Dec 2013 08:23:39 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
  
  
\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)  

[ ![](/preposterous/assets/75-image.jpg) ](assets/75-image.jpg)

---
title: Better Fox
date: Sun, 29 Dec 2013 17:41:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Took a few tries, but I think this might be the best print I've done yet:

[ ![](/preposterous/assets/170-image.jpeg) ](assets/170-image.jpeg)

---
title: Boogie Board Sync first impressions
date: Thu, 26 Dec 2013 16:36:40 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
\- Jason

[ ![](/preposterous/assets/157-bb_00016.pdf) ](assets/157-bb_00016.pdf)

---
title: Brain jar
date: Sun, 15 Dec 2013 23:54:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Sent with my thumbs

[ ![](/preposterous/assets/94-photo.jpg) ](assets/94-photo.jpg)

---
title: Building an analog network monitor with WiLDFiRE & RESTDuino
date: Sun, 31 Aug 2014 11:21:38 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
  

  

If all is well we can move on to the software side of the project.  
  

To display network utilization on the gauge we need to measure it somehow.
There are a number of ways to do this but the most universal is to use a
protocol called SNMP (Simple Network Management Protocol).  SNMP provides a
lot of information, and it's supported by most network devices, so it's a good
tool for this job, even if it's a little tricky to use (dispite its name).
Fortunately there's a python library called pysnmp that works well and is
fairly easy to work with.  
  

The hardest part about getting the bandwidth utilization via SNMP is figuring
out what to monitor.  SNMP uses something called an "OID" to identify each
parameter that can be monitored and determining the OID for the interface
(port) on your router that you want to monitor can be challenging depending on
how well documented the router is.  In my case I'm using an Apple Airport
Extreme, and by poking around on the web a bit and using a tool called
snmpwalk I was able to get a list of interfaces out of the router. From there
it was a matter of figuring out which interface I wanted to monitor, and which
counter of the interface would tell me what I wanted to know (the inbound data
rate).  I could write a few paragraphs about this but I'll leave that as an
exercise to the reader and get back to the fun part for now.  
  

Once you know the OID of the counter you want to monitor you'll find that you
can't simply read it and display that value on the gauge and see how much
bandwidth you're using.  SNMP counters start at zero and then go up until they
reach their maximum value and then reset to zero again, so in order to
determine how much bandwidth is being used at any moment in time it's
necissary to take two readings and determine the difference between the two,
which is the number you want to display on the gauge.  
  

I'm used to thinking in terms of bits per second so my code takes a reading
once every second and subtracts the value read in the previous second from the
current one.  An additional kink is that it sometimes takes longer than a
second to send data to RESTDuino, so I collect 10 samples and average them
before sending them over to RESTDuino for display on the gauge.  
  

With this little bit of python we can now measure the utilization of the
network interface and display it on the analog gauge.  Of course we could
measure any number of things and display them on the gauge in a simular
fashion, or even add another gauge or other indicators (LED's, buzzers, etc.)
by connecting additional WiLDFiRE pins and sending additional HTTP requests
from the python script (I'm planning to do just this to provide a red light
indicating that utilization has crossed into the "danger zone").  
  

This is just one example of how the WiLDFiRE board running RESTDuino makes it
easy to create physical things that interact with the digital world.  It's
certainly possible to do the same thing with other components, but by exposing
Arduino functionality via a REST interface it's possible to build Arduino-
based devices with no Arduino programming, and since the WiLDFiRE board
integrates wireless networking and pre-installed RESTDuino firmware, the
complexity of selecting an Arduino board, network shield and configuring them
is eliminated, allowing you to focus on the software part of the project.  

  

The python code for the software side of this project can be found [ here
](https://github.com/jjg/RESTduino/blob/master/examples/python/monitor.py) in
the [ https://github.com/jjg/RESTduino ](https://github.com/jjg/RESTduino)
RESTduino repository.  

[ ![](/preposterous/assets/265-image.jpeg) ](assets/265-image.jpeg)

[ ![](/preposterous/assets/265-image.jpeg) ](assets/265-image.jpeg)

---
title: Burn-down Month
date: Wed, 28 Jan 2015 08:06:05 -0600
author: jjg
draft: false
tags:
  - preposterous
---
February is Burn-down Month!

Lately I've been feeling like I may have too much work in-flight.  To address
this, I'm going to try to go an entire month with no new ideas, and instead
attempt to clear-out a number of projects that are taking up space in my head.
I'm not going to pretend that I won't come up with new ideas during this
month, but like meditation I'll acknowledge them and then put them away.

So for the month of February, 2015 I'll be working on "burning down" this list
of projects, getting each to a state where it doesn't demand my attention on a
daily basis:

  * JSOS (includes media processing network, etc.) 
  * WebVR support for OpenDive 
  * SceneVR 
  * Recyclotron 
  * Chromebook for Developers 
  * NodeLua/ESP8266 
  * Wrist Reader 
  * 2soc.net 
  * Mofos 
  * The RPM Challenge 
  * Botter 
  * Tantillus 
  * Some machine intelligence stuff 
  * Some quantum computing stuff 
  * A dozen other things that are not coming to mind 

Not all of these are my own projects but they are all things that are
occupying my mind at the moment.  The plan is to finish the work where I have
work to do and to commit the rest to more durable memory (expect an increase
in blog posts next month).

Like everything else I do, this is an experiment, and the outcome may not
match the ambitions set forth at the beginning of the project.  Nevertheless
it will be a learning experience, and I intend to document the process along
the way.

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Byword test
date: Sat, 10 Oct 2015 11:04:09 -0500
author: jjg
draft: false
tags:
  - preposterous
---
#  byword test

##  Why byword?

Trying out a markdown editor as an input source for [ Preposter.us
](http://preposter.us)

  
  
// jjg

---
title: Chromebook for Developers
date: Fri, 2 May 2014 17:43:53 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Thirteen days ago I switched from a loaded Macbook Air to an [ HP Chromebook
11
](http://www.amazon.com/gp/product/B00FJXVRM8/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00FJXVRM8&linkCode=as2&tag=jasontheprodu-20)
.

  

Admittedly, software developers are not the target market for Google's
Chromebook and Chrome OS.  Most of the developers I know believe that their
work requires the most powerful computer they can afford.  I was once in this
camp as well, and when I purchased my previous machine (even though it was
under-powered in comparison to larger notebooks) I made sure that I "optioned
it up" so I wouldn't find myself suddenly trapped without enough processing
power to do my daily work.  As it would turn out, it wasn't the limits of the
hardware that held me back, but those of the software.

  

I've been a Unix and Linux user for years, and switched from Thinkpads running
Redhat to Macbooks when Apple switched to using the NeXT-based OSX, having
been a long-time fan of NeXT STEP, but over the last few years I've decided to
make a conscious effort to use only open technologies, and after multiple
failed attempts at getting the MBA to run Linux well, I decided to look for an
alternative.

  

I won't go into details about the search, but suffice it to say in the end the
machine that fit the most of my needs while being great at running Linux
turned out to be the surprisingly nice and inexpensive Chromebook 11.

  

I'll cut to the chase and say that after having used my daughter's 11 for a
few hours (as we attempted to make it run Minecraft), the only concern I had
about making one my "daily driver" was the 16GB of internal storage.  All my
other needs were met, including a good keyboard (as laptop keyboards go), good
battery life, a nice display and a good balance between screen size and
portability.  On-board processing power seemed adequate under Chrome OS and
even better under Linux, and in every other way that matters to me the machine
performs as well (or better) than the Macbook Air.

  

What genuinely surprised me was just how well it runs Linux, and how easy it
was to get it setup thanks to David Schnider's [ Crouton
](https://github.com/dnschneid/crouton) .  Following the instructions on the
Crouton Github page, you can have your Chromebook running Linux in about 30
minutes (most of which is waiting for things to download and install
themselves).  When you're done, you'll have a nice fast Linux box where all
the hardware works and best of all you can flip almost instantaneously between
Linux and Chrome OS using a keyboard combination.  This lets you use Chrome OS
for the things it does well (web browsing, Google Docs, etc.) and use Linux
for what it does well and keep both environments running side-by-side.  Save
for the fact that you can't cut-and-paste between the two, it's quite a
seamless setup!

  

So in my case almost all of my "productivity" work (email, documents,
drawings, etc.) are done using web-based applications which work great under
Chrome OS.  Additionally I do most of my web debugging and testing using
Chrome so these tasks can be carried out under Chrome OS as well.

  

The Linux side comes in to play when it's time to run code, most of which for
me these days is node.js or Python which easily runs well within the limits of
the Chromebook's hardware.  Additionally other system utilities (network
monitors, etc.) are run under Linux as well as databases (postgresql, redis)
and other server components necessary for my software development projects.

  

The Linux side also gets use running applications that are not available via a
browser, such as media management, file conversion and some 3d-printing
related things (modeling tools, slicing software, etc.), all of which perform
adequately as well.

  

So with all this good news what about the limited file storage?  For now I've
learned to be cognizant of it and work within the constraints of what's
available.  I don't store media files and other types of storage-consuming
things and instead stream the media I need or store it in Google Drive (I
should mention that the Chromebook 11 comes with a bonus 100GB of Google Drive
storage).  That said it's a concern, and I'm planning to test running my Linux
environment on a USB flash drive to give everything a little more room to
breathe.

  

My next post will discuss the process of selecting a device to expand the
Chromebook's storage, migrating an existing Linux chroot environment and an
assessment of the impact this has on usability of the environment.

---
title: Chromebook for Developers (part 2)
date: Sun, 4 May 2014 07:20:49 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Last time I gave a little background about why I chose a [ Chromebook 11
](http://www.amazon.com/gp/product/B00FJXVRM8/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00FJXVRM8&linkCode=as2&tag=jasontheprodu-20\))
to replace my Macbook Air development machine.  In this installment I'll dive
into slightly more technical waters and discuss how some of the limitations
such a modest machine can be addressed.

  

**Storage**

My original post notes that the only limitation of the Chromebook's hardware
that concerned me was the 16GB of internal file storage.  If you are using the
device under Chrome OS exclusively I don't think this will ever be a problem,
but since we're running a full-blown Linux installation alongside Chrome OS it
was something that concerned me.  Fortunately this is easily addressed thanks
to [ Crouton's ](https://github.com/dnschneid/crouton) flexibility and the
availability of tiny, cheap USB flash drives.

  

**Sandisk Cruzer Fit**

Nobody wants a big dongle sticking out of the side of their laptop, and it's
also likely to eventually cost you a USB port (when you inevitably catch it on
something and snap it off), so I knew if I was going to make a USB flash drive
part of the Chromebook's regular config, it would have to be something as low-
profile as possible.  The [ Cruizer Fit
](http://www.amazon.com/gp/product/B00FJRS6QY/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=B00FJRS6QY&linkCode=as2&tag=jasontheprodu-20)
was the smallest I could find, and coincidentally, it's inexpensive as well.

  

![](/preposterous/assets/224-img_0745.jpg)  
​  

  

Like most flash drives this one comes formatted, but if you're going to use it
to run your chroot environment, it needs to be formatted with a filesystem
that can preserve the chroot's permissions (this should be obvious but I found
it out the hard ware regardless).  Unfortunately the Chrome OS GUI doesn't
provide a way to do this but it's pretty easy to do from the command line.

  

_**BFW** \- this could ruin your day, proceed with caution _

  1. open a terminal (ctrl-alt + t)   

  2. type "shell"  <enter>   

  3. type "ls /dev/sd*" <enter> and note the files returned   

  4. insert the USB flash drive and repeat the command above, noting the additional device files (something like "/dev/sda" or "/dev/sdb")   

  5. unmount the flash drive (easiest way is to eject it using the "Files" app)   

  6. type "sudo fdisk <device file>" where <device file> is the flash drive device, then press <enter>   

  7. type "p" <enter> to list the existing partitions   

  8. type "d <partition number>" <enter> to delete each partition in turn (do this for each partition listed in the previous step)   

  9. type "n" <enter> to create a new partition   

  10. press <enter> about 4 more times, accepting the defaults (unless you know what you're doing)   

  11. type "w" <enter> to save the changes to the partition   

  12. back in the shell, type "sudo mkfs.ext3 -L <name for flash drive> <device file>1" <enter> to create a filesystem on the new partition (this may take awhile)   

  13. remove the flash drive and re-insert it   

  

Chrome OS should automatically mount the drive and pop-up the "Files" app.  If
you see a USB drive with the name you chose in step 12 above, you're ready for
the next step.

  

You may want to create a directory on the flash drive to store your chroots
(as opposed to just dumping them in the root of the flash drive, but hey, no
judgement here).  Since this filesystem understands Unix permissions, you
can't use the "Files" app to do this because your regular user doesn't have
permission to modify the flash drives filesystem.  How I dealt with this was
by creating my chroots directory from the shell as such:

  

sudo mkdir /media/removable/usb32/chroots

  

I also created a directory called "files" and modified the permissions so I
*could* put stuff in there using the Files app:

  

sudo mkdir /media/removable/usb32/files

sudo chown chronos /media/removable/usb32/files

sudo chgrp chronos /media/removable/usb32/files

  

  
![](/preposterous/assets/224-screenshot 2014-04-11 at 9.03.51 am.png)  
  

  

If you're not seeing something like the screenshot above, review the steps
above and Google any errors you encountered.  It's important that this goes as
expected otherwise the next steps may not work correctly

  

**Moving the Chroot**

Here again Crouton has anticipated our needs and provides the *edit-chroot*
utility making it easy to move the entire chroot environment with one command.
For example, here's how I moved my Debian wheezy chroot:

  

sudo edit-chroot -m /media/removable/usb32/chroots /usr/local/chroots/wheezy

  

Once moved you'll need to tell Crouton where to find your chroot in order to
start it, which is just a small change from before

  

sudo enter-chroot -c /media/removable/usb32/chroots

  

I noticed it takes awhile to start the chroot after moving it to the flash
drive, but once running performance seems very similar to running from
internal storage.

  

It's worth noting that even though the chroot is now stored externally,
internal storage can still be impacted by chroot activity.  In particular the
system's temp directory will still be stored on the internal flash drive
(maybe you can move this, but I haven't figured it out yet) so things like
apt-get can end up using a lot of internal storage while they're working.
This may be worked-around by using individual apt-get commands to install
components (allowing the temp files to be cleaned out between runs) but I
haven't tested this yet.  If I was really good with apt-get I might know of a
way to redirect these temporary files, perhaps that will be a future post.

  

**Side effects**

There's another advantage to moving the chroot on to USB storage.  When stored
internally, there's no way (that I've found yet) for Chrome OS apps to access
the files in the chroot directly.  However, once stored on USB, the file open
dialog can browse into the contents of the flash drive where the chroot is now
stored and access (and modify) these files.

  

This opens up what can be done from Chrome OS significantly.  For example,
with the chroot stored internally I couldn't use the excellent Chrome OS
editor [ Caret
](https://chrome.google.com/webstore/detail/caret/fljalecfjciodhpcledpamjachpmelml?hl=en)
to edit my code, because it was checked-out inside the chroot environment
using Git, and running there under node.js.  Since the "open file" dialog
doesn't display the root level of the Chromebook's internal storage, there was
no way to browse to these files.  This necessitated running a full X
environment in order to edit and test code*.

  

However, with the chroot on a USB drive, Caret can now browse to and edit
these files, which means the other Linux tools I need (Git and Node.js) can be
run in console "tabs" under Chrome OS, so there's no need to run X to use a
Linux-based editor.

  

This means that most of the time I can do everything I need to do right inside
of the Chrome OS UI, which is very convenient and also more efficient; win-
win.

  

Next time I'll talk more in detail about setting up specific development
environments and some of the Chrome OS apps that have worked well for me.

  

_*Yes I could use a terminal-based editor in a Chrome terminal tab, but the
font size makes it a little cramped, and there are other reasons a GUI editor
can be nice :)_

---
title: Chromebook for Developers (part 3)
date: Sun, 4 May 2014 07:53:44 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This post is going to be about developer tools I've used and can recommend on
Chrome OS.  Previous posts describe setting up a Linux chroot environment so
you can use all your familiar Linux dev tools, and as such I won't spend any
time on those here (you already know who they are).  Instead, here's a list of
favorite Chrome OS tools I've been using for my daily development tasks.

  

**Caret**

  

![](/preposterous/assets/225-screenshot 2014-04-21 at 1.11.40 am.png)  
​

[ Caret
](https://chrome.google.com/webstore/detail/caret/fljalecfjciodhpcledpamjachpmelml?hl=en)
is a lightweight, attractive, performant text editor along the lines of
Sublime Text and others.  It has basic project management skills without being
overbearing.  It's quick to launch, handles large files with ease and is very
configurable.  It's also free.

  

  

**JackDB**

  

![](/preposterous/assets/225-screenshot 2014-04-21 at 1.16.46 am.png)  
​

[ JackDB ](https://chrome.google.com/webstore/detail/jackdb-sql-database-
clien/mfcjpbmafdljmciineiieedkcbeikkpo?hl=en) is a surprisingly complete SQL
database client that works well on Chrome OS.  It's largely a web app and
seems like it does most of its heavy-lifting back on the server (as opposed to
a pure Javascript wire-level SQL protocol implementation) but this affords a
lot of convenience features such as browsing databases of popular cloud
hosting platforms simply by logging in, etc.

  

Performance is good and JackDB has all the features I need on a regular basis
to manage data stored in PostgreSQL.  It may not be the best way to execute
large or complex jobs, but for that you can always drop down into your chroot
and use psql.

  

  

**Postman**

[ Postman ](https://chrome.google.com/webstore/detail/postman-rest-
client/fdmmgilgnpjigdojojpjoooidkmcomcm?hl=en) is an http client (the lazy-
person's curl) which is a very convenient way to interact with HTTP-based
API's or other HTTP interfaced systems that don't provide a user interface.
If you're comfortable with curl, Postman might seem like overkill, but it does
have some features like saving common queries and pretty-printing JSON data
that can be handy for even the most hardened console cowboys.

  

**Draw.io**

**  
**

![](/preposterous/assets/225-screenshot 2014-04-21 at 1.30.47 am.png)  
​

[ Draw.io
](https://chrome.google.com/webstore/detail/drawio/plgmlhohecdddhbmmkncjdmlhcmaachm?hl=en)
is the app that turned me back on to Google Drive.  It's the first drawing
tool I've enjoyed using since MacFlow (unless you count GraphViz, but that's
more of a programming experience).  Integration with Drive is awesome although
the bootstrapping process of sharing a drawing with another Drive user who
hasn't already used Draw.io can be a bit bumpy (probably Googles fault more
than the app developers).

  

  

**Chrome RDP**

Sometimes you just can't (or don't want to) run something on the Chromebook,
and sometimes that's something that only runs on Windows.  There's a lot of
RDP clients in the Chrome Web Store, but the one I've had the best luck with
is called simply [ Chrome RDP ](https://chrome.google.com/webstore/detail
/chrome-rdp/cbkkbcmdlboombapidmoeolnmdacpkch?hl=en) .  I don't do a lot of
Windows dev these days, but for the times I've needed to this app has worked
as well as any other Remote Desktop client I've used on any other platform.

  

  

**JSON Editor**

**  
**

![](/preposterous/assets/225-screenshot 2014-04-21 at 1.35.14 am.png)  
​

[ JSON Editor ](https://chrome.google.com/webstore/detail/json-
editor/lhkmoheomjbkfloacpgllgjcamhihfaj?hl=en) is another tool like Postman
that you can probably get by without, but comes in handy when dealing with
particularly large or complex JSON files.  It's also very helpful when you
need help working on JSON data by non-programmers (or programmers who just
don't spend every day sifting through raw JSON data)

  

  

**BitTorrent Sync**

Finally I'll mention my favorite personal filesharing software, [ BitTorrent
Sync ](http://www.bittorrent.com/sync) .  It's not *really* a Chrome OS app,
but I mention it here because its just so damn useful, and there are a few
tricks to getting it working on the Chromebook see [ this link
](http://forum.bittorrent.com/topic/22534-btsync-on-samsung-arm-chromebook-
with-crouton/) for details.  Once the initial fiddling is done, BitTorrent
Sync runs wonderfully in the background and is pretty easy on system resources
while keeping what you need in sync with the rest of your systems.

  

A better fit for Chrome OS might be a version of BitTorrent Sync that works
more like the mobile versions, which allow you to manage as many shared
folders as you like and browse their contents without actually synchronizing
all of their contents locally (a nice feature on a device that is storage-
constrained).  A clever hacker could make this happen using the [ BitTorrent
Sync API ](http://www.bittorrent.com/sync/developers) , but for now there's no
off-the-shelf solution that does this.

  

  

**Conclusion**

There's a number of other cool applications available for Chrome OS but these
are the ones I use daily for software development.  As stated above once you
have a Linux chroot setup you can just stick to your favorite Linux tools if
you like, but I've found that the hybrid approach has significant advantages
in terms of convenience and system utilization.  For the most part, using the
tools above, I use my Linux chroot to run console-based applications and
Chrome OS for GUI tools, which seems to be much easier on battery life than
running and additional X session, and it's also just a bit more convenient not
having to flip between screens.

  

For my next installment I'm planning to detail the things I've grown to love
and hate (maybe hate is too strong of a word) in the last month or so of using
a Chromebook daily.

---
title: chron error test 1
date: Sun, 15 Dec 2013 20:08:18 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Test1

---
title: C
date: Wed, 02 Sep 2015 08:54:47 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I'm writing some production code in C today.

I don't need to reach for C very often.  The last time was when I needed to
write a service that manipulated video DVD's and the best way to do that was
to use libdvdnav which has a C api.

Today I'm writing a caching server that has some very specific behaviors which
unfortunately don't work correctly with any off-the-shelf HTTP cache I've
found.  I started writing it in node.js, but ran into some limitations that I
couldn't overcome in node that prevented it from working the way I wanted it
to.  I knew I could do what I needed in C, but I'd never written networking
software in C before, so I had a bit of a learning curve to overcome.

I wish I could use C more often.  It takes me awhile to get warmed up, but
once I get back in the groove, it's just so powerful, and so non-mysterious.
C does what you ask it to, for better or for worse, but I can't think of a
single time that a C program didn't work and I found the reason outside of the
code I wrote.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: cron test 3
date: Sun, 15 Dec 2013 20:09:40 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 2

---
title: Curation
date: Wed, 14 Jan 2015 06:40:02 -0600
author: jjg
draft: false
tags:
  - preposterous
---
I'm going to be migrating some of my favorite posts from my original [
Preposter.us blog ](http://preposter.us/oregon-king-uranus-autumn/) to this
one over the next year.  Along the way I plan to update them as I see fit, so
consider them the "second edition" if you will.

I'm doing this because I want to make sure this information remains public,
but importing them in a way that reflects their original chronology is
complicated due to the email-based nature of [ Preposter.us
](http://preposter.us/) .  At first I was frustrated by this, but upon
reflection I think the end result will be of higher quality, and I'm less
concerned with establishing the strict age of these post than I am about
simply making them easily available to myself and others.

\- Jason

---
title: Dam Camp
date: Sat, 03 Oct 2015 09:57:41 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I'm thinking about "organizing" another Damcamp (Beaver Dam barcamp).

We did one a few years ago and it turned out pretty cool.  Nothing huge, but a
good mix of presentations and a lot of great conversations.

I think in the intervening years the community has become more interested in
this sort of event (or perhaps I've just come in contact with more of the
community).  So it might be time to revisit.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Deadbeat Dads
date: Sat, 5 Apr 2014 08:12:44 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Software developers who launch programs and then fail to support them are
sometimes referred to as "deadbeat dads".  It makes me wonder, what is the
female equivalent of a deadbeat dad?

  

Don't panic, I don't plan to abandon [ Preposterous
](https://github.com/jjg/preposterous) , but from all outward appearances it
might seem like I have.  Preposterous was born in a fit of fiery passion,
version 1.0 being released just days after the project began.

  

After taking a week off from the project (something I try to do with every
type of project) I sat down renewed and penned a somewhat (by comparison)
ambitious plan for a 2.0 release, a release that would make Preposterous have
the capabilities that are expected from a blogging platform but would retain
the elegance and simplicity of Preposterous 1.0.

  

Charging forth, these features fell to my keystrokes one after another and
within two weeks all but a handful had been committed to the repository.  With
only a few more remaining, I felt confident to let the scope creep a little
and work in a few things that I thought would be really cool.

  

Now mind you that I've been doing this long enough that I should have known
better, and I definitely should have known that, consciously or not, I had
pushed the hardest problems to the back of the line.  What I _should_ have
done was stick to the original list and kill all the original bugs and
features before adding more, but you know, hindsight and all that.

  

Then came the new bugs.  Some came from the new code I was writing, some came
as the result of more extensive testing and use of 1.0, but in either case the
amount of work that needed to go into 2.0 started to mount, and this wasn't
the fun kind of work like building new features, it was the not-fun kind of
work like trying to figure out why every email program on the planet did
different things with embedded/attached images, seemingly even between
versions of the same program!

  

This state of affairs was really wearing me down, so much so that I stopped
using Preposterous frequently because my own dog food was just making me too
sick.  

  

What I did do _right_ was continuously integrate the 2.0 features into the
mainline and release them into production unannounced.  So in reality by the
time I made the 2.0 release announcement, most of the new features would have
had plenty of production use and may have even been through a few debugging
cycles.

  

This is where Preposterous stands today.  A considerable subset of the
features slated for 2.0 are (and have been) available to production users
along with a few "bonus" features I slipped in along the way.  What is missing
are a couple of things a real blogging platform should have (like proper
deletion of posts) and a few very nasty bugs around handling attached/embedded
media.  Instead of letting this "almost 2.0" state get me down, I'm going to
embrace it, and begin working frequently with Preposterous even if I have to
limit that use to the "happy path" until I'm able to address some of these
shortcomings.

  

Along the way I hope to encounter others who see the beauty in the idea of an
email-only driven blog, and can feed off their encouragement and help bringing
Preposterous to a point where it's valuable to a wider audience.

  

  

  

\- Jason

---
title: Debugging weird A things part 1
date: Sun, 7 Dec 2014 23:50:26 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a long-ish post attempting to debug the problem where a weird A-like
character shows up in posts that are posted from Gmail.

  

Hopefully we'll catch one in the act here.  With my luck we won't and I'll
have to find another way to recreate the problem.

  

Here's to hoping,

  

  

\- Jason

---
title: DELETE:hard delete test 0
date: Fri, 10 Jan 2014 22:46:08 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: Dogfood
date: Wed, 1 Jan 2014 00:17:13 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I knew the only way I'd be sure to continue to improve [ Preposterous
](https://github.com/jjg/preposterous) was if I was forced to use it
regularly, so I decided that on January 1st, 2014 I would switch Gullickson
Laboratories from Wordpress to [ Preposterous 2.0
](http://www.gullicksonlaboratories.com/thoughts-on-preposterous-2-0.html) .

  

Of course [ Preposterous 2.0 ](http://www.gullicksonlaboratories.com/thoughts-
on-preposterous-2-0.html) isn't ready yet, but I didn't let that minor detail
stop me from meeting my arbitrary goal of switching over the site.  I had also
hoped to have something in place to handle incoming links to pages that no
longer exist, but you can't make an omelet without breaking a few eggs, right?

  

So, things are going to be a bit of a mess around here for awhile but what
better motivation could I have to make Preposterous better than making my own
blog count on it, right?  Over the next few weeks I'll be bringing over the
most popular posts from the old site but I didn't want to simply _import_
them...

  

Instead, I want to take time to "curate" the old posts.  Some will simply be
re-posted as originally written but others will be annotated to provide
context and framing so they can be seen in the light of their time (remember
some of those posts go back to 2000, and some things have changed since then).
Others, especially those that relate to specific technology may be updated so
they can be useful with current versions of software, etc.

  

In addition I'll continue to pursue the remaining work outlined for [
Preposterous 2.0 ](http://www.gullicksonlaboratories.com/thoughts-on-
preposterous-2-0.html) and beyond, with this blog being the key source of
information about updates, releases and other information about the project.

  

Thank-you for your patience during this transition, it's not the first, and it
certainly won't be the last.

  

  

\- Jason

---
title: Durovis Dive build
date: Sat, 20 Dec 2014 09:01:29 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Last nights project was finishing off a Durovis Dive ( [
http://www.durovis.com/index.html ](http://www.durovis.com/index.html) ) head-
mounted display.

  

![image1.JPG](/preposterous/assets/6-image1.jpg)  

  

More specifically this is the do-it-yourself OpenDive which consists of a $9
parts kit (lenses and a strap) an STL file of 3D printable parts.  Here's a
few shots of the build in-progress:

  

![image2.JPG](/preposterous/assets/6-image2.jpg)  

  

![image3.JPG](/preposterous/assets/6-image3.jpg)  

  

![image4.JPG](/preposterous/assets/6-image4.jpg)  

  

The results are frankly **amazing** .  I haven't had a lot of experience with
other HMD's, but I've studied (and coveted) them since the late 1980's so I'm
familiar conceptually with the most common problems associated with other
designs.  The Dive isn't perfect, but it's very good, and very clever as well.

  

Probably as important as the hardware is the software, which runs on the phone
you strap into the headset.  Instead of writing a specific, all-inclusive (and
proprietary) app for the device the developer instead chose to implement a
component which can be used with the popular game development tool Unity to
work with the hardware.  This is a quite clever approach as makes any existing
game compatible with the device with only a small change made by the
developer, and it makes creating new titles as easy as creating any other type
of game with the Unity tool.

  

At first I was disappointed about this as I would prefer an open software
solution (and one that works on Linux) but after looking more closely I think
it's a good trade-off.  There's also no reason someone ambitious couldn't go
the open route as well, and the Unity approach lowers the bar for authoring
content substantially.

  

That low bar is the next step for this project.  I started reading a Unity
development book last night and other than it shackling me to our Mac, I don't
think learning the tool will provide too much of a barrier.

  

  

\- Jason

---
title: Easier to test than to research
date: Sat, 03 Oct 2015 12:34:13 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Seems like I didn't get notified when my last post was posted.  I thought
about digging in to troubleshoot, but realized that it was so easy to post
another message that a quick test would be faster.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Electric Cafe
date: Thu, 4 Sep 2014 10:51:42 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a summary of an idea I've had for awhile that was rekindled when
talking with my friend Mike who owns a Nissan Leaf.

  

A chain of restaurants that are primarily located in rural towns, serving
"slow food" made with locally-grown ingredients.  Each franchise is located
within 100 miles of a major metropolitan area, or another Electric Cafe
location.

  

Each location features EV charging stations at every parking lot stall,
allowing EV drivers to recharge their vehicles for free while they enjoy a
well-made, wholesome meal.

  

Each location features local produce and other ingredients, so the menu varies
but also contains a number of "staples" so that the fare is varied enough for
the adventurous but does not alienate those who desire predictability while on
the road.  In addition to meals other amenities such as coffee and Internet
access are provided to encourage customers to "hang out" and relax while their
vehicle is charged.

  

Proximity to metro areas (and at the intersection of metro-to-metro routes)
ensures a supply of city-dwelling EV owners (where charging stations are more
plentiful) that can be drawn safely out of the city because ample and fast
charging is available.  Proximity to other Electric Cafes allows EV owners to
travel cross-country from location to location allowing daily travel ranges
otherwise not possible (expansion to provide overnight lodging could be an
additional vector for growth that could allow EV's to be used for truly long
trips).

  

By concentrating EV owners in a relaxed atmosphere and encouraging them to
"hang out" while their vehicles charge, Electric Cafe enjoys a strong "network
effect" and encourages word-of-mouth advertising.  Since customers are drawn
from disparate areas this word-of-mouth can travel from city to city easily,
and the uniqueness of the cafe will also attract non-EV owners to visit and
interact with EV culture.

  

In addition to marginally increasing employment via staff, management, etc.,
rural towns benefit from the additional traffic generated by visitors, but
more importantly these are truly new visitors who would otherwise be unlikely
to visit due to "range anxiety".  Additionally, these towns are uniquely
qualified to support the dining style of the Cafe, and the regional diversity
encourages visitors to explore multiple locations.

  

There are electrical grid considerations when deploying a Cafe capable of peak
electrical draws described here, however many rural towns were once home to
large industrial manufacturing concerns and therefore retain the electrical
infrastructure capable of supporting these large electrical capacities.
Moreover, previously offline electrical generation sources as well as new
renewable sources (wind, solar, etc.) are more deployable and less expensive
to operate in rural areas.

  

Finally, the franchise model allows for the initial R&D and engineering costs
associated with designing and tuning the model to be amortized through sharing
with each new location.  Furthermore, other aspects of operating the Cafe
(collaboration with utility companies, charging equipment providers and EV
manufacturers) may be easier to negotiate as a group vs. a single
entity/location.

  

I'll be investigating what it would take to "bootstrap" such a business in my
own small town.  If you're interested in discussing this project feel free to
contact me via twitter **@jasonbot2000** .

  

  

\- Jason

  

---
title: Elixer for the Luddites Lament
date: Tue, 04 Aug 2015 08:05:12 -0500
author: jjg
draft: false
tags:
  - preposterous
---
The world I wanted to live and work in when I was a teenager is just beginning
to emerge.  Electric vehicles, Virtual Reality and Artificial Intelligence are
all beginning to reach a level of usefulness that I expected to spend my adult
life working with.  This is somewhat disappointing since it's about 30 years
behind schedule, but I'll take it.

Other aspects of this word seem to have moved backward.  Culturally, socially
and philosophically we seem to have slid backward thirty years or more, and
it's easy to get dragged down by this, because it feels like advances in
technology are meaningless if we are loosing what makes us human.

I don't think you have to trade one for the other, technology for humanity,
although this is often how the situation is portrayed.  I think the reason for
this has more to do with the financial binding between these two forces, and
we forget or are simply ignorant to the fact that this binding is artificial,
a system we volunteer to persist.

So I choose to focus on realizing the positive potential of these tools, and
hope that by doing so, in an open, collaborative way and with the intent of
making them as accessible to as many people as possible, will offset the power
invested in maintaining the status quo.

We simply have to build the world we want faster than they can take it down.

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: 2 Weeks
date: 2009-04-21
tags:
  - evernote
---
I was starting to get really bummed about the bike Saturday morning, it was
starting, then surging and dying, and no matter to screw-tweaking seemed to
make a difference. I decided that for some reason maybe that overflowing carb
was involved and that instead of trying to "tune the whole guitar" maybe I
should "replace the broken string" first, so to speak.  
  
This was a good move in that it got me in the right mindset for the job.
Visions of riding the bike were now far away and out of focus and all I was
looking at was the carb, and why fuel kept coming out of the overflow. As far
as I know, the only way fuel flows out of this port is when too much fuel is
in the bowl. So I turned my attention to how fuel gets in the bowl, which
works surprisingly like a toilet.  
  
The bowl is at the bottom of the carburetor and inside this bowl is a float
(really a pair of connected floats). When fuel fills the bowl, these floats
move upward and at a set point, engage a valve which cuts off the fuel. As
fuel is used by the engine, the floats drop as the fuel level drops and at a
certain point, fuel once again flows into the bowl. If this mechanism fails to
do its job, there is an overflow tube that lets the fuel run out the bottom of
the carburetor. This is what was happening to my carburetor.  
  
The first thing I did was pull the bowl off. I was really afraid that the
gasket was going to jump out and I was going to have to wait another week for
it to shrink back to normal size so I could put the carb back together again.
To my surprise, it just stayed in place, so I let it be and I was careful not
to disturb it the rest of the day.  
  
First I wanted to test the shutoff valve, since a failure or leak here would
explain everything. I manually lifted the floats to their maximum "closed"
position and turned on the fuel: no leak. I slowly dropped the floats with the
fuel on and as expected, the fuel began to flow as soon as the floats allowed
the valve's needle to fall away from its seat.  
  
The next test I did was of the floats themselves. One common cause of this is
a hole in the float, so that it is no longer buoyant and doesn't lift when the
bowl fills with fuel. I did a crude test of this by removing the floats and
placing them in the removed bowl. I added fuel to the bowl and the floats
lifted as expected. Examining the floats after this test, there was no
evidence that they had taken on any fuel, or had any holes or openings that
should not be there.  
  
Another cause of this problem can be a mis-adjustment of the cut-off point for
the fuel valve. Like a toilet, if the valve waits too long to close the fluid
will continue to flow even if the float is at its maximum height. On the
carburetor this is adjusted by bending a small metal "tang" on the float
assembly which contacts the needle portion of the fuel valve.  
  
There is a recommended procedure for adjusting this tang in the manual, but it
requires you to tilt the carburetor and take some measurements. Obviously this
is impossible with the carb attached to the engine. Removing the carb from the
engine is surprisingly allot of work on this bike due to the position of the
exhaust and how the throttle linkage works so I thought instead that I would
take a "cut-and-try" approach to adjusting the tang.  
  
Either I thoroughly underestimated the precision necessary for this
adjustment, or there is some other cause for the overflow because I tried this
adjustment no less than 27 times through the course of the day, with not
satisfaction. I was able to adjust the tang to the point where no fuel flowed,
and adjust it the other direction to cause the overflow, but I was unable to
find the "sweet spot" where the float action correctly metered the fuel in the
bowl.  
  
I'm surprised that the "window" of adjustment is as small as it appears to be
and I have a feeling there is more to this than I'm understanding.  
  
At this point it's clear that another approach is necessary if I'm going to
correct this problem. It is unclear if correcting this will make the bike run
right, but I've decided to fix the things that I know how to fix, and since
this engine running problem is more complex and elusive, I'll focus on the
carb for the moment.

---
title: 3l33t
date: 2012-12-01
tags:
  - evernote
---
[ ![image](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/11/wpid-2012-11-30_12-33-26_781.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/11/wpid-2012-11-30_12-33-26_7811.jpg)

---
title: A beautiful morning for a ride through the county...
date: 2009-06-26
tags:
  - evernote
---
  
  

---
title: About
date: 2013-09-30
tags:
  - evernote
---
  
What's this about?

  

I'm in the process of testing [ Postach.io ](http://Postach.io) as a
replacement for [ Wordpress ](http://www.wordpress.org) .

  

If all goes well, I'll be switching [ www.gullicksonlaboratories.com
](http://www.gullicksonlaboratories.com) over, but first I need to work a few
things out (and wait for the import tool to be ready).

  

  

\- Jason

---
title: A Clue Perhaps
date: 2012-12-05
tags:
  - evernote
---
Did a few more experiments with the QU-BD extruder tonight and came to the
conclusion that the temperature being reported by the thermistor is probably
incorrect.  
  
I thought more about what the filament looked like when I pulled it out of the
jam, and it wasn't even hot to the touch, so tonight I reviewed my thermistor
table settings in the firmware to make sure I was using the correct settings.  
  
Turns out I am, or at least ones that are close according to [ this post
](http://www.fabric8r.com/forums/showthread.php?728-Thermosistor-delta-values-
etc) on the QU-BD forum (interestingly enough started by me, and the first
post to their forum...).  Of course I couldn't just grab the actual table from
the manufacturer because they posted it as a PDF...so I'll have to dissect
that on another machine (my Linux lab workstation doesn't do PDF).  
  
I'm going to give the manufacturer's table a try, but I don't have great
expectations.  Out of curiosity (i.e., rage) I cranked the heat up to an
indicated 260C and I was finally starting to smell melting PLA and pull
(jammed) filament out that was starting to melt.  I would have went further
but since I have to disassemble the extruder to reload it after a jam, I ran
out of time waiting for it to cool-down enough to take it apart.  
  
I'm going to ask the QU-BD folks what the resistance should be at room temp as
well, because I have a feeling this thermistor is completely off the chart.

---
title: Adventures in Mailserving
date: 2000-11-16
tags:
  - evernote
---
So, I had written a long article about a quest I was on to build a new server
for another of my projects, www.fish-zine.com, and I began writing an article
for this site about that process, but it was boring and I dropped it. Suffice
to say, I bought an SE/30 off of eBay and thought it would be funny to run
that site off a web server with a 40 meg disk and 5 megs of RAM.  
  
It turned out to be less funny and more frustrating.  
  
However, it did work, and I had planned on putting up a link here so you can
check it out, however something interesting happened. In addition to
installing a web server on this box, I also built a mailserver. I spent about
three days troubleshooting the mailserver, fixing routing loops, etc, and
finally, night before last, I got it working...how cool.  
  
So I come home last night and check on it, and the system is locked up. Big
deal, right? Just turn it off and back on again. No...it just comes up with a
screen full of garbage, and no startup noise. Damn.  
  
I let the box sit overnight, and still nothing. It's like it burned up or
something. The weird thing is that it has been on since I recieved it (like
three weeks), and it didn't burn up untill everything was working...bitch.  
  
So, instead of my cool webserver, I have a new poll for you, I'm sure you'll
enjoy it.

---
title: A Faster Horse
date: 2016-09-03
tags:
  - evernote
---
I finally understand why people choose a car like the Mustang over better
cars; because they are not looking for a car, they are looking for a Mustang,
and no car is a better Mustang, than a Mustang.

  

The same reasoning applies to a Harley-Davidson or an iPod.  The experience of
the particular "make" (for lack of a better word) defines the type, or
"microtype".  Comparisons to other items outside the microtype are
meaningless.

  

This doesn't mean progress is impossible, it just means that it must be
measurable across all properties that define the microtype, not just typical
engineering or design measures.

  

It might seem like "brand" is one of these metrics, but not exactly.  The film
brushes on the development of a more modern Mustang which was ultimately
rejected by the Mustang audience because too un-Mustang, even though it came
from Ford and bore the Mustang name.  In this way, once the microtype has been
established, it's no longer controlled by the creator, but by the consumer.
There's a number of examples of this in other products, and in the case of the
"modern mustang", the solution was to turn it into a new vehicle (in this
case, the Ford Probe).

  

There's another gem in the film where the chief engineer talks about
leadership.  I wanted to include it here, but I couldn't find an unencumbered
copy of the media.

  

The film compells me to create something that people love as much as the
Mustang.

---
title: After Much Consideration...
date: 2016-09-12
tags:
  - evernote
---
I've decided to build Crypton on [ JSFS ](https://github.com/jjg/jsfs) .

  

Originally I thought it would be faster to build it using a more traditional
API framework, or something like [ Express ](http://expressjs.com/) or [ Flask
](http://flask.pocoo.org/) , because it has a heavy client-side component as
well.

  

But after thinking hard about how the back-end should work, it became clear
that most of the work would be happening on the front-end.  Also, it's the
back-end which presents the most vulnerable surface to attack, so it should be
as simple as possible, and the application shold rely on it as little as
possible.

  

This, coupled with my obvious familiarity with the platform made it apparent
that I should at least try to build it on top of JSFS.

  

I have a few more projects I’d like to wrap-up before diving deeper into
Crypton, also it’s one of those rare times of the year where the atmosphere
here is cooperative, so I’m not planning on getting started on this until that
changes.  However, come winter, I expect to be making some progress.

  

In the meantime I need to attract some fans, because I’m going to need a lot
of feedback and actual user mileage to get it right.

  

---
title: All-carb diet
date: 2008-06-20
tags:
  - evernote
---
I spent some quality time with the carbs in the basement tonight.  
  
At first I was a bit worried to see that there were more parts in the "carb
kits" that I bought than I could find a place for in my carburetors, however
consulting my handy service manual indicated that my CL came with two
different types of carb, and these extra pieces were for the "other" kind.  
  
Other than that things went very smoothly and I had both carbs back together
in about 90 minutes. I still need to adjust them (as much as you can before
mounting them) but I was just happy to have all the pieces together that I
called it a night.  
  
I'm working on a longer post to outline everything I have planned for the bike
at this point, I'll post it soon once I've updated the list.  
  
Progress!  
  
(BTW, I took a few pictures but my damn VQ1000 threw an error and I wasn't
able to pull them off the camera...I think I'm going back to film...)

---
title: and now for something completely different...
date: 2000-10-25
tags:
  - evernote
---
I've decided on something new for the weekly (or whatever) article; since I
can't seem to come up with anything clever each week, I'm going to start
posting something that I can come up with endless amounts of...  
  
...movie reviews...  
  
You might think "who cares?"...well, if you're asking that, you're at the
wrong website in the firstplace, since the only reason this exists is because
register.com gave me the domains for free, and I have little intention of
giving you, the reader, any really compelling reason to visit the site. It's
more of a testbed for code that isn't ready for any of my "real" (hmm...what
does that mean?) sites anyway.  
  
So, without further adue, my first review...  
  
Mission to Mars  
  
Mission to Mars was recently released and DVD, although I was unable to track
down the DVD version (not that I tried real hard), so I can't comment on the
DVD "extra" stuff, but here's what I thought of the film itself.  
  
I'm not exactly sure where we're trying to go with a film like Mission, from
the title and cover (and the press that I had seen on the item), it looked
like a near-future view of what our first possible mars mission might be like,
with some drama bla-bla-bla thrown in to keep it viewable by humans (as
opposed to typical sci-fi viewers). After watching the film however, I have a
decidedly different opinion.  
  
I think this was supposed to be maybe a more watchable, hipper "2001"-type of
film where we take something somewhat fantastic (a mars mission, something
possible in our lifetime), which then becomes something less based on reality
(meeting martians who look like lava lamps and are our ancestors) with a
somewhat surreal spin to it. I don't think there is a problem with this
formula, but what irks me about this film (and most films like it) is that the
connection between the real and the unreal never seems too well thought out.  
  
In this movie, there is an original mission that takes a group of astronauts
up to mars, and there they encounter something weird that kills all but one of
them. Then, a recovery mission is setup to return to mars and fetch the
remaining astronaut (and possible figure out what happened to the rest).
Durring this rescue mission, tiny asteroids damage the rescue ship, causing
one of the rescue mission astronauts to be killed, and destroying their
landing craft, forcing them to use a supply satellite to get to the surface of
mars.  
  
Tiny asteroids?  
  
What's weird about this is we spend about 30 minutes of the film dealing with
these asteroids, and I'm not really sure why this even happens, since once
they get to the surface, in the end they are able to use the original vehicle
to return to earth; thus being no worse off than before the tiny asteroids
(except minus one astronaut).  
  
My best guess is that this was either:  

  

  1. A cost issue (less screen time for one of the big actors) 
  

  2. More special effects for the trailer 
  

  
Either way, it doesn't seem neccisary, and adds nothing to the story other
than possibly a "bridge" between the "normal" world of space travel and the
"weird" world of Mars (remember the 30-minute long fight scene in "They Live"?
This is the same technique, minus the pro wrestler).  
  
If it wasn't for this bizzare occurance, and the mediocre special effects (at
least for a sci-fi movie), I might actually recommend the film, because it
does have some interesting spots, and some fun plot points.  
  
Overall, I'd say it's a "Sunday on USA"-class film, and makes decent
background if you're up at say 12:40 am on a weeknight...  
  

---
title: ...and the hits keep comin'!
date: 2001-02-01
tags:
  - evernote
---
This has been a productive week here at jasongullickson.com, we gave you the
library at the beginning of the week, and today we have another new toy for
you, the oracle.  
  
The oracle is an experiment in artificial intelligence utilizing the latest in
high-speed neural-network technology, evolutionary machine learning, and dash
of old-school hard-core-coding that results in the most sophisticated example
of a dialog-based machine intelligence.  
  
click here to give it a whirl, and let us know what you think; it is a work in
progress and like anything else here, no warranty, etc.  
  
Oh yeah, there's also a new poll for those of you that are into that kind of
comedy, and also, on a side note, we are very near to launching madiscene.com,
so if you'd like to know when we do, click the link above and add your name to
the annouce list.  
  
...over and out...

---
title: Any of you played with Asterisk?
date: 2009-09-24
tags:
  - evernote
---
  
Let's chat.  

---
title: "Evernote Archive"
date: 2018-12-29
draft: false
tags:
  - archives
  - evernote 
---

Posts from my [Evernote](/tags/evernote) archive are now avaliable, many for the first time in over a decade!

This one was particularly exciting becuase it includes posts from every previous incarnation of jasongullickson.com as well as the original Gullickson Laboratories blog, a journal I kept during the making of American Cafe and many other sources going all the way back to the year 2000 (about 200 posts in all).

As with previous archives, these posts are not perfectly preserved (notably all images are missing).  I'm working on extracting these from the Evernote archive file but for now the words will have to speak for themselves.
---
title: are you finished?..oh well allow me to retort
date: 2000-11-20
tags:
  - evernote
---
**A retort to the Queen**  
  
Your Gracious Majesty, I must retort. I will not apologize for nor can I
explain the 97.85% of American dullards that cannot seem to elect leadership
for this great nation. However, I beg mercy, Your Grace for those of us who
have attempted to right this nefarious wrong by voting for a third party
candidate.  
  
Your Highness must be aware first and foremost that while a vast majority of
American nitwits have grappled amongst Bumbling Bush and Asinine Al, an
infinitesimal bunch have not. We, Your Exaltedness, deserve immunity. I
therefore propose the following compromise to your order.  

  

  1. A small and separate nation will exist inside of your British American borders. We prefer Nevada, but will accept Utah if you promise to take the Mormons (and all their wives). 
  

  2. Our sovereign nation maintains the right to Microsoft. You may take Bill Gates into British custody and torture him as you wish. However, without Microsoft we won't have an economy except for prostitution and gambling..on second thought, scratch that demand. 
  

  3. We gladly renounce "American" football. However, we would like you to accept a new sports development that involves use of the current House and Senate members. We have not yet fleshed out the details, but we will be working with the Russians (duh, we know they are not the bad guys!) to formulate some "bloody" rules. 
  

  4. We will keep "duh", "like" and "you know" in our vocabulary, else we may loose many of the dumb blonde hookers necessary to our primitive economy. As a compromise we will institute the use of "bloody". 
  

  5. We will keep the Constitution. We will not be denied the right to free assembly, free speech and all the other freedoms that those Congressional bastards try to take away. Most importantly we will not be denied the right to bear arms in case your fat British ass decides to try and renege on this agreement. 
  

  
Sincerely yours,  
  
The Sovereign Leader of the New American Republic -  
  
THE PEOPLE!  
  

---
title: Asterisk Round 1
date: 2009-09-27
tags:
  - evernote
---
  
Got the pbx online and one client attached, but no luck getting an iPhone
client to work. Laptop battery is cached, so on to something else for the
night.  

---
title: A Week Without Records
date: 2011-06-24
tags:
  - evernote
---
"A Week Without Records" is an exercise in musical appreciation. The idea is
simple, for one week avoid listening to mechanically-reproduced music. The
benefits to you are twofold:  
  
  
  
First off you will be encouraged to seek out non-mechanical sources of music
to listen to. The most obvious source of this is live performances of
professional musicians, however you may find other sources of music as well in
street performances, friends and family who sing or play an instrument or
perhaps you will be motivated to learn to make music of your own?  
  
  
  
The second benefit you may experience is a heightened satisfaction with the
music you are able to experience. By making music "scarce", it is easy to
savor the music you have and encourage listening deeply instead of passively.
I believe that this "mindful" listening will allow you to see the quality in
forms of music you may otherwise have overlooked (and perhaps see the opposite
in music that is easily available otherwise).  
  
  
  
AWWR 2011 will be observed July 3rd - July 9th. This week was selected to
coincide with Independence Day celebrations in the United States to increase
the likelihood of live music being available in most US communities (not that
AWWR is US-only, but that is the area I am most familiar with). Also there
just seems to be more live music in the summer :)  
  
  
  
Background:  
The idea for AWWR first came to me while spending a day at Old World
Wisconsin. In this re-creation of 1800's America, the prevailing soundtrack is
one of nature and some human voice. In the afternoon we happened across a
musician playing an instrument on the porch of a shop and the music was as
sweet as any I had ever heard. Even thought the style of the music was outside
of my normal "taste", the combination of the context in which it was presented
and in the void of musical sound leading up to the experience, it allowed me
to hear the music in a deeper way, seeing past genre and style and feeling
only the quality of the musicians work and the instrument's creators
craftsmanship.  
  
  
  
I originally set out to create AWWR with detailed descriptions and complexity
in 2010 and never completed the project. So instead of all that I'm starting
it in 2011 with this simple post and if others find the idea enriching we can
work together to make the experience more accessible to others in the
following years.  
  
  
  
If you'd like to discuss AWWR join me on Twitter (I'm @jasonbot2000) or maybe
we'll run into each other at a concert next month. If you choose to
participate I'd love to hear about your experiences!

---
title: Baby Steps
date: 2009-08-04
tags:
  - evernote
---
Another step in the right direction (?), the previous owner of "This Old
Honda" got a letter from the DMV requesting some sort of "Bond". I'm not sure
what that's all about, but Dan said he did, so I guess that's progress?  
  
Anyway, some good news; the film is coming along fine. We're deep in post and
planning on shooting some "supplementary" footage in a couple of weeks that
should wrap up just about all of the loose ends we have on the photography
front. I've also been working on the score with our "court composer" Derek
Schyvinck and we seem to be on the same wavelength.  
  
I'm hoping we will have a "teaser" available in the next month or so to share,
but our focus at the moment is solidly on getting all remaining footage in the
can so we can focus on the edit. That said, we could all use something to
"point at" when we're discussing the film and I'm sure Matt could use a break
from cutting interview footage.  
  
In the meantime, go riding, it's beautiful out there!

---
title: Back in Black
date: 2001-02-12
tags:
  - evernote
---
Oh boy, more new code!  
  
We rebuilt our primary server this weekend, skylab, something sweet about a
fresh install of everything, hopefully we can keep the logs free of errors for
at least a week (and my man kob can get some access to Oracle 1.2 is out, now
with email support for you lazy bastards that don't wanna come back to get
your answers, and the library has some updates, including title counts in the
browse menu and a new feature that lets members leave comments for titles
(mini-reviews, smart ass remarks, whatever).  
  
Oh yeah, and then there's alf on the projects page, if you use msearch, you'll
dig it, if you don't, you might still find it useful, just the result of me
abusing Visual Basic...  
  
...and yeah, there's a new poll for yall to not respond to...  
  
later sk8er...  
  

---
title: Bennie
date: 2009-03-18
tags:
  - evernote
---
I picked up a new battery from Fleet Farm, the same as the old one, just not
fried. Added the acid, threw it on the charger and headed to the gas station
for some fresh fuel.  
  
I thought I'd be draining the "over winter" gas from the fuel tank but when I
popped the gas cap, the tank was dry. I had noticed some leaking over the
winter, but I had no idea how severe it was. I'm not sure where or why it
leaked, because all last summer it held fuel (once it was treated) but when I
topped it up in the fall, it somehow got back out.  
  
Something to keep an eye on; anyway…  
  
I dropped the freshly charged battery in the bike and tested the lights: all
systems go. Poured in maybe a gallon or so of Premium (I thought I'd leave
Ethanol out of the equation, for now) and after about five kicks, she started
right up and settled down into a nice idle.  
  
Unexpected, but nice.  
  
The idea here was to make sure the thing would start as-is before I started
messing with the jets. I didn't want to swap them out and have trouble getting
it to run and be left wondering if something else had happened during the off-
season or if it was just the recent changes. Now I know that if it doesn't
work, it's my fault.  
  
So I shut 'er down and inspect the bike as the engine cools. Everything looks
good, surprisingly, although the top-end sounded a little lubrication-starved
to me (not sure if this is normal or not for this bike, but something to check
up on).  
  
My plan is just to drop the bowls and swap the jets for the largest ones that
came in my "kit". I figure since I more or less removed the exhaust system and
opened up the intake as well I should start large and work my way down. The
logic, hopefully not flawed, is that the side-effects of too rich a mixture
(smoke and wet plugs) are better than that of too lean (overheating,
detonation, etc.).  
  
Unfortunately it wasn't going to be as simple as I would have liked to get
these bowls off. First off, the right-side was too tight for any screwdrivers
I had on hand, so I had to run to Fleet to pick up some stubbies and a pair of
offset drivers (which are pretty cool). This was enough to get the bowl off,
which led to the next problem.  
  
When the bowl came off, the (new) gasket jumped out and somehow was too large
to fit as-is back into the groove on the bowl. After trying in vain to get it
back in there I decided to "shorten" it. I'm pretty sure I'm going to regret
this, but since the only recourse was buying a smaller one, I figured there
was no harm in trying it. Even after shortening it, it was impossible to get
it to stay in the groove (if anyone has any tips for this, please pass them
on, I'm going to be doing this a few more times). After messing with it for
awhile I was able to get the bowl back on with the gasket in place and I think
it's in the right place…  
  
The left side was worse.  
  
High pipes, while cool looking are not very practical. In this case, the top
one made it almost impossible to get at the carburetor bowl with the carb
mounted on the engine. Dropping the pipes would help, but since jet selection
(for me) is a trial-and-error process, I really want to avoid having to do
this each time I want to try different jets. After about thirty minutes I was
able to get the bowl out and drop in the new jets.  
  
The same gasket problems exist here (how did these things grow?) and are only
compounded by the inaccessibility of the left carb. After another thirty
minutes it was starting to get dark so I decided to call it for the night.  
  
Thinking it over later, I should have just dropped the pipes. It would have
only taken a few minutes, and I may have even gotten everything back together
before dark. The worst part was that I knew this at the time, but I was too
focused on trying out the new jets that I didn't take the time necessary to do
it right.  
  
At least I know what to do next time…  
  
(sorry for the blandness of this post, I was in too much of a hurry to take
pictures. Lucky for you, I'll be doing this again and I'll try to snap a few
next time.)

---
title: Beta 0.4 of the new iPhone app posted...time for a victory lap!
date: 2009-06-04
tags:
  - evernote
---
  
  

---
title: Big Copyright
date: 2016-09-06
tags:
  - evernote
---
I found out something surprising today: there’s no copyright protection for
raw data.  Here’s my source:

  

[ http://www.nolo.com/legal-encyclopedia/can-you-copy-raw-data-from-a
-protected-database.html ](http://www.nolo.com/legal-encyclopedia/can-you-
copy-raw-data-from-a-protected-database.html)

  

Now of course this is one source, and like most things involving U.S. law,
it’s debatable, but the fact that there is precedent to support the fact adds
some weight to the argument.

  

If this is true it opens up some interesting areas of discussion around Big
Data.  One of the key tenants to most big data architectures is a focus on
capturing “raw data”, data that hasn’t been selected, coordinated or arranged.
The idea behind this is that it allows you to defer decision-making about what
data you need and what data you don’t.  By capturing all the data raw, you
have everything available to you if at a later time you want to extract
something unexpected, or correlate the data a new way.

  

However, in light of the legal explanation above, this means that companies
who are operating Big Data initiatives are currently amassing large pools of
data for which traditional legal protections do not apply.  This data is
currently bought-and-sold for-profit among many different companies and
agencies; I wonder if they understand that they don’t own it?

  

The barrier, of course is access.  While you and I might be able to use say,
Target’s customer database without violating copyright, we would first need
(legal) access to it.  This seems like a pretty secure barrier, but maybe
there’s a loophole.  If such a loophole exists, the investment these companies
have made in collecting and managing this raw data may not provide the return
they expect.

---
title: Black Friday
date: 2000-10-09
tags:
  - evernote
---
What follows is the english version of my post on www.fish-zine.com today
reguarding the outage we encountered on Friday, Oct 6.  
  
The reason for black friday:  
  
For the beginning of our life online we were utilizing bandwidth and servers
that were so gracefully donated to us from my employeer. For the first few
months this went just fine.  
  
After awhile, some employees of this organization didn't agree with the
content of the 'zine and starting bringing this to the attention of other
employees. This manifested itself as several conversations with myself about
"censoring" or "editing" of the site.  
  
I do not and will never consider the censorship of the site an option, and
this is what caused our outage on friday.  
  
Because of this disagreements at our previous home, I took the site down
immediately and moved it into our secret, secure bunker known only as
"skylab". This is where all of our scientific research goes on and is a
somewhat hostile place for such a delicate flower as the fish-zine.  
  
In the light of this, we will be dedicating a new server to the 'zine as soon
as I get it (it's ordered), and as soon as I can figure out how to work it
into the current configuration (more beard). In the meantime, the 'zine will
reside on a box in the 'lab and hopefully will generate as little grief as
possible.  
  
Long live Zork!  
  

---
title: Boards Not Trees
date: 2012-10-29
tags:
  - evernote
---
To increase the value of social networks, interactions should be based not on
individuals but on relevant fractions of individuals.  
  
For example, when I see an interesting post on 3D printing from someone on
Twitter and follow them, I may then see 1000 posts about political candidates,
cats or other things that I don't care about from the person I just followed.  
  
All current social networks function in this way, filtering at the level of
granularity of the individual contributor. The result is harmful on multiple
levels, the first being an overall increase in ongoing filtering effort (the
need to assess and reject content of those you follow manually). The second is
unnecessary alienation due to personal-level incompatibilities that become
apparent unnecessarily due to forced out-of-context information sharing.  
  
Instead, a better design would create associations between publishers and
subscribers which are sensitive to the context of the establishment of the
relationship. Instead of "following" or "friending" a person, the act of
selecting an individual post for quality establishes an interest in a subset
or context of the publisher.  
  
I call this "boards not trees"; consider the case of a carpenter building a
cabinet. The carpenter could (in theory) build the cabinet by acquiring
several whole trees, but instead it makes more sense for them to acquire only
a subset of the tree in the form of a board that can be used directly, or
perhaps broken-down with little waste into the parts that make up the cabinet.  
  
In current social networks we take the opposite approach, and establish every
relationship directly with other members of the network. Google+ comes close
to getting this right, and to some degree this goal can be accomplished
manually but to be truly useful the context establishment needs to be
automated.  
  
Returning to my original reference to a post about 3D printing on Twitter,
it's not hard to imagine a primitive system that would be capable of
identifying the context of my interest by examining keywords (or better yet,
hashtags) in the selected post and then use this context to present future
posts that are likely to be interesting to me from this user (and nothing
more). Taking a (small) step further, future feedback on these suggestions
could provide positive or negative reenforcement and a basic neural network
would be capable of automatically keeping the feed in line with my interests,
even as they change, providing quality content with minimal manual filtering.  
  
Moreover, this same system could be used to provide feedback to the author to
steer them toward topics of interest (perhaps even suggest topics when input
is solicited) and by analyzing the response to this input, establish better
overall value rankings compared to the simplistic systems currently in place.  
  
I've been noodling on what the implementation of a system of this type would
look like. If you'd be interested in participating in the development and
testing of such a system, leave a comment below.

---
title: Boatage
date: 2004-06-05
tags:
  - evernote
---
I like water, so does my lovely bride and our beautiful baby girl.  
  
However I've never been the boat type, I don't really have any interest in
water skiing or any of that. I like the idea of being on the water but I don't
dig the complication of traditional watercraft.  
  
So when I saw these things that look like a cross between a jet-ski and a
fishing boat, I got interested.  
  
I like the idea of having something that the three of us could take out on a
lake for the weekend without having to give up my entire garage and all my
free time maintaining.  
  
Now I know nothing about this stuff, so I'm still doing homework. I'm open to
other ideas, it would be cool to get something that could go on the roof of
the new ride, but I don't think a canooe would work for the little one.

---
title: Bringing the bike home
date: 2007-05-20
tags:
  - evernote
---
[
![](http://lh3.google.com/image/jason.gullickson/Rk9oms6JA9I/AAAAAAAAAQg/rm2tGeJCCqU/s288/IMG_1993.JPG)
](http://picasaweb.google.com/jason.gullickson/CL35051907/photo?authkey=b5iVmIqEldA#5066383119914435538)
This journal is about the bike I plan to ride in the fall 2007 Crud Run. Given
to my by the father of a friend, this Honda CL350 is a beautiful example of
engineering and design prevalent in 1970's Japanese motorcycles. Today Jacy
and I picked up the bike from his parents house and trucked it back to my
house in Beaver Dam.  
  
This bike hasn't run for years, I'd guess at least 10 years if not more. At
one point Jacy began to get it back in shape (which you can see as the
carburetors and exhaust have been removed) so when I picked up the bike it
came with a nice kettle of parts to go along with it. Not quite a true "basket
case", but a good beginning nonetheless.  
  
[
![](http://lh3.google.com/image/jason.gullickson/Rk9o6s6JBAI/AAAAAAAAAQ4/kkbb7JYyYm4/s288/IMG_1996.JPG)
](http://picasaweb.google.com/jason.gullickson/CL35051907/photo?authkey=b5iVmIqEldA#5066383463511819266)
I'm still evaluating the bike and deciding where to start. With a project like
this, there's almost no "wrong" place to go to work first but if you spend
some time thinking about it you can save yourself some trouble down the road
by avoiding "re-work". Since the exhaust seems to be in the worst shape (the
mufflers in particular, the headers don't look too bad...yet) one of the first
things I'll need to do is look into tracking down replacements or alternatives
and do some cost comparisons to see what makes the most sense.  
  
Since my goal here is to ride the bike in the fall 'run, I'm willing to
compromise when it comes to restoring the bike to "stock". Ideally I'd like to
keep the bike as original as possible, to preserve its charm but I don't
expect to have a numbers-matching museum piece when I'm done.  
  
[
![](http://lh4.google.com/image/jason.gullickson/Rk9of86JA8I/AAAAAAAAAQY/M3rb0w3xCSw/s288/IMG_1992.JPG)
](http://picasaweb.google.com/jason.gullickson/CL35051907/photo?authkey=b5iVmIqEldA#5066383003950318530)  
  
Another obvious area is the fuel system, first off because one of the most
common dangers of an old bike like this is rust in the tank but also because
most of the fuel system components have already been removed which is half the
work. With a bike that has sat as long as this one you can pretty much count
on having to replace any rubber components so items like the fuel lines, etc.
are something you should start shopping for immediately.

---
title: Cache to the Future
date: 2012-12-31
tags:
  - evernote
---
**Proposal:** Use macroscopic branch prediction to cache data before it's ever
requested.  
  
[ Pipelined microprocessor architectures
](http://en.wikipedia.org/wiki/Pipelined_CPU) use a technique called " [
Branch Prediction ](http://en.wikipedia.org/wiki/Branch_prediction) " to keep
their pipelines full of instructions; without this it would be necessary to
load the instructions from memory one at a time, rendering the pipeline mostly
useless.  
  
[
![](http://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Pipeline,_4_stage.svg
/375px-Pipeline,_4_stage.svg.png)
](http://en.wikipedia.org/wiki/Instruction_pipeline#Illustrated_example)
Generic 4-stage pipeline  
  
A pipeline is something like a cache, in that it improves performance by
storing instructions in fast, on-chip memory instead of having to load them
one-at-a-time from slower system memory. This works fine when the processor is
executing instructions one after another, but when a branch is encountered (a
piece of code that can go one of two ways, like an "if" statement in a
programming language) the only way to keep filling the pipeline is to predict
which way the branch will go and pre-load the instructions predicted to be
selected.  
  
[ ![](http://upload.wikimedia.org/wikipedia/commons/c/c8
/Branch_prediction_2bit_saturating_counter-dia.svg)
](http://en.wikipedia.org/wiki/Branch_prediction#Saturating_counter) 2-bit
saturating counter  
  
When the prediction is correct, it means the next instruction executes from
the pipeline at the highest possible speed.  When it is incorrect, the next
instruction is loaded from slow system memory, but this is no worse than what
would have been if no prediction were made at all.  
  
So it's clear that at least trying to predict the outcome of a branch is
worthwhile, but what does this have to do with caching, since caches contain
data that has already been loaded?  
  
What I'm proposing is to apply the model and logic of branch prediction
techniques to higher levels of programming, and to use these techniques to
select the outcome of user decisions _before the user selects them_ for the
purpose of loading data into a cache, resulting in increased response time the
first time the data is requested.  I'm hoping to carry out some experiments to
determine the value of this technique in the next few weeks, I'll update this
post when I have some results to report.

---
title: Capo de Kappa
date: 2016-08-30
tags:
  - evernote
---
A few years back I began working on a dynamic computing platform based on
Javascript.  The idea came to me when a cluster of nodes I was using to
perform media processing grew large enough that updating the code on them
became a multi-hour endeavor.  It occurred to me that instead of simply
delivering the job data to each node, the code it executes could be delivered
via the same queue, and this way the software on each node could consist of
nothing more than an operating system and a runtime (in this case, Node.js).

  

This led to the question of administration in general, and I started to design
a way to deploy & manage nodes using the same mechanism, so that a fleet of
nodes could be managed by simply placing jobs in the queue.  Of course this
led to re-evaluating the queue, so on and so forth.

  

In the end I designed an architecture I called “JS/OS” and did some
experiments.  Ultimately I didn’t implement this as a solution to the task at
hand (some other emergency preempted completing the work) but a number of
interesting architectural patters became apparent when I explored this
approach.

  

Today as I study data science and the architectures for “big data” systems,
what I see are the infant forms of the systems I was designing for JS/OS, and
what’s exciting is that there was an even more exotic idea which derived from
that work that might be applicable to contemporary streaming “Kappa”
architectures.

  

Once, in a meeting where I was discussing the idea of having dynamic code on
processing nodes some exceptions came up about node hardware and media
processing limitations of the inexpensive computers we were experimenting
with.  Thinking along these lines I proposed that using a hybrid ARM/FPGA-
based machine might address this because an FPGA codec implementation could
provide hardware assistance.  This train of thought led to thinking “could we
make the FPGA’s code dynamic as well?”, which in turn led to the thought “What
about a cloud of FPGA’s?"

  

I thought this was fascinating, but the missing component was making FPGA
programming accessible to the average programmer.  I won’t try to explain why
here, but the key difference is that FPGA logic happens immediately, signals
go in, signals come out.  This is a lot different from how software tends to
work, with internal state, looping structures, etc.  So a cloud of FPGA’s
would be cool, but it would require specialized programming skills (and a
skill-set that most cloud-oriented developers lack).

  

But what if we could make it approachable?  There is some design overlap
between the stateless design patterns popularized by work centered on
implementing REST API’s in Node.js.  If you could describe FPGA programming in
these terms, it’s conceivable that typical programmers could take some
advantage of an FPGA cloud.

  

However even this seemed too specialized to pursue, and in any event, there
was another emergency to deal with so I set it aside.

  

This is the part of the story where it all comes together.

  

While studying Hadoop and other big-data processing architectures I started to
see similarities to the problems and solutions I had explored while working on
JS/OS.  Furthermore, the stream orientation of kappa architecture systems
dovetail nicely into the stateless, stream-oriented approach I had in mind for
applications suitable for a “cloud of FPGA’s”.  This being the case, an FPGA-
enabled JS/OS might not just be a suitable platform for kappa architectures,
but it could provide orders of magnitude leaps in performance, if it can be
realized to its potential.

  

I’m going to set this thought on the back burner (but still on “simmer”) while
I continue to study the current state of big-data infrastructure, but if
nothing I learn invalidates the idea, I might invest some time in revisiting
JS/OS, even if it means changing the name due to implementation details  😃

  

---
title: Ch..Ch..Ch..Changes
date: 2000-11-28
tags:
  - evernote
---
Ok, so I made some changes around here, I dropped the ads because frankly they
looked like hell and we not making any money, so screw them. From now on I'm
only putting up links to sites that link here, so if you wanna make an
exchange, I'll make you an offer you can't refuse (well, you could refuse, but
what would be the fun in that)?  
  
Instead of ads, I decided I needed a logo or a mascot, and since I lack the
talent to create such a thing, I went surfing and found these:  
  
  
  
  
  
  
  
I got these from [ The Orphanage of Cast-Off Mascots
](http://www.lileks.com/institute/orphanage/index.html) , and I'm considering
adopting one of them as the logo/mascot for this site, what do you all think?  
  
The future mascot of jasongullickson.com  
  
I pick you,  
Your Age:  
Your Email:  
  
I'll keep this one up for a week and incorporate the winner next week.

---
title: Cherrypicking
date: 2016-08-03
tags:
  - evernote
---
I’m going through my original Postach.io notebook and re-posting some things
that still seem interesting or relevant.  Over time (assuming the platform is
stable) I may even re-introduce some content from other blogs of the past,
just so that the content is published and available to others should it come
in handy.

---
title: Choose your own presentation
date: 2016-07-23
tags:
  - evernote
---
A different presentation style that begins with questions from the audience.  

  

Instead of a linear progression through a predetermined presentation.  The
presentation is broken into sections which correspond with the key topics or
points of interest.  When the presentation begins, the presenter asks the
audience for questions (presumably they are at the session because they have
questions on the topic; if not the presentation can be done in traditional
linear fashion).  Questions that are provided by the audience are then
answered by the presenter, with the aid accessing the pertinent components of
the presentation in random-access fashion.  

  

This allows the presenter to prepare in advance for the presentation while
also accommodating the needs of individual audience members to get their
questions answered.  This also allows the audience to indicate their interest
level in various areas of the subject matter, allowing the presenter to
improve the presentations over time by increasing information in areas where
more curiosity lies and pruning areas of the presentation which represent less
interest.  

  

This is also useful in gaining an understanding of the interest levels of a
group.  If used in a classroom, this method allows the instructor to gain
insight into the interest and experience level of the class on the subject.
This can then be used to enhance or alter focus of presentation of the
material to ensure that key concepts are delivered with enough emphasis
without over emphasizing things that are well-known territory for the group.  

  

This method could be carried out with existing presentation methods, but it
could be enhanced by software that makes creating a "choose your own
adventure"-style presentation easier.  It would also be helpful for the
presentation software to gather metrics during the performance of the
presentation to capture what sections are presented and how much time is spent
on eacy.  Deeper functionality could be incorporated, such as audio/video
capture of the questions and topic coverage.  This would make it easier to
enhance the presentation as well as collect new questions to be answered.  

---
title: Cloning Open-Source Hardware
date: 2013-11-05
tags:
  - evernote
---
"Cloning" is a term used to describe the process of creating a product by
simply copying an existing Open-Source design without making any changes.
Typically this is done in a way that reduces the cost of the product through
less expensive manufacturing processes.

  

This is generally frowned on by the open-source community, and is considered
by some to be an abuse of the freedoms provided by open-source licensing.

  

I have mixed feelings about this stuff, but I lean toward it being more
important to increase access than it is to protect the profits of a specific
company.

  

There's two sustainable ways to look at this:

  

1\.  The first is that I think people who can afford to support the original
creators should do so, if nothing else it's our way to reciprocate the
advantages we have.

  

2\.  The second is that good open-source hardware maintains competitive
advantage through continual innovation, not hiding behind artificial
protection mechanisms.  I don't know what Arduino these boards are based on,
but I doubt they are the latest tech from the Arduino team.  As long as
Arduino keeps innovating, there will be a market of makers who can afford the
new stuff to support them.

  

Profiting off legacy designs is an old-fashioned business model that was
required in the days of years-long tooling and decades long design periods.
Let's move on, and in doing so empower those with fewer resources to become
part of the community so they can come up with the next generation of open
source tech.

---
title: Coding by the moonlight
date: 2009-06-04
tags:
  - evernote
---
  
...and watching "Alfred Hitchcock Presents" in the background...two bugs
down...any more to go?  

---
title: Coming home
date: 2012-06-04
tags:
  - evernote
---
After years of wonderful service the future of [ posterous.com
](http://jasongullickson.posterous.com) (my current blog host) is coming into
question due to an acquisition by [ Twitter ](http://twitter.com) . In light
of this I will finally be getting around to bringing my blog back in-house,
here to Gullickson Laboratories.  
  
This isn't the first time I've hosted my own blog, in fact my first blog was
hosted at jasongullickson.com, on blogging software I wrote myself before the
widespread use of the word "blog" existed. Unfortunately little remains of
that initial series of posts, but if I ever find them I'll add them to the
backlog here.  
  
After the original Skylab fell (thanks to Charter Communications shutting down
inbound traffic to port 80), I switched to hosting my sites elsewhere and at
one point off-the-shelf blogging software was good enough that I stopped
hosting that content myself. This lead to livejournal, blogger, Wordpress and
eventually posterous.  
  
Posterous was so easy that I was hooked after day one. Needless to say when I
brought [ gullicksonlaboratories.com ](http://www.gullicksonlaboratories.com)
online I saw little reason to move the blog and happily kept it hosted abroad.  
  
However the time has come, and having setup a self-hosted Wordpress site for
several others I decided it was time to do the same for myself. Perhaps as i
become more familiar with Wordpress I may expand the use of it to other
aspects of gullicksonlaboratories.com, but only time will tell.  
  
For now all future blog posts will be landing here, and I will be bringing
over any interesting posts which currently reside elsewhere (as time allows).

---
title: Crypton
date: 2016-08-19
tags:
  - evernote
---
A social network built on public-key encryption.  

  

**Objectives**  

  1. Make it easy and fun to have private conversations between individuals and groups   

  2. Make it impossible for anyone other than the intended user (system administrators, server operators, etc.) to view private information, or be coerced into sharing private information with others   

  3. Make it harder for accounts to be stolen, hacked or for users to be impersonated   

  

**Description**  

Public key encryption a great way to keep information private, almost the best
(the best being [ one-time pad) ](https://en.wikipedia.org/wiki/One-time_pad)
.  It's a little bit of work, but for one-on-one communication there are tools
that make it almost as easy as using no encryption at all.  However, due to
the nature of individual-specific keys it's not great for sharing information
privately with more than one person.  

  

I've thought a lot about how to leverage the power of public-key encryption in
an environment where people share information with not only one another, but
also with groups of trusted friends.  I wanted to preserve the privacy of
encrypting information client-side (before it goes out over the network) and I
wanted to avoid any possibility of unencrypted data being stored on a
somewhere other than on the computer of the users it is intended for.  I
essentially wanted the same level of privacy you get from sending messages
using OpenPGP, but also the utility of a basic forum or social network.  What
I came up with is something I call "Crypton"*.  

  

For the user, the experience is similar to familiar systems like Twitter or
Facebook with the exception that a user needs to have a GPG keypair setup in
advance.  The public key from this pair is then used to authenticate to
Crypton.

  

When an account is setup, a new user supplies their public key to the system,
which in turn sends them a message including a link to log-in to the site.
Once logged-in, the user can send direct messages to other users, or post
messages to a feed which is visible to everyone on the user's "trusted
friends" list.  The user can view messages sent directly to them, and can
browse a feed containing messages posted by their trusted friends.  All of
these messages are encrypted before they leave the user's computer, and are
only decrypted when they are displayed on the user's computer.  Any data
stored elsewhere is stored _in encrypted form only_ .  

  

**How does this work?**  

When a user sends a direct message to another user, it works just like sending
an OpenPGP email: the message is encrypted using the public key of the
destination user.  However when messages are posted to the feed, we have to do
things a bit differently.  

  

When a user posts a message to their feed, the message needs to be readable by
everyone in their "trusted friends" list.  The only way this is possible is if
the message is encrypted using each friend's public key; so this is what is
done.

  

When a user posts the message to their feed, it is encrypted multiple times,
once for each member of their friends list, using the friends public key.  The
system then displays these messages to the viewer of the feed so that only the
messages encrypted with their key are displayed, which gives the appearance of
a shared feed.  

  

**Challenges**  

One of the first issues that comes to my mind with this approach is the impact
of changes to the trusted friends list.  In the system described, a friend
added to the list cannot view messages posted to the feed in the past, as no
copy of the message was encrypted for them.  This may be desirable, but it's
different from how existing systems work so it's worth noting.  There may be
work-arounds for this, but it would require some careful thought to do so
without creating situations that violate the premise of the platform.  

  

The more troublesome issue occurs when someone is removed from the trusted
friends list.  Since messages are encrypted with the users public key,
technically friends removed from the list can still decrypt messages sent and
posted before they were removed.  Again this may or may not be desirable, but
it may differ from what's expected.  This could be managed by the system via
deleting the copies of posts and messages encrypted for the removed friend,
but this opens up the issue described above should the user be added-back to
the trusted friends list.  

  

Another potential challenge is sharing, because once a message has been
decrypted by a trusted friend, it's contents could be re-posted by that friend
and made accessible to users outside of the original author's trusted friends
group.  There is probably no mitigation for this and hence the term "trusted
friends" is used throughout this document, because users on this list should
be limited to people trusted not to do things like that.  

  

Other complexities like this potentially arise when considering how comments
or responses to posts might work, or how to integrate features beyond basic
messaging and posting in a way that maintains the principle privacy of the
system.  

  

Finally, there are implementation details that have not been discussed that
would allow the system to work efficiently.  Given the amount of duplicate
data, there may be ways to avoid the exponential resource consumption that
could be associated with a system like this if this was overlooked.  I have in
mind several strategies to mitigate these concerns but I won't go into them
here.  

  

**Going Further**  

The system described provides basic functionality for a social network system
with strong privacy, but it doesn't have a lot of features.  This was done
deliberately to focus on what makes Crypton different and keep the explanation
as simple as possible.  

  

That said, it's easy to imagine additional features that would make Crypton
more useful.  The first one that comes to my mind are "scopes" that allow the
user to group their trusted friends so that messages can target subsets of the
friends list.  It's easy to imagine how you might want to share some
information with family and other information with co-workers, but not both,
etc.  

  

Something else that isn't discussed is discovery, or how one builds a list of
trusted friends.  Given the nature of GPG and GPG users, it seems like the
starting point would be the ability to allow a user to share their GPG keyring
with the system so anyone the user already communicates with via OpenPGP
messages could be added to their trusted friends on Crypton.  Given the
encrypted nature of the system, traditional means of searching content to find
friends is less of an option, but there may be ways to accomplish that for
users who opt-in to participating in an index, etc.  

  

**References**  

  * [ https://openpgpjs.org/ ](https://openpgpjs.org/)   

  

  

* Names are hard, I'm sure this is already in use, it's just a codename   

---
title: Crypton Part 2
date: 2016-08-25
tags:
  - evernote
---
I've managed to spend a little time working on Crypton.  I've created a Github
repository (https://github.com/jjg/crypton) which at the moment consists of
some slightly more detailed implementation notes and some experiments with
openpgp.js.

  

Things are progressing even if there's not a lot of code to show for it yet.
I've been modeling the system in my mind and working through various workflows
and came to a few conclusions that resulted in significant departures from my
original design.

  

For example, I started by defaulting to implementing a REST API for the
server-side of the system, but after realizing that the bulk of the system
resides in client-side code, and that a communication system should be as
"realtime" as possible, I'm leaning toward implementing a pure websockets
interface to the server.

  

I used to have to write code and experiment with it to reach these kind of
conclusions, but I've noticed over the last couple of years that I can do more
and more of this in my head.  This is helpful because I don't have a lot of
time in front of the terminal to work on my independent projects, and it's
also faster than typing it all out by hand.

  

There's still a long way to go, and I'm still working to round-up a small
group of alpha testers, but I'm excited to work on the project, and I with any
luck I'll have something usable to experiment with before the end of the year.

---
title: Crypton Part 3 - Why?
date: 2016-09-02
tags:
  - evernote
---
I feel like the reasons for creating something like Crypton are self-evident,
but that's probably only true for people who are familiar with encryption
technology or who understand the risks of deligating responsibility for your
privacy to others.

  

So instead of assuming, I thought I'd talk about some of the reasons I'm
motivated to work on Crypton.

  

I want to make it impossible for users privacy to be violated because someone
gains control over my server.  This could be a nefarious hacker, a government
agency, or it could simply be a programming mistake.

  

By encrypting everything before it is sent to the server, and by only
decrypting it after it's delivered to the user's computer, the only thing a
hacker or cop can get from the server is unreadable encrypted data.

  

Other services claim to store user data securely, but they generally have
access to the keys necisary to decrypt the data.  In Crypton, the user
supplies the encryption keys, and they are never sent to the server.  This
means the user has complete control (and also responsibility) when it comes to
their encryption keys.

  

Another reason I want to build Crypton is because existing encryption systems
are designed for one-on-one communication, and I think there is a lot of value
in the ability for groups to interact privately.

  

By automating the process of encrypting and sharing messages with a trusted
group, Crypton provides the convenience, sharing and collaboration environment
of a social network with the strong privacy only public-key encryption can
provide.

  

Another interesting side-effect of the Crypton architecture is that most of
the compute-intensive operations are handled by the client.  This means that
it's possible to support more users per server than in other systems.  I'd
like to combine this along with my current studies in "big data" architecture
to design Crypton to have the ability to scale affordably.

  

I don't expect most people to start using Crypton right away, and I'm
deliberately designing the initial versions to require users who are familiar
with public-key encryption.  I think it is possible to grow the system into
something that could be used by a more general audience, but it's more
important to me to make sure that it serves people well who need something
like it today, and once that's perfect we can move on to mass appeal.

---
title: Das Keyboard Model S Ultimate
date: 2012-12-01
tags:
  - evernote
---
What is there to say?  If you need this and you already know about it, you
know it's the right arrow for your quiver.  
  
But for the uninitiated, the [ Das Keyboard Model S Ultimate
](http://refer.ly/a2ym) is a proper full-sized keyboard featuring buckle-
spring mechanical keyswitches.  What does that mean?  It means it feels like a
finely-crafted instrument to type on, with both audible and tactile feedback
for each keystroke.  
  
The Ultimate takes the sensory experience to a higher level by eliminating
legend from the keys entirely, forcing you to use your sense of touch and
well-trained hands to find the right keys and never take your eyes off the
work.  
  
It's hard to over-emphasise how good it feels to type on this keyboard.  It's
akin to using a [ finely-crafted power tool ](http://refer.ly/abWS) , or a [
German Super-8 movie camera ](http://designspiration.net/image/477145408410/)
, or an [ Italian-made firearm ](http://en.wikipedia.org/wiki/Beretta_92) .  
  
I've only spent about 15 minutes typing on the Model S so I'll have to provide
a more complete review after a few million more keystrokes but the initial
impression is that this is an amazing tool that I can't believe I waited this
long to try.  If you spend the majority of your time working with a keyboard
you owe it to yourself to take a Das Keyboard for a spin.

---
title: Day 5 & 6 #wwr
date: 2011-07-08
tags:
  - evernote
---
It's early in Day 6 but...  
  
  

Yesterday I was very tempted to stray from the course.  I was talking with
some friends about [ Week Without Records
](http://jasongullickson.posterous.com/a-week-without-records) and the more I
talked about what it meant the harder it was to maintain.

  
  

I'm trying to capture the ideas that arise through this exercise, it has given
me understanding about music and the music industry that I never had before,
but there's so much, I'm having a hard time getting it all down in a
communicable way.

  
  

I should probably just focus on getting it down in any way possible and then
sort it out later.

  
  

In the meantime be sure to check out the [ final Space Shuttle launch
](http://www.space.com/12046-nasa-space-shuttle-final-flight-atlantis-
sts135.html) , it's scheduled for today but weather may delay it a bit.

  
  

Anxiously awaiting Sunday...

---
title: Desktop Manufacturing Part 1 - Revolutions
date: 2012-10-02
tags:
  - evernote
---
A lot of people have compared the rise of desktop 3d printing to that of the
personal computer revolution of the 1970's, myself included.  It's a logical
comparison; they both are the commoditization of existing industrial
technology by ambitious do-it-yourselfers who reverse-engineered million-
dollar devices in their garage workshops and basement laboratories.  
  
It's also easy to predict that the two technologies may follow the same
product arc, from basement to small collection of niche companies and then on
to one or more "standards" sold by large-ish companies grown from that same
primordial vat.  
  
Having given this much thought however I think that this is an incorrect
analogy, and I believe that a more accurate parallel can be drawn; not between
the desktop 3d printer and the personal computer, nor even the laserprinter
(closer but there's a better one); I think the best analogy is between the
destop 3d printer and the digital video revolution of the early 2000's.  
  
This relationship occured to me this morning when I was thinking about the
directions various printer projects and manufacturers have taken in an effort
to introduce more people to the technology.  A lot of these efforts have been
in the interest of improving the device; its accuracy, speed, reliability and
ease of setup & maintainance.  Some attention has been put into the software
side of the system as well but most of these efforts again address
deficiencies in the system that are important to the existing user base, the
"experts" in this model.  
  
When it became practical to record digial video on video cassette tapes the
benefits of this technology were obvious to everyone working with video on a
daily basis.  The capability to easily capture footage from tape into a non-
linear editing system without generational loss was a godsend and adoption of
the format was switft in the professional and semi-pro circles who were able
to take advantage of it (those outside of technical constraints on format).
Several software companies sprang-up to provide tools for consuming these new
formats and manufacturers began to produce hardware specifically for
professionals workign with the format as well.  
  
Over time, along with the professionals some consumers desired to adopt a
digital format as well, treating it as a more durable version of previous
analog formats and with this some consumer-grade equiptment was produced.
This lead to the introduction of the [ Mini-DV format
](http://en.wikipedia.org/wiki/Mini-dv) .  The price of these units ranged
between $500-$1000 and were generally not used along with editing software due
to the cost of the sophisticated professional-grade software avaliable (along
with the sophisticated skill set these tools required).  
  
Then came [ iMovie ](http://en.wikipedia.org/wiki/Imovie)  
  
Apple released iMovie for the iMac in 1999 with a simple ad campaign
demonstrating that "professional" results that could be had with nothing more
than placing a Mini-DV camera and iMovie into untrained hands.  iMovie worked
because it allowed the user to exert just enough creative control on the
output to create a sense of ownership and a feeling of creativity while still
constraining creative options sufficiently to almost guarantee sucess in a
time investment that was realistic for the untrained user.  
  
This application alone drove not only iMac sales (as iMovie was bundled with
these machines, which in turn possessed the necissary FireWire port common to
Mini-DV cameras) but it also created an entire new class of media of
sufficient quality to be consumed by the masses (a step above classic "home
movies") but created by the untrained.  
  
Hand-in-hand with iMac sales, camera manufacturers stepped-up to meet the
needs of these new filmmakers by producing cameras and related equiptment
geared toward the casual or non-pro user and as a function of this increased
volume, per-unit device prices dropped.  
  
Over time iMovie has evolved to provide more and more sopisticated results,
raising the quality bar of the program's output without raising the learning
curve for new users.  Features such as " [ The Ken Burns Effect
](http://en.wikipedia.org/wiki/Ken_burns_effect) " which would seem absurd in
a professional editing program are amoung the most used in iMovie projects
(and by implementing this as a semi-automatic feature, some tastefulness can
be preserved).  KBE can be acheived in almost any video editing program as a
series of filters, effects and keyframes but the one-click nature of applying
the effect in iMovie creates an instant "ah!" result in the user which
following a long chain of instructions (and usually several failed attempts)
cannot.  
  
Over time, amateur filmmaking has outgrown the old iMovie and a new iMovie has
sprung-up in its place, and other digital video formats have replaced Mini-DV
but an entire new class of video producer and consumer has sprung up around
this event (you might even consider [ YouTube ](http://www.youtube.com) a bi-
product of this) and entirely new markets have been formed where before there
was none.  
  
When I look at desktop 3d printing's history, I see similar lines.  The 3d
printer is unlike the personal computer in that the device itself isn't the
"blank canvas" that the [ general-purpose stored-program computer
](http://en.wikipedia.org/wiki/Stored_program_computer) was; it is a means of
translating ideas inside the GP computer into the physical realm.  This isn't
a knock against the device, to the contrary as mentioned above it is similar
to the relationship between the laser printer and traditional paper
publishing.  It's hard to believe but before 1984, if you wanted print-quality
flyers or a newsletter or the like you were still taking the work to a
commercial publishing house who would print these items for you (but only in
large quantities), and the idea of printing even a 'draft' of the work to
verify the layout was unimaginable.  The invention of the affordable ($2500)
laser printer made the capability of generating print-grade output at home a
reality.  
  
But even the mighty laser printer was limited in value until it was paired
with a [ WYSIWYG ](http://en.wikipedia.org/wiki/Wysiwyg) editing system which
first arrived with the [ Apple Macintosh
](http://en.wikipedia.org/wiki/Apple_macintosh) .  In a few short years this
combination replaced all small-to-medium-scale print publishing, carving a
market out of the commercial sector and placing it on the desks of individuals
and small businesses.  
  
This is where the 3d printer is different from the laser printer.  At the
moment there is no significant commercial market for producing 3d objects at
the demand of the individual or small business.  There are a handful of
companies that cater to this audience, but as in the case of digital video,
_most people don't know they need it until they have it in their hands._ The
pairing of an affordable ($2500), reliable desktop 3d printer with an
empowering editing system (along the lines of iMovie) is what will open the
door to a new market of individual and small-business creativity, breeding
predictable and unpredictable marketplaces alike.  Devices within reach of
this goal exist now, however as of this writing a creative tool that strikes a
balance between creativity and automation, targetted at the right audience has
not yet appeared.  
  
It is my intention to investigate the creation of such a tool and to lay the
foundation for its creation, to erect it on my own or to participate and shape
the work of others toward this goal.  As [ recent events have demonstrated
](http://hackaday.com/2012/09/20/makerbot-occupy-thingiverse-and-the-reality-
of-selling-open-hardware/) , the power of this new tools while within grasping
distance of the general public is also at risk of slipping back into the
professional market (akin to Unix and the [ workstation
](http://en.wikipedia.org/wiki/Next_Computer) , which due to cost and
complexity remained accessible only to professionals for decades) if no action
is taken to drive the technology into the hands most likely to find new and
innovative applications for it.  
  
  
  
_note: if you find any factual errors in this post please leave a comment, I'm
writing from memory in an effort to get the thoughts down before they
evaporate but will gladly make corrections where necessary_

---
title: Ebay
date: 2007-05-25
tags:
  - evernote
---
I think [ Ebay ](http://ebay.com) is the number one reason owning a motorcycle
like this is possible. When I say "a motorcycle like this" what I mean is
owning a bike that is old enough to qualify for "classic" plates without
spending more money on getting it running than you would pay to buy a similar
new bike.  
  
For example…  
  
When I was a teenager my first bike was a [ 1976 Yamaha XS650
](http://beaver.vinu.edu/650.htm) (the "Custom", I believe). I bought the bike
for $125.00 from an ad in the Janesville Gazette. The bike was drivable and I
was able to ride it home that day. When I got there I started going over the
bike to see what needed to be done. All the typical items (tires, battery,
etc.) looked like they would need replacement but in addition there were a few
things I didn't expect as well. In particular, the lever for the choke was
missing.  
  
Now this is a tiny piece of metal with a little bit of a rubber/plastic handle
on it and a mounting screw. Based on this I figured it would cost about $5.00
at most so I called up the local parts shop to have them order it. You can
imagine my surprise when the total came to about $40.00.  
  
That's just crazy!  
  
As it turned out, this was just the beginning of the pain as the bike would
later need a new airbox, rectifier and other various items which had they been
ordered directly from the parts place would have easily put the cost of the
bike over the $1000.00 mark, and that's before replacing the tires and the
battery.  
  
Needless to say buying these parts just wasn't in the cards on my Musicland
salary and I had to improvise, leading to additional "learning experiences"
down the road; but I digress.  
  
Had Ebay existed back then, things might have been different.  
  
The bike I'm working on now, my CL350, came with a few "questionable" or just
plain broken parts as well. I noticed that in addition to the rust the exhaust
system has suffered there was also a bracket that was just plain broken. It
might be possible for me to fabricate a replacement but it just so happens
that I was able to find a NOS (New old-stock) piece on Ebay for about $5.00
(yes I see the irony in the pricing). The stator cover was also damaged at
some point (from the looks of it, pierced by the shift lever when the bike
went down on the left side). The cover has been repaired with what appears to
be JB Weld, but since I was able to find a replacement one for $6.00 on Ebay I
picked it up, I'll keep the original as a spare.  
  
[
![](http://lh6.google.com/image/jason.gullickson/Rk9pPc6JBDI/AAAAAAAAARQ/2o0HqyZmp7Y/s288/IMG_1999.JPG)
](http://picasaweb.google.com/jason.gullickson/CL35051907/photo#5066383819994104882)
I was also able to find a NOS shift lever (the one I have is still a bit
crooked and missing the rubber bits) but at $15.00 it's just a little more
than the part is worth to me, at least for now. I'll wait until a better bit
comes along.  
  
I'm sure that Ebay is just as much a boon to those selling these items as it
is to people like me buying them. Now that box of parts in their garage can be
turned into cash for other projects, and I know I feel better about
contributing to that than the insane markup of a parts dealer.

---
title: Electric Cafe
date: 2016-08-03
tags:
  - evernote
---
A chain of slow-food restaurants which provide charging services for electric
vehicles.

  

Electric Cafe objectives:

  * Combine charging with dining to give drivers something to do while charging their vehicles 
  * Link metropolitan centers by providing strategically-located charging based on typical EV range limits and distance between large cities 
  * Provide both high-quality, locale-oriented dining alongside a consistent, reliable experience for both adventurous and conservative travelers 
  * Rejuvenate rural & small towns by drawing EV traffic out of cities 
  * Tap into rural renuable energy sources like wind, solar and hydro 

The number of electric vehicles and electric vehicle owners in the U.S. are
growing each year.  While EV range is increasing, many of these vehicles are
limited to around 100 miles per charge.  This, combined with "range anxiety"
means most of these vehicles are purchased by city dwellers doing most of
their driving within city limits.  While charging stations are expanding in
urban areas to meet this need, drivers often resort to fossil-fuel vehicles
for city-to-city trips.  Electric Cafes placed at mid-points between these
cities would allow these drivers to take their EV's instead.

  

By strategically placing the cafes safely within range of these EV's, but
outside of the city, drivers can safely consume most of their range during the
first leg of the trip, visit the cafe to recharge and then complete their
journey to the final destination.  While in the destination city, their
vehicle can once again be charged and visit another cafe on the return trip.

  

By combining dining with charging, the time it takes to charge the vehicle is
less noticeable and by focusing on quality dining and slow-food atmosphere,
more charging time is avaliable without making drivers impatient.

  

In addition to providing city-to-city capability, the cafes can be a
destination in their own right.  By placing them in interesting rural
communities and locating them in areas that are amicable to foot traffic
(downtown "mainstreet" businesses, town squares, etc.), visiting a cafe can be
an enjoyable day-trip and can help revitalize business and culture in these
communities.  By proving a menu that is decided between locally-grown
specialties along with consistent favorites, each cafe can provide a unique
experience worth the trip.  Combine this with seasonal variation and the low
costs associated with EV travel and you can see how Electric Cafe's could
benefit EV drivers and small communities equally.

  

In terms of business model it could be rather straightforward.  The cafe
provides charging facilities using the current standards avaliable for fast
charging.  These facilities are powered ideally by renewable sources when
avaliable.  Charging incurs fees, however fees are waived for customers who
dine at the cafe (this may also apply to customers who make purchases from
associated local businesses).

  

A prototype cafe could be created on an existing restaurant site if a suitable
one can be found in the right location.  Coincidentally Beaver Dam, Wisconsin
might make for an ideal candidate location, as it is situated between the
cities of Madison, Milwaukee and Appleton.  It is also within 50 miles of a
number of interesting small communities, which would allow EV owners from
these metropolitan areas the ability to use the cafe as a way to visit these
small communities.

  

A prototype cafe as described could be used to fine-tune the product and
business model and define an implementation pattern which could be used to
build additional cafes in strategic locations.  In addition to cafes, other
related businesses could be pursued as the network grows (service centers,
vehicle dealerships, hotels, etc.).  This in turn could provide data which
could be used to improve future generations of electric vehicle design as
well.

  

By expanding the operational envelope of current electric vehicles, and
exposing EV owners to the unique qualities of rural areas, Electric Cafes
could have a significant impact on the expanded use of electric vehicles, and
in turn increase the existing benefits associated with their use, while at the
same time expanding both the electric vehicle industry, and revitalizing the
industriousness of small towns.

  

  

---
title: Electricity
date: 2007-06-08
tags:
  - evernote
---
We're going camping this weekend so I know I won't be spending any time with
the bike; I figure this is a good time to get the journal caught up even
though I don't have any pictures to post right now.  
  
Last week just by chance I picked up a battery, they were [ on sale
](http://www.fleetfarm.com/) and I figured I'd need one eventually (although I
was going to hold off until the bike was closer to running, just so I didn't
have to let the battery sit any longer than necessary). Although I was going
to tackle the carbs next, maybe I'll change directions and take a look at the
electrical/ignition systems.  
  
After giving the battery a night to charge (they say it comes charged but that
a "topping-up" is a good idea) I dropped it into the bike. The trickiest part
of this is routing the overflow hose, which is why some people don't do it.
I've been on the receiving end of bikes where the previous owner didn't think
the hose was important and I can say that it's not pretty, especially because
often the bikes charging system is located downstream from the battery vent.
But anyway…  
  
Once it was in and the terminals fastened down I had to at least try turning
things on. After figuring out the three positions of the ignition I was able
to verify that at least the base electrical system worked (headlight turns on,
neutral light goes green, horn works). The exceptions here were the turn
signals (which would turn on but not blink) and the brake light (which worked
for the rear brake but not the front). The turn signals, I know there is a
"flasher" device that I would guess in a bike this old is mechanical, so I
need to figure out where that thing is and see if it's stuck or something. The
front brake light problem is probably going to be more trouble, but for now
I'm not going to sweat it.  
  
I did give the electric starter a push, just to see what would happen and it
made some sounds. There were sounds of the engine actually turning over, and
there was an occasional sound best described as "a fork in a garbage disposal"
that used to emanate from my 1976 Yamaha XS650 electric starter. I'll leave
this alone for now.  
  
Next I wanted to check out the ignition to complete my testing of the
"essential" electrical systems. I first tried the old trick of pulling a plug
and grounding it against the head to see if it would spark when I turned the
engine over; no dice. I checked over the connections, the spark wires, the
coils, etc. and everything looked good so I figured either there was something
wrong with my test or there was something major wrong with the ignition
system. I had even peeked at the points a few days back and they looked almost
new.  
  
A few days later I received the service manual that I had procured from Ebay
and it described another ignition test that bypasses the spark plug. You pull
what I would call the "boot" of the plug wire (the part that goes over the
sparkplug) off and then try to get a spark from the wire inside the plug wire
to the head. Bingo.  
  
[ Bad plugs ](http://www.mkiv.com/techarticles/read_plug/plugdiag.jpg) ,
**duh** .  
  
So it looks like we have spark, now all we need is fuel. I had gone through
the carbs somewhat maybe a week or so ago and decided they looked good enough
to try, so the last thing I did during this session is start to hang them back
on the bike. I got as far as the right-side carb and ran out of time, so the
rest of this discussion will have to take place after the big camping weekend.  
  
I'm also looking for a cheap digital camera that I can get dirty to make
keeping this journal easier. I've decided on the [ Vista Quest VQ1005
](http://www.vistaquestusa.com/prod_pages/digital_camera/models/VQ-1005BL.htm)
, but I'm having a hell of a time finding one locally. If you know of a store
that carries these, pass that info along.

---
title: Eucalyptus
date: 2009-05-28
tags:
  - evernote
---
  
Simply amazing and beautiful example of what an iPhone app can be:

[ http://th.ingsmadeoutofotherthin.gs/eucalyptus/
](http://th.ingsmadeoutofotherthin.gs/eucalyptus/)

  

  

---
title: Excuses
date: 2001-02-26
tags:
  - evernote
---
Yeah, I know, I promised you all a new toy this week, however due to
circumstances beyond my control (ie, launching another new website), I've been
unable to deliver this week's toy.  
  
On the other hand, you could consider the new site this weeks toy, in that
case, I've more than delivered (not that I can take all the credit/blame for
this one).  
  
I guess either way, I'm slackin', but don't let this discorage you, dear
reader, I will return next week with a new toy, and it will be worth the wait,
i promise ; )  
  
In the meantime, heres a new poll to keep you busy...

---
title: Familiarities Authentication
date: 2012-05-23
tags:
  - evernote
---
This document describes a replacement for passwords which uses knowledge of a
container's contents to authenticate an agent's requests.  
  
  
  
Before we begin, a few terms:  
  
  
  
A "container" is something which contains information. This information can
take the form of simple files, multimedia or more complex constructs (a
database, discussion forum, social network, etc.). The container itself is of
no particular value.  
  
  
  
An "agent" is anything that can utilize what is inside of a container. This
could be a human user, an automated agent, an application accessing an API,
etc.  
  
  
  
An Example:  
An agent attempts to access a resource within the container. The container
first attempts to establish "familiarity" with the agent. Any properties of
the agent can be considered and compared to the containers previous
interaction logs to establish familiarity with the agent. The number of
property matches required to establish familiarity are determined by the level
of security requested by the contents of the container.  
  
  
  
If familiarity fails to secure access, a conversation begins between the
container and the agent to establish the agent's familiarity with the contents
of the container. Steps are taken to avoid disclosing sensitive contents, and
here again the number of "correct" answers required to authenticate is
determined by the level of security requested by the containers contents.  
  
  
  
This discourse is truly conversational and perhaps uses something akin to a
clever Eliza engine to engage in a "natural language" conversation with the
agent to establish sufficient knowledge of content to verify authenticity.
Depending on the nature of the contents, other conversations may be possible
(perhaps recognition of a single still image of video, or knowledge of the
primary color of a photograph).  
  
  
  
If authentication is successful, all available properties of the agent are
recorded in the container's logs to be used later to establish familiarity and
avoid unnecessary authentication conversations.  
  
  
  
The essential element to this approach is that the contents themselves define
the key, and therefore there is an automatic, natural relationship between the
key and the contents. This can be seen as similar to password systems which
require the establishment of "verification questions" for the purpose of
password retrieval, however the questions (and their answers) are tightly (and
automatically) bound to the contents of the container, not an abstract series
of generic questions and answers established by the original user of the
system.

---
title: FirefoxOS on ZTEOpen - First Impressions
date: 2013-08-23
tags:
  - evernote
---
Last night the [ ZTEOpen
](http://www.ztedevices.com/product/smart_phone/2bcf2d56-0c9a-4129-a25c-
acce58c8e502.html) handset I [ ordered via Ebay ](http://stores.ebay.com/ZTE-
Mobile-US?_trksid=p2047675.l2563) arrived (much faster than expected).  
  
http://www.youtube.com/watch?v=BitGnNyDYJk  
  
To be clear, I had pretty low expectations for this device in light of its
specs, cost and maturity of the operating system.  That said, my expectations
have been exceeded so far.  
  
This weekend I'll be spending some time experimenting with installing some of
the apps I've been working on via the [ simulator ](https://addons.mozilla.org
/en-us/firefox/addon/firefox-os-simulator/) and next week I'll have more to
say about the device, writing code for it and living with [ FirefoxOS
](http://www.mozilla.org/en-US/firefox/os/) on a daily basis.  
  
The other order of business is securing a carrier (can't just swap it with my
Pre since Verizon doesn't seem to support UMTS); that's another process that
I'll try to write-up to streamline the process for others.  In the meantime
I've come up with this:  
  
[ ![pmc](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/pmc-
300x224.jpg) ](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/pmc.jpg)  
  

---
title: First Shufflito review
date: 2009-09-13
tags:
  - evernote
---
  
AppTap reviews Shufflito: [ http://bit.ly/4dNaF8 ](http://bit.ly/4dNaF8)  

---
title: First Week with FirefoxOS
date: 2013-08-28
tags:
  - evernote
---
I've been using the [ ZTE Open
](http://www.ztedevices.com/product/smart_phone/2bcf2d56-0c9a-4129-a25c-
acce58c8e502.html) phone running [ FirefoxOS ](http://www.mozilla.org/en-
US/firefox/os/) for about a week and overall it's been a fun ride.  Here's the
highlights:  
  
**Hardware Setup**  
  
Unboxing and setting up the hardware is very straightforward.  Like most
phones, the ZTE Open has a user-replaceable battery that needs to be installed
before first use, and there is also a 4GB micro SD card that comes with the
phone that you'll need to put in.  
  
[ ![unboxing1](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/unboxing1-300x214.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/unboxing1.jpg) ZTE Open NIB OMG!  
  
Opening the case is easy, if a bit unnerving (the phone's plastic back feels
delicate, but I've removed it several times and haven't broken the tabs off
it...yet).  There is a handy little gap designed into the lower-left (looking
at the back of the phone) corner of the case that you can slide a fingernail
into and then unzip the back by sliding that nail around the lower perimeter
of the phone.  Once the bottom and sides are unsnapped, the back swings up and
away from the top edge of the phone.  
  
[ ![Jason's _IMG_0284](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/Jasons-_IMG_0284-300x224.jpg)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/Jasons-
_IMG_0284.jpg) With the back of the phone removed, the SD card can be seen
installed behind a metal clip at the bottom and the empty SIM slot above.  
  
The SIM and the SD card go in next and the only trick here is that the SD card
holder has a little swing-out metal band that is released by sliding it toward
the bottom of the phone and then lifting it outward.  This makes it very easy
to drop in the card and I'm surprised the SIM doesn't work the same way (the
SIM just slides under a similar-looking metal band).  
  
With the SIM and SD card installed the battery goes in next, which simply
tips-in from the bottom of the phone.  Button-up the back and you're ready to
boot!  
  
**First Boot**  
  
The phone is turned on by pressing the power button located on the top-right
of the case.  I noticed that if you quickly press the button the phone won't
start, but a red LED will flash near the earpiece speaker for reasons that are
unknown to me.  If you press and hold the power button, after about a second
you'll get a quick zap from the vibration motor to let you know things are
underway.  Shortly thereafter, the screen will jump to life and display the
animated start-up screen (so colorfully described by the commentators of my [
earlier post ](http://www.gullicksonlaboratories.com/firefoxos-on-zteopen-
first-impressions/ "FirefoxOS on ZTEOpen – First Impressions") ).  
  
FirefoxOS boots surprisingly fast, especially in light of the devices' limited
hardware.  Once started, a one-time setup "wizard" is engaged which quickly
dispatches with the phones initial configuration.  This is short and simple
and one of the friendliest I've encountered in open-source software.  
  
**Updates**  
  
Shortly after the phone booted-up it notified me that there was an update
available.  Since I wanted to have the best chance at a good first experience
I stopped what I was doing and applied the update (this is a simple matter of
selecting the notification and confirming the action, very straightforward).
The phone quickly downloaded the update and restarted.  
  
The reboot was surprisingly fast and I was back to the home screen in a few
minutes.  Just as I was about to get started poking around the phone another
notification came in, indicating that another update was available.  Being a
good sport I let it apply the update but now I was getting anxious to start
using the phone and was less delighted to see how quickly the phone could
reboot again.  
  
This repeated several times.  I'm not sure if the updates are not cumulative,
or if there is a bug in the update system, but all told there were at least
five updates during my first hour of using the phone.  I held off for awhile
after the third one, thinking that maybe it was a bug, but later I let it
complete and after a couple more it didn't ask to update again for a few days.  
  
**Gaia**  
  
The first app you'll encounter is [ Gaia ](https://developer.mozilla.org/en-
US/docs/Mozilla/Firefox_OS/Platform/Gaia) , which is the launcher or "shell"
of FirefoxOS.  Like everything else in FirefoxOS, Gaia is an [ HTML5
](http://en.wikipedia.org/wiki/HTML5) application written in HTML and [
Javascript ](http://www.gullicksonlaboratories.com/the-future-is-written-in-
javascript/ "The future is written in Javascript") , accessing device features
via [ Web API ](https://wiki.mozilla.org/WebAPI) .  
  
[ ![Gaia lock screen](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-27-10-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-27-10.png) Gaia lock screen  
  
The lock screen uses a unique unlock mechanism of sliding-up a pair of buttons
(camera and unlock) from the bottom edge of the phone.  
  
[ ![Camera and unlock buttons](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-27-22-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-27-22.png) Camera and unlock buttons  
  
The "home screen" is similar to other phones in that there is a bar or "dock"
of icons at the bottom and additional screens of icons that can be displayed
by swiping left.  
  
[ ![2013-08-28-07-27-30](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-27-30-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-27-30.png) The default home screen.  
  
As with Android (and now iOS), a notification view can be pulled-down from the
top of the screen which also contains a few essential system-wide controls and
indicators (WiFi, Bluetooth, Airplane Mode, etc.).  
  
[ ![Notification view shows recent events and system status
controls.](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-29-14-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-29-14.png) Notification view shows
recent events and system status controls.  
  
Swiping to the right eventually brings up the "Everything" view, which is
something I haven't dived into yet but looks interesting.  
  
[ ![Mysterious "Everything" view, need to play with that
sometime...](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-30-23-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-30-23.png) Mysterious "Everything" view,
need to play with that sometime...  
  
**Settings**  
  
I don't know about you, but one of the first things I do with a new device is
explore the available settings.  For the most part these are what you might
expect from a phone (WiFi setup, display options, etc.) but there are a few
interesting settings that I hadn't encountered before.  
  
_Do Not Track_  
  
[ ![2013-08-28-06-20-26](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-20-26-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-20-26.png) Do Not Track settings screen.  
  
This setting tells websites they should not track this phones access to their
site.  This might seem unlikely to be honored but some companies have already
committed to doing so and public policy is underway to increase this number.
You can learn more about Do Not Track on the [ Mozilla website
](http://www.mozilla.org/en-US/dnt/) or read about the technical
implementation here: [ http://donottrack.us ](http://donottrack.us/)  
  
_Git commit info_  
  
This is something you'd only find in an open-source OS, the Git commit hash
for the currently running OS code (along with the date and time of the commit)
is listed under Settings - > Device Information -> More Information.  
  
_Developer Settings_  
  
[ ![2013-08-28-06-22-22](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-22-22-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-22-22.png) Developer Settings screen
(not all options are visible here).  
  
Most phones have some sort of developer support, but there are a few in here
that make improving performance, layout and interactions much easier for
developers.  These are found under Settings - > Device Information -> More
Information -> Developer.  
  
**Built-In Applications**  
  
[ ![FirefoxOS's built-in applications](http://www.gullicksonlaboratories.com
/wp-content/uploads/2013/08/2013-08-28-06-43-51-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-43-51.png) FirefoxOS's built-in
applications  
  
As you'd expect, FirefoxOS comes with a number of built-in Apps including a
Marketplace app that makes it easy to add more.  Here's a run-down of the
built-in ones:  
  
_E-Mail_  
  
[ ![2013-08-28-06-44-19](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-44-19-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-44-19.png) The built-in E-Mail app.  
  
The E-Mail app supports multiple POP/IMAP accounts although lacks a
"consolidated inbox", so the contents of each account need to be viewed
individually.  Account setup is very straightforward and all the features
you'd expect from an email program are there, but I've never received
notification of new messages so I wonder if it doesn't check for new messages
in the background?  
  
_Contacts_  
  
[ ![2013-08-28-06-50-40](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-50-40-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-50-40.png) The built-in Contacts app.  
  
The Contacts app is simple and easy to use, its only obvious shortcoming is
that the only built-in import option is "Facebook" (fortunately there are
third-party applications that can pull in addresses from Gmail and other
sources).  
  
_Calendar_  
  
[ ![2013-08-28-06-51-18](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-51-18-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-51-18.png) The built-in Calendar app
(month view).  
  
The calendar has all the view, create, edit appointment features you'd expect
and synchronizes well with Google Calendar (I haven't tested others yet).  
  
Overall the calendar is good, but could use a few small improvements.  It
doesn't remember what view (day, week, month) was last used and when
displaying a day it starts at Midnight which means it usually looks empty (I
don't know about you, but I don't book a lot of appointments between Midnight
and 4:00AM).  Scrolling this view to the first appointment of the day, or
defaulting to say 8:00AM (perhaps configurable?) would be helpful.  
  
_Notes_  
  
[ ![2013-08-28-06-56-42](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-56-42-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-06-56-42.png) The built-in Notes app.  
  
The Notes app is nicer than its simple appearance would let on.  It supports
multiple notebooks and I get the impression from reading some reviews that it
can synchronize with Evernote (although I haven't done this yet myself).  
  
_Camera_  
  
The camera is basic and works well.  On this phone the lens is fixed-focus,
which means close-up capabilities are limited (but generally work better than
expected).  On the other hand, the camera takes pictures faster than it would
with an auto-focus lens.  
  
[ ![Example of incorrect orientation detection \(or maybe my whiteboard is
hung sideways?\)](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-09-25-02-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-09-25-02.png) Example of incorrect
orientation detection (or maybe my whiteboard is hung sideways?)  
  
The results are pretty good in good light, but I've noticed that low-light
sensitivity is very poor, more so than I would expect even in an inexpensive
sensor.  It makes me wonder if there is something wrong with the exposure
control and gives me hope that it can be corrected.  
  
The only other problem I've had with the camera is that it gets confused about
detecting the right orientation, and under some circumstances the app's icons
will oscillate between landscape and portrait modes so fast that a picture
can't be taken.  It's worth pointing out that my last Android phone had the
same issue, so it's not a unique problem to the ZTE Open or FirefoxOS.  
  
_Gallery_  
  
The Gallery (where the pictures from the camera go) is simple and fast and
offers a few nice touches in addition to basic organizational functions.  
  
[ ![2013-08-28-07-02-52](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-02-52-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-02-52.png) The built-in Gallery app
(view and edit photos).  
  
Photos can be viewed, shared and some basic editing facilities are provided
including some filters, exposure adjustment and cropping/framing options.  
  
_Usage_  
  
[ ![2013-08-28-07-06-57](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-06-57-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-06-57.png) The built-in Usage app
(monitor and set alerts for data usage).  
  
The Usage app quickly displays carrier utilization (minutes, data, etc.).
This data is also integrated into the Notification view which is handy, and
alerts can be set to notify you explicitly if usage thresholds are hit.  
  
_Clock_  
  
[ ![2013-08-28-07-07-22](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-07-22-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-07-22.png) The built-in Clock app.  
  
The clock is simple and attractive, and has a very pleasant alarm.  
  
_Calculator_  
  
[ ![The built-in Calculator app \(simple but
functional\)](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-09-25-41-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-09-25-41.png) The built-in Calculator app
(simple but functional)  
  
What can I say, I need this a lot :)  
  
_Video_  
  
Perhaps surprisingly, I haven't used this app yet.  I'm curious about what
formats might be supported but haven't had a chance yet to find out.  
  
_Music_  
  
[ ![2013-08-28-07-11-23](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-11-23-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-11-23.png) The Music app ("now playing"
view).  
  
[ ![2013-08-28-07-11-45](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-11-45-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-11-45.png) The Music app (Albums view)  
  
Simple but complete, playback is smooth and surprisingly high-quality for a
piece of hardware this inexpensive.  Playing MP3 does seem to take its toll on
the system (doing other things while music is playing slows things down) which
surprises me because I think this phone has mp3 decoding hardware.  
  
_FM Radio_  
  
[ ![The built-in FM Radio app](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-09-25-22-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-09-25-22.png) The built-in FM Radio app  
  
This one was a surprise, I've never had a phone with a built-in radio.  I
didn't think I'd get much use out of it, but until I finish writing an app for
[ my favorite music service, ](http://www.murfie.com) it's a nice way to
"stream" music without eating up data plan.  
  
_Phone_  
  
[ ![2013-08-28-07-15-19](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-15-19-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-15-19.png) The built-in Phone Dialer
app.  
  
The phone is clean, simple and works well.  I hate talking on the phone so I
haven't done a lot with this, but the speakerphone works surprisingly well.  
  
_Messages_  
  
[ ![2013-08-28-07-16-56](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-16-56-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-16-56.png) The built-in Messages app
(SMS).  
  
The texts, as you would expect, simple and effective.  
  
_Marketplace_  
  
[ ![2013-08-28-07-20-52](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-20-52-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-20-52.png) The FirefoxOS Marketplace.  
  
Probably the easiest way to install additional apps (but not the only way),
the Marketplace is pretty simple and straightforward.  Interestingly enough,
the built-in apps are here as well and can be updated or replaced (which is a
departure from other platforms).  
  
_Firefox_  
  
[ ![2013-08-28-07-19-56](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-19-56-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-19-56.png) Firefox Browser on FirefoxOS.  
  
Of course, the browser itself.  
  
**Development**  
  
Let's be honest, the thing that interests me most about FirefoxOS is the
development platform, but then again _I'm a developer_ .  
  
_It's just the web_  
  
The hardest part about writing apps for FirefoxOS is remembering that _it's
just the web_ , you don't need to know much more than that; there's a lot more
cool stuff you can do than you can do in a regular web browser but you don't
_have_ to.  Once you come to accept this, things start happening very fast.  
  
I know this might sounds obvious, but consider my case:  I've been writing
HTML5-style web apps since before that was the term for it and I still got
hung-up when I started playing with FirefoxOS due to baggage from working on
other mobile platforms.  I spent a lot of time trying to find the right way to
use standard UI elements or connect to the application life-cycle, etc. and
the bottom line is that for the most part these things are just simply not
prerequisites for writing an app for FirefoxOS.  They are good things to
learn, and can even be useful, but if you can do what you want to do in a
browser, you can probably do it _exactly the same way_ in a FirefoxOS app.  
  
The only thing you _really_ need to learn about is [ Application Manifests
](https://marketplace.firefox.com/developers/docs/manifests) , and fortunately
these are pretty simple.  I'm going to dedicate an entire post to writing apps
for FirefoxOS where I'll go into details about the manifest, but if you want
to jump right in here's a good read from the [ Mozilla documentation
](https://developer.mozilla.org/en-US/docs/Web/Apps/Manifest) .  
  
Since I'll be writing a more detailed post about app development for FirefoxOS
later, here's a quick walk-through of what I did to port one of my [ existing
iOS apps ](http://www.gullicksonlaboratories.com/ios-apps/horsepower/) :  
  
_App Template_  
  
I started with an [ App Template ](https://developer.mozilla.org/en-
US/docs/Web/Apps/App_templates) .  App Templates provide a starting point for
a few basic app types and give you enough code to have an app that will run,
then you fill in the blanks.  This isn't a bad way to get started quickly but
I felt that it is unnecessarily complicated and forces you to use tooling and
frameworks that you may not need.  Next time I'll probably forgo this step.  
  
_Flesh out the UI_  
  
[ ![UI code for the FirefoxOS version of "Horsepower" \(i.e., good-ol'
HTML\).](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08
/Screen-Shot-2013-08-28-at-8.02.08-AM-300x238.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/Screen-
Shot-2013-08-28-at-8.02.08-AM.png) UI code for the FirefoxOS version of
"Horsepower" (i.e., good-ol' HTML).  
  
[ !["Horsepower" UI](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-24-35-200x300.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/08/2013-08-28-07-24-35.png) "Horsepower" UI as it appears
on FirefoxOS  
  
There's a collection of [ standard UI elements ](https://developer.mozilla.org
/en-US/docs/Web/Apps/Design/Building_Blocks) for FirefoxOS that you'll
encounter when using the built-in apps.  These are neat and I like consistency
so I started here when building my UI.  That said, there's nothing stopping
you from using standard HTML elements for your UI or existing assets, so again
you don't need to let learning about these elements slow you down.  
  
_Test drive_  
  
As soon as you have a template (or a hand-rolled manifest) you can start
trying out your app using the [ FirefoxOS Simulator
](https://addons.mozilla.org/en-us/firefox/addon/firefox-os-simulator/) .  The
simulator is a add-on to the Firefox browser which aside from making debugging
easier, it also lets you test your app without any hardware!  
  
  
  
[ ![The FirefoxOS Simulator dashboard](http://www.gullicksonlaboratories.com
/wp-content/uploads/2013/08/Screen-Shot-2013-08-28-at-7.58.14-AM-300x227.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/Screen-
Shot-2013-08-28-at-7.58.14-AM.png) The FirefoxOS Simulator dashboard  
  
Add your app's directory or url and the simulator will try to validate the
code and the manifest.  If everything looks good, you'll see your app listed
in the Dashboard and you can click "Connect" to boot it up in the simulator.  
  
"Connect" means that the app running in the simulator is connected to
Firefox's debugger, so you can interact with and debug the app while running
inside the simulator as if it were any other web application (if you've tried
to debug web apps on other platforms you know just how cool this is).  
  
[ ![FirefoxOS Simulator debugging an
application](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08
/Screen-Shot-2013-08-28-at-8.00.01-AM-300x168.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/Screen-
Shot-2013-08-28-at-8.00.01-AM.png) FirefoxOS Simulator debugging an
application  
  
If you have a FirefoxOS device, you simply enable Remote Debugging and plug it
in.  The Simulator will recognize this and add a "Push" button next to Refresh
and Connect, clicking this button will take care of everything needed to
install the current app code on the device for testing.  
  
_Add Code_  
  
[ !["Horsepower" Javascript code that reads accelerometer data via Web
API](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/Screen-
Shot-2013-08-28-at-8.02.21-AM-300x238.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/08/Screen-
Shot-2013-08-28-at-8.02.21-AM.png) "Horsepower" Javascript code that reads
accelerometer data via Web API  
  
Once I had the UI where I wanted it I, add the necessary Javascript.  The App
Template encourages the use of the [ Zepto ](http://zeptojs.com/) framework
and since I'm familiar with jQuery I didn't have any objections.  
  
_Deploy!_  
  
Once you're happy with the app, you can bundle it up and share it a number of
ways.  One of the nice things about FirefoxOS is that there's no restrictions
on distributing this code, and so long as your manifest asks for permission,
your HTML5 app can have access to almost anything the device is capable of.  
  
In addition to simply handing your app out via email or posting it to a
website you can distribute your app in the Marketplace described above.
There's a [ number of steps ](https://developer.mozilla.org/en-
US/docs/Web/Apps/Publishing/Submitting_an_app) involved in validating the app
and preparing a developer account necessary for the submission process, but
since I haven't completed all these steps myself I'll hold off on telling you
how it's done until I write my developer-oriented post next.  
  
**What's Next?**  
  
At this point the ZTE Open is my "daily driver" and I'll continue to take
notes on how it performs in these duties, but my primary focus now is to dig
deeper into FirefoxOS as a development platform and document my experiences
with developing increasingly complex applications and delving deeper into the
device features exposed by Web API.  
  
Once I have successfully [ submitted an app ](https://developer.mozilla.org
/en-US/docs/Web/Apps/Publishing/Submitting_an_app) to the Marketplace I'll be
sharing a more developer-oriented post with more details on developing for
FirefoxOS and how the Marketplace experience compares to other platforms
(primarily iOS and Android) that I've worked with.  
  
I'm also planning a mini-post (or series of mini-posts) about finding a
carrier for this phone in the U.S.  It's been a surprisingly confusing
adventure.  
  
One more thing: I'm planning to present an introduction to FirefoxOS at [
Barcamp Fon du Lac ](http://www.barcampfdl.org/) , so if I generate any
interesting materials for that session I'll post those in a follow-up here as
well (and if you're attending, I'll see you there!).  
  
  
  

---
title: G7
date: 2003-03-25
tags:
  - evernote
---
Welcome to G7 - jasongullickson.com version 7.0  
  
This is my seldom-maintainend personal website. Here you will find pictures,
videos and ramblings from myself, family and friends.  
  
In addition I occassionally post information about projects that I am working
on or events that I'm involved in, but not too often.  
  

---
title: Galternatives
date: 2013-02-19
tags:
  - evernote
---
Last night something [ Google ](http://google.com) did something _particularly
evil_ to my daughter (more on this later), so I'm looking for alternatives to
the Google services we currently rely on:  

  

  * Gmail 
  

  * Google Docs/Drive 
  

  * Google Calendar 
  

  * YouTube 
  

  * Google+ 
  

  
I think those are the big ones.  I'm sure there are other services that we use
in a less direct way (Google Checkout comes to mind) but by eliminating the
services above I think that related dependencies will go away as well or at
least their use will be pushed into obscurity.  
  
Of all of them the one that seems the trickiest to replace is Google Docs.
There are of course hundreds (thousands?) of similar products but the ones
I've worked with have limitations that make them fall short of GD for our use
cases.  One thing that might replace GD for us (and in many ways surpass it's
capabilities) is [ iPython ](http://ipython.org) , but I need to play with
that some more to see if it will do what we need and serve all the platforms
we commonly use.  
  

---
title: GNU Digital Video Part 1
date: 2004-06-10
tags:
  - evernote
---
One of my steps in moving my home workstation to a GNU platform is to find a
replacement for Vegas Video under Windows.  
  
Starting out with my stock Fedora Core 1 system, I first downloaded the RPM
for Cinelerra. This installed without incident and ran on the first try!  
  
Not only this, but with a little tweaking (setting the recording preference to
Firewire) I was able to open the capture tool and record some video off of my
digital camcorder.  
  
This was going amazingly well!  
  
Of course, it was too good to be true. As soon as I added the clip to the
timeline and attempted to play it back, it was nothing but garbage.  
  
Ok, maybe I was getting ahead of myself with something as heavy-duty as
Cinelerra. Let's take a step back and try to get something more basic running,
Kino.  
  
Kino is not avaliable as a binary RPM, so we're going to do this the old
fashion way and download the source. The source is built using the typical
configure, make, make install cycle, but I immediately ran into a snag running
the configure script; libdv is required.  
  
No matter, we download libdv and follow the same process (configure, make,
make install)...wait a minute, there is an RPM, cool!  
  
I want to mention that Fedora has a nice pretty GUI that comes up when you
double-click an RPM. So far I've used it twice and it's been nice to me.  
  
Ok, libdv installed from RPM, let's try to configure Kino again...  
  
Hmm...same problem. Oh wait, we need libdv-devel, not just libdv (silly me!).
That's OK, we have an RPM for that as well, so let's just down that and see
what happens...  
  
...ok, now we need libraw1394-devel (I'll get the devel package the first time
this time around...) Oh boy, the link from Kino's site is 404, but we can
still get to linux1394.org. More bad news, no RPM, and no source for
"libraw1394-devel", just the source for libraw. Ok, we'll try that.  
  
./configure (ok)  
make (ok)  
make install (ok)  
  
Now, let's try configuring Kino again...sonofabitch; now we need
libavc1394-devel...  
  
Another tarball, another configure, another make, another make install...  
  
Ok third time's the charm (this is only the third time, right?). No,
libsamplerate is next up to bat.  
  
(on an unrelated note, I like seeing things like "fortran compiler" when a
./configure script runs, it's just entertaining).  
  
...and wouldn't you know that my damn camera battery just died...  
  
Anywhoo, libsamplerate configures, makes and make installs without error, but
Kino's configure script still can't find it. It's looking for samplerate.pc
and suggests adding it's directory to PKG_CONFIG_PATH. Ok I have a crazy idea;
this file exists in the directory where I un-tared samplerate; what if I ran
the Kino configure script from this directory?  
  
no dice.  
  
OK I'll add this directory to the path...swing and a hit!  
  
So now we're configured, let's try to make kino! (for every compilation,
churn, churn churn). Holy crap this is taking a long time...Ok, make is done,
now make install!  
  
Ok, now that we're installed...what? Well, let's start the app by typing kino.
Frak.  
  
"error loading shared libraries"; it can't find libsamplerate, christ. It's in
/usr/local/lib, wtf?  
  
OK, maybe when I set the PKG_CONFIG_PATH to the directory where I un-tared the
thing I broke something. Let's switch it to /usr/local/lib, reconfigure and
see what happens.  
  
Nope.  
  
So off to google and what do I find? Someone posted the exact same problem on
the kino message board, last night.  
  
I think this is a sign, it's time for bed.  
  

---
title: Google app engine coding...in Python!
date: 2009-06-22
tags:
  - evernote
---
  
  

---
title: Go print yourself with 3dna
date: 2013-07-15
tags:
  - evernote
---
[ 3dna ](https://github.com/jjg/3dna) is a [ Python ](http://www.python.org)
script which derives a 3d model suitable for printing from raw genome data
avaliable from services such as [ 23andme
](http://refer.23andme.com/a/clk/NPdh3) .  
  
[ ![First attempt to print the output from
3dna.](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/jjg-
300x224.jpg) ](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/07/jjg.jpg) First attempt at printing the output from
3dna.  
  
The physical output allows you interact with the data in a way that engages
the senses more directly that traditional visual-only on-screen methods.  
  
The idea (and name) for 3dna came from Jamie and her interest in genetics.
This led us to try [ 23andme's genetic analysis service
](http://refer.23andme.com/a/clk/NPdh3) , and when I found out that I could
download the raw genome data I knew I had to do something cool with it.
Applying 3d printing to the data seemed like a obvious choice.  
  
[ ![Raw genome data used as input for
3dna](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-14-at-2.47.59-PM-300x220.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-14-at-2.47.59-PM.png) Raw genome data used as input for 3dna  
  
It took some noodling to decide just how to translate the genome data into a
3d model (the options seemed infinite).  The method selected for 3dna came to
me while printing several different [ vase or cup designs
](http://www.thingiverse.com/thing:31722) that were created by extruding
2-dimensional shapes and altering them in various dimensions along the Z axis.
After struggling with the geometry (I am notoriously bad at basic math) I came
up with an algorithm for converting the chromosome pair data into points and
mapped these in space along a Z axis that coincides with each chromosome.  
  
[ ![OpenSCAD is used to preview the output before converting it to
STL](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-14-at-10.39.19-AM-300x211.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-14-at-10.39.19-AM.png) OpenSCAD is used to preview the output
before converting it to STL in preparation for printing  
  
The results are varied and interesting.  If printed as-is they are solid
(closed-top), but if printed hollow could produce cup of vase-like structures
(I'm considering baking this into the program in a future version).  Of
particular interest to me was to print the genome of myself, my wife Jamie and
our daughter Liberty and examine how the models are similar, how they are
different and how they might fit together.  
  
[ 3dna ](https://github.com/jjg/3dna) is [ Open-Source
](https://en.wikipedia.org/wiki/Open_source) and the code is available on [
Github ](https://github.com/jjg/3dna) .  I have a few improvements in mind but
feel free to add your own to the [ Issue tracker
](https://github.com/jjg/3dna/issues) , or if you have questions (or would
just like to try it out but don't know where to start) leave a comment and
I'll try to help you out.  
  

---
title: Gravity Generator
date: 2012-11-28
tags:
  - evernote
---
Most references to a gravity generator refer to something that creates
gravity, but this morning I was thinking about what it might look like to
extract energy directly from the force of gravity?

---
title: Great balls of cheese
date: 2016-08-03
tags:
  - evernote
---
Inspired by a video on [ Hackaday ](http://hackaday.com/) , I decided to take
a whack at building a cheeseball gun.

  

  

The original design called for a leaf blower, a PVC barrel and fittings
fabricated from modified leaf-blower accessories. Since I have a 3D printer, I
figured I could design some custom parts to mate things together.

  

  

It was a little tricker than you might expect because the outlet of my leaf
blower isn’t round but oval, so it took a little fiddling in OpenSCAD to get a
shape that looked right. I figured it would probably take a couple of tries to
design a part that fit, but that’s the beauty of having rapid-prototyping
equipment.

  

  

Of course the downside of rapid-prototyping equipment (at least the kind I
build) is that you don’t always get what you expect. In this case that was OK,
because even though the part wasn’t usable, it was good enough to verify that
the design would connect to the leaf blower correctly.

  

  

Even a second failed print was useful in suggesting improvements to the
design.

  

  

  

Finally I had a part that worked and the whole thing came together. Initial
testing showed that the design worked, although it didn’t perform up to my
standards, and of course the power system was rather awkward in the field.

  

  

Based on these findings I started working on Mark II, which eliminates the
need for the leaf blower (the most expensive part if you don’t have one) and
is designed to improve performance. Here’s a few shots of the progress so far:

  

  

  

---
title: Gullickson Laboratories
date: 2013-09-29
tags:
  - evernote
---
Experimenting with what it would be like to replace [ Wordpress
](http://www.wordpress.org) with [ Postach.io ](http://postach.io) .

  

I do find the idea of my content stored somewhere other than a platform-
specific SQL database attractive, and while I haven't verified it, I think
that having it accessible via the [ Evernote API
](http://dev.evernote.com/doc/) would be a step closer to where I'd like to
have things (as well as provide a way to integrate/migrate toward a server
less, distributed system).

  

Additionally, the Evernote tools & apps are pretty good, and other than the
lack of a [ ﻿  FirefoxOS ](https://developer.mozilla.org/en-
US/docs/Mozilla/Firefox_OS) client, they are well integrated into the various
platforms I spend most of my time on.

  

We'll see what happens.  There's still the matter of importing all of my
previous posts into Evernote and seeing what would happen, and then of course
there is the unknown aspects of long-term hosting things on Postach.io.  In
either case, I imagine a migration, or addressing of these issues is just a
few API calls away, so I won't be sweating it too much along the way.

  

In the meantime, I'll keep playing with it and see if I can get what I want
out of it and perhaps depart from the server-based CMS business altogether.

---
title: Happy Holidays...from the future!
date: 2012-12-01
tags:
  - evernote
---
[ ![image](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/11/wpid-2012-11-30_20-41-41_425.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/11/wpid-2012-11-30_20-41-41_4251.jpg)

---
title: Hmm...
date: 2009-05-07
tags:
  - evernote
---
The spring crud has come and gone and I still have that overflowing carb
issue. Poking around for a new carb (yes I'm ready to punt), I ran across this
tasty little tidbit:  
  
"What caused the overflowing was the float binding on the overflow tube. There
is not much clearance between the float and the overflow tube. If any
difficulty is experienced removing the float bowl, there is a good chance that
the float may be bent towards the tube, and, as a result, binding on the
overflow tube. Having set the float level to 19mm, and squaring up the floats,
the problem has been resolved. I hope this extra bit of insight is useful."  
  
taken from: [ http://scootrs.com/tech.cfm?tip=float
](http://scootrs.com/tech.cfm?tip=float)  
  
I've noticed that the float bowl goes on with more difficulty than expected,
so it's worth a look.

---
title: Holes in my Experience
date: 2016-09-04
tags:
  - evernote
---
It might not seem like it, but this was one of the things I was most worried
about on today's project.

  

  

  
  

  

For some reason I've been avoiding drilling holes in round steel.  I don't
know what got me paranoid about it but for some reason I've been freaked about
doing it and thought it would be harder than it is.

  

Aside from finishing this step of the project, getting over it makes it
possible for me to take steps in other projects that require the same task.  I
may finally complete the second 3D printer I started a few years back.

  

This happens to me often, getting hung-up on the fear of failing at some basic
task without really understanding why.  Most of the time the cost of failure
is low, and when I eventually just try to do it, it works out, so I don't know
where this comes from.

  

I'm going to try to identify these hang-ups more deliberately and like today,
drag them into an arena where the stakes are low so I can get over them, or
have a real reason to avoid them.

  

  
  

---
title: Home Dashboard
date: 2013-04-20
tags:
  - evernote
---
This morning I was just noodling on an idea to put some of the idle hardware
we have around the house to use.  
  
Old computers, tablets and even new things like TV's, etc. have network
capabilities and embedded web browsers. I'd like to create a simple household
"dashboard", that would display information useful around the house
(temperature (inside), weather (outside), status updates from occupants,
etc.). It would be implemented in html/javascript so that it could be
displayed on anything capable of running a reasonable web browser.  
  
Architectually, I'm thinking a static html/javascript page that gets its data
by polling other services via ajax calls. These services can be local
computers, remote servers or even devices capable of emmiting status data (or
consuming control requests) over the local network. To this end, it might be
interesting to use uPNP or zeroconf to discover these devices.  
  
I don't know of a way to do either of those things from javascript however, so
it might be necissary to create a program or script that runs on a "proper"
computer to generate the dashboard page. This could be as simple as a python
script that polls the local network for avaliable services and then creates a
menu of these devices and other pre-defined sources of information (weather
service, calendar service, etc.) that can be selected and then spit out an
html page that can be loaded on the various screens and devices that display
the dashboard.  
  
I imagine there are things like this out there already, but most of the
dashboard implementations I'm aware of are focused on business-oriented things
that are not really what I have in mind here, or they require configuration
and infrastructure that is overly complex, especially for this application.  
  
That said, if you're aware of something like this I'd love to hear about it,
and if I get around to building it, I'll be sure to post some updates.

---
title: Idealized Writing Schedule
date: 2013-12-09
tags:
  - evernote
---
I took two days off of work to work on my Reprap book.  Here’s what the
scheduled worked out to be:

  

06:30AM: wake

06:30AM - 07:30AM: breakfast for Lib, dishes & other chores

07:30AM - 08:00AM: take Lib to school

08:00AM - 11:30AM: write

11:30AM - 12:00PM: lunch & watch YouTube videos about 3D printing

12:00PM - 02:30PM: write

02:30PM - 03:00PM: pick-up Lib from school

03:00PM - 04:00PM: write

04:00PM - 05:00PM: dinner with the girls

05:00PM - 05:30PM: dishes & chores

05:30PM - 06:30PM: reading

06:30PM - 08:00PM: distraction

09:00PM - 02:00AM: write

02:00AM - 0  6:30AM: sleep

---
title: Ike
date: 2009-02-03
tags:
  - evernote
---
It's hard to think about motorcycles when it's -5 degrees Fahrenheit, but I'll
try.  
  
Matt and I have been busy capturing/logging footage and taking a step back to
look at what we have so far and how that fits into the "picture" of the film
we started out with. Some things have come out exactly as we expected and
others have exceeded our expectations in many ways.  
  
There are two ways to approach a non-fiction film, you can set out to tell the
story a certain way from the start and "force" this story in the way that you
acquire footage and information or you can pick a "starting point" and let the
film take shape as you gather media and information. I think that you have to
have the whole story in mind when you start out, because you need to be able
to envision the end of the film, but conversely I think you need to be able to
let this go if, along the way, the story reveals itself in a way that is
contradictory to your original vision. Ike said it best;  
  
"In preparing for battle I have always found that plans are useless, but
planning is indispensable."  
  
Matt and I had a very solid vision of the film when we first started shooting
in 2007, but since then so much has changed. When we started we could scarcely
imagine the resources that we would be able to gather and today we stand here
with such a rich collection of stories and images that to force them into the
mold we originally set would be to sacrifice too much. So we have spent the
last few months taking all this in and contemplating how to grow the idea of
the film, in light of all that has changed, and we've come up with something
that we are very excited about and we think you will be too.  
  
…of course I can't spill the beans here, you weren't expecting that were you?  
  
All grand plans aside there are some very basic and concrete tasks remaining
before we can consider production "a wrap". Next on the agenda are interview
sessions with yours truly and Matthew to gather updates on the project bikes,
and speaking of project bikes there is the small matter of readying them for
the spring run! At the moment the biggest obstacle for Matt is the self-
draining fuel tank and for myself, aside from a few (what I consider minor)
mechanical issues is the greater matter of getting the bike registered and
street-legal.  
  
So my next post will return to a discussion of these technical/mechanical
items, but I wanted to take a moment to update you all on the status of the
film itself. We are very excited to reveal more, and in the coming months
we'll be leaking more details. Stay tuned.  
  
BTW, for those of you who missed it on television you can see Matt and I on
Wisconsin Public Television's "Directors Cut" online and catch a brief "sneak
peek" of some of our Crud Run footage near the end of the episode: [
http://www.wpt.org/directorscut/111gullickson_cribben.cfm
](http://www.wpt.org/directorscut/111gullickson_cribben.cfm)

---
title: I'm thinking about getting into autocross
date: 2009-06-02
tags:
  - evernote
---
  
I was looking for a new air filter for my Fit and that eventually led to some
cool YouTube videos of autocross...

...more to come  

---
title: Industry Lesson #1
date: 2001-04-17
tags:
  - evernote
---
If you're going to solicit customers as a web designer- developer/whatever,
it's a good idea to have a website that works:  

[ can you find the content? ](http://www.katymarketing.com/)

  
  
I'm not trying to poke fun at anyone here, but this seems to be another "how
many businesses can I run out of my home" operation (did you find the home
inspection link? what the?).  
  
If you're going to do something like this and specialize in, oh I don't know,
five or six things, you might want to at least buy a few different domains and
build doorway pages to make it look like you're an expert or something.  
  
Maybe I'm just ranting, but this is why when I answer an RFP I come in twice
as high as some of the companies on the list....only to loose the work and
come back a year later to clean up after the web designer/home
inspector/advertising agency/accouting firm who doesn't realize that this is
software design, not a catalog, not a quilt, not plumbing and you need to
apply some real development and project management methodologies to a
commercial website, instead of the crap you see here (or here for that
matter).  
  
But I digress, who am I to critique another person's business or website, what
do I know?

---
title: Inital Entry
date: 2007-05-20
tags:
  - evernote
---
"The Crud Run Diaries" is a film currently in production that will feature a
history of the (in)famous "Slimy Crud Run" motorcycle rally as well as the
stories of two motorcyclists attempting to restore vintage Honda motorcycles
in time to make the fall run.  
  
I am Jason; this is my story.

---
title: iOS App Info Pages
date: 2012-11-27
tags:
  - evernote
---
If you're here from the iTunes App Store and you're looking for iOS App
information, please visit this page:  
  
[ iOS Apps ](http://www.gullicksonlaboratories.com/ios-apps/ "iOS Apps")  
  
I'm working on updating the links in the App Store, however Apple won't let
you update the links without posting new screenshots which I don't have yet.  
  
Thank-you for your patience.

---
title: i(patch)TV
date: 2009-09-05
tags:
  - evernote
---
  
For me one of the biggest problems with [ Apple's iPhone App approval process
](http://www.pcworld.com/article/171494/apple_app_store_needs_reality_check.html)
is that, in order to cope with my impatience, I have to find ways to distract
myself.

This time around it's awoken my continual desire to operate my own TV station.

I've had this dream for a long time.  The earliest inklings I can remember
involve converting an abandoned TV repair shop into a studio and taping shows
using [ PXL-2000 ](http://en.wikipedia.org/wiki/PXL-2000) camcorders.  Since
then the idea has stirred in my mind repeatedly, ever two years or so and each
time it comes a little closer to fruition.

In this day and age you might wonder why I wouldn't turn this ambition to
producing a [ video podcast ](http://en.wikipedia.org/wiki/Podcast) or some
other sort of " [ webisode ](http://en.wikipedia.org/wiki/Webisode) "-based
entertainment but while that make so much more sense, it lacks the romance of
beaming your productions out into the world where they can be enjoyed by
anyone with a simple, lowly television set in the comfort of their living
room.

Before tonight, the last time the idea occurred to me was when the "official"
date was set for the end of analog tv broadcast (the original date).  I
thought it would be awesome to have an analog station up-and-running the day
after the cut-off, on some popular broadcast channel, so that unsuspecting
viewers who didn't figure out the switch-over would just continue watching
analog tv, but with my station on the other end.

Apparently I wasn't the [ only one ](http://omgimon.tv/drupal/) with this
idea.

So tonight as I'm catching up on my [ Hack A Day ](http://hackaday.com/) RSS
feed I come across a [ collection of articles ](http://hackaday.com/2009/09/02
/build-an-analog-tv-station/) about setting up your own tv station and since I
haven't heard back from Apple about [ Shufflito
](http://www.gullicksonlaboratories.com/shufflito) , I'm more inclined to dig
into the tv station dream than to work through the new features I have planned
for Shufflito 1.1.  Whether or not this will amount to any action is really up
to Apple, since a three-day-weekend lies ahead and if I don't hear something
that redirects my attention I may just be strapping a [ yagi
](http://en.wikipedia.org/wiki/Yagi_antenna) to the new treehouse...

...and if I do, I'll be sure to post the frequency, [ Kenneth
](http://en.wikipedia.org/wiki/William_Tager) .

  

---
title: IPhone learning curve
date: 2009-06-26
tags:
  - evernote
---
  
[ http://broadcast.oreilly.com/2009/06/big-learning-curve-for-iphone.html
](http://broadcast.oreilly.com/2009/06/big-learning-curve-for-iphone.html)

...I don't necissarilly disagree, but I don't think it's as bad as all that...  

---
title: iPhone SDK Beta 5
date: 2009-05-11
tags:
  - evernote
---
  
Seriously getting my butt kicked by codesign after upgrading my dec tools, and
not having any luck finding solutions in the community...  

---
title: iPod Shuffle battery replacement
date: 2016-08-03
tags:
  - evernote
---
I like to fix things and my mother gave me a particularly interesting
challenge. The battery had died in her second-generation iPod Shuffle, and she
wanted to get it replaced.

  

  

Of course the first question most people ask is why; why not just replace it,
it’s certainly less work and probably even cheaper than buying the parts. That
may be true (to some degree), but she was attached to it, and she’s not the
kind of person who just throws things away that can be fixed (or improved!),
so she asked me to take a look at it.

  

I found the idea intriguing because it fits into a category of work I find
relaxing. it’s in an area of work that I’m comfortable with (electronics),
it’s not particularly mysterious (the process is well-known) but it also
requires skill and patients. It’s also a low-risk job because the device is
already “dead”, an no-one else is interested in repairing it, so if something
goes wrong the outcome isn’t any worse than the origin.

  

My go-to source for these types of jobs is [ fixit.com
](https://www.ifixit.com/) . They have the parts, tools and documentation
needed to make work (especially work on Apple products) go smoothly. Of course
I probably should have looked a little closer at the [ 2nd gen iPod Shuffle
guide
](https://www.ifixit.com/Guide/iPod+Shuffle+2nd+Generation+Battery+Replacement/3656)
, because I didn’t notice that the job required soldering until I started
prying the thing apart.

  

Like most small Apple electronics, the shuffle is held together largely by
adhesives; Apple clearly has no interest in producing serviceable hardware.
That said the device is otherwise well-made which makes repair less
treacherous than fixing something where the screws are stripped (or so cheap
the heads come off when you turn them), or the parts are forced together, etc.
Once the glued-on covers are free the rest of the disassembly is done with
triple-zero Phillips screws and flexing tiny spring clips.

  

The real trick is disconnecting the old battery and reconnecting the new one.
This requires soldering, which I don’t mind but there’s a couple of
complications to this job. The first is that the leads and pads are very
small, not quite SMD small but tiny, less than a few mm long and soldered
pretty close together on the board. The second problem is that they are
located next to a steel spring clip that is mounted at a right-angle to the
board, which makes it hard to get into the area with any tools. The third
problem is that you’re dealing with a potentially charged li-ion battery with
no internal regulation, so short-circuits could [ get exciting fast
](http://batteryuniversity.com/learn/archive/lithium_ion_safety_concerns) ,
and you’re using a conductive soldering iron to connect leads to pads on the
board which are perhaps 2-3mm apart.

  

  

  

Desoldering went rather smooth. I put the motherboard in my [ Panavise
](https://www.amazon.com/gp/product/B000B61D22/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&tag=jjg00-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=B000B61D22&linkId=6b6d2cf838b1ce97f473ef3fc49ddfbd)
and used a [ hemostat
](https://www.amazon.com/gp/product/B00EKQ7FY4/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&tag=jjg00-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=B00EKQ7FY4&linkId=235a2ded805a11355c739ee4b4563bd9)
to hold each wire in turn while heating the solder and pulling gently. They
popped-off without a lot of drama and other than being careful to not cross
the leads, it was all rather uneventful. Attaching the new battery was another
matter.

  

  

Maybe I’m just getting old, but the pads and leads were small enough that I
needed to use a jeweler’s loupe to see what i was doing while fishing the
leads of the new battery around the steel clip and on to the board. This means
having my face a couple cm from the work, and the soldering iron. I was
pleasantly surprised to not have branded myself at this point.

  

  

Once the soldering was over it was simply a matter of reversing the
disassembly steps to return the device to its original form. There were a
couple of snags getting things to fit right, and afterwards there was still a
gap on one side where I think the aluminum of the case flexed while I was
prying it apart, but the music played and the controls worked correctly, so I
can’t complain too much about the fit and finish.

  

  

Overall it was about an hour’s worth of work and a relaxing way to spend a
Saturday morning. It’s too bad that so many of our devices are designed to be
disposable, because I think there are others like me who enjoy the distraction
of this kind of work, and would be willing to do it regularly if there was any
way to make a living at it.

  

  

Rock-on Ma!

---
title: It's time to start coding
date: 2009-05-07
tags:
  - evernote
---
  
  

---
title: J-Head Mk V-BV
date: 2012-12-17
tags:
  - evernote
---
If you've been here before you're familiar with my struggles with the [ QU-BD
filament extruder ](http://www.gullicksonlaboratories.com/?s=qu-bd) .
Thankfully those days are over due to the arrival of the [ J-Head Mk V-BV
](http://reprap.org/wiki/J_Head_Nozzle#Mk_V-BV) from [ Hot-ends.com
](https://www.hotends.com/index.php?route=product/product&product_id=88) .  

![2012-12-16_14-01-54_780](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-16_14-01-54_780-225x300.jpg)

  
The first hot-end I used was the [ Makergear unit
](http://www.makergear.com/products/operators-pack) that came with my [ RepRap
Mendel Prusa kit ](http://www.makergear.com/products/3d-printers) .  It worked
well enough, but I often managed to jam it, and looking back I think the
biggest cause of this was the distance between the heating element and the
nozzle.  This can result in the filament freezing before it's able to be
pushed out the nozzle, and conversely this arrangement the heat source closer
to the "cold-end" of the extruder which can cause the filament to melt too
high up and in my case, ooze out over the top of the hot-end and encase
everything in a plastic tomb.  
  
![It's hard to see in this photo, but liquid plastic has backed-up and began
to flow outside of the extruder barrel, which is
bad.](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-04-21_13-40-10_377-225x300.jpg) It's hard to see
in this photo, but liquid plastic has backed-up and began to flow outside of
the extruder barrel, which is bad.  
  
This is what led me to the QU-BD, thinking that instead of replacing the
Makergear hot-end for almost the same price I could get two complete extruders
(and they looked so pretty!).  As it turns out, you get what you pay for.  
  
The design of the J-Head addresses these heating problems by placing the
heating element inside the nozzle itself, far away from the cold-end of the
extruder.  This design looked to me to address the problems I was encountering
so I put the QU-BD's in the parts bin, ordered a J-Head and dug out my old
Makergear extruder.  
  
The J-Head from Hotends.com arrives as a "kit", but there's really only three
pieces to assemble and the hardest part is crimping on the appropriate
connectors to the heating element (a resistor) and the thermistor.  The
resistor needs to be installed into a hole in the head and based on the
instructions on the RepRap Wiki I wrapped the resistor in aluminum foil to
ensure a tight fit (I didn't have any of the muffler compound they suggested
on-hand).  
  
Next the thermistor is mounted by simply taping it into the pre-drilled hole
with some Kapton tape and it was ready to mount to the cold end.  
  
The J-Head using the same mount as my stock Makergear hot-end so installation
was easy and I had everything back on my RepRap in a few minutes.  
  
After about an hour of updating firmware & host software (lots of new code
since the last time my printer worked) I hit the extrude button and cheered.  
  
![I've waited six months for this
moment...](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-16_16-23-08_740-300x225.jpg) I've waited six
months for this moment...  
  
I had planned to add a glass build platform as well but this all came together
so quickly I just had to try out a real print.  Here's a couple of shots from
the tests I did, there's still plenty of calibration to be done and
adjustments to be made but overall I'd say that the J-Head has out-performed
everything else I've worked with.  
  
![I had good luck with this on Steve's Printrbot so I gave it a shot with the
J-Head for comparison.](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-16_23-42-09_760-300x225.jpg) I had good luck
with this on Steve's Printrbot so I gave it a shot with the J-Head for
comparison.  
  
  
  
![This is a S-Hook I've printed dozens of times so it's a good "eyeball
calibration" piece for me.](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-17_00-43-43_702-300x225.jpg) This is a S-Hook
I've printed dozens of times so it's a good "eyeball calibration" piece for
me.

---
title: J-Head Take Two
date: 2013-02-24
tags:
  - evernote
---
A couple weeks ago I started noticing some black goo on some of my prints.
Within a few days I found the cause, the resistor that provides the heat for
my J-Head had gone critical and melted down.  
  
I did a little reading on this and I have a few ideas on ways to improve on
the design to avoid this in the future, but I've had such luck with the
standard setup I ordered a replacement so I could get back to printing and
then experiment with the old head on the side.  
  
[ ![20130223-230444.jpg](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/02/20130223-230444.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/02/20130223-230444.jpg)  
  
One thing I'm doing different with the replacement is using "muffler putty"
instead of aluminum foil to ensure a tight fit between the resistor and the
body of the nozzle, since failure at this interface can lead to premature
resistor death and may have played a role in the failure of my last nozzle.  
  
[ ![20130223-230542.jpg](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/02/20130223-230542.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/02/20130223-230542.jpg)  
  
I wasn't exactly sure what kind of putty to get, so I went with Blue Magic's
"QuickSteel" since it had the highest temperature limit of the options I had
available (the others topped out a little too close to ABS temperatures for my
taste).  
  
I'm somewhat concerned about what it might take to remove a resistor secured
in this fashion when it eventually fails, but I guess that's what drills and
Dremel's are for right?

---
title: Just posted the latest beta of the new iPhone app
date: 2009-05-30
tags:
  - evernote
---
  
I got an hour or so of coding in and fixed a few bugs. The list is getting
shorter, but it always has a way of growing near the end of each app...  

---
title: Just Wifi
date: 2016-10-19
tags:
  - evernote
---
A configure-and-forget device that provides reliable, fast, no-frills WiFi.

  

It seems like all consumer wireless access points available today focus on
features over quality.  This is probably because it's hard to market something
simple, and hard to compete from a price perspective unless you can offer some
obvious, user-facing value.  The problem is, as with most mature tech
products, this focus on features undermines the ability for the device to
deliver basic, essential functionality.

  

What I propose is a device (or perhaps a collection of devices) that do the
opposite.  A wireless access point which, once configured, can be forgotten
about.  I would like wireless networking equipment that is as sexy and
glamorous of a circuit breaker box.

  

At one time commercial-grade devices existed that met this criteria, and
perhaps they still do, but the ones I'm familiar with focus on features as
well, even if it's a different subset of features.  Most of these devices (and
their associated administration systems) provide features to manage an army of
access points and deal with the kind of complexities that exist in corporate
or industrial environments (privacy, security, backward-compatibility, etc.).
While necessary in these environments, they don't really have a place in the
home.

  

So what I'm proposing is a device which features high-quality hardware running
the simplest possible firmware to provide wireless network access.  I'm even
considering eschewing routing features to keep the device as simple as
possible (perhaps offering routing as a separate device).  The question is, if
I constrain the problem space to wireless only, what's required?

  

Minimally this will require:

  

  * An Ethernet interface to connect to the wired network 
  * A wireless radio to provide signal 
  * A bus to connect the Ethernet interface and wireless radio 
  * A cpu, microcontroller or similar to manage communication between the wired and wireless network, as well as provide a configuration interface, etc. 
  * A power supply 

  

It would seem that there may also be a need for an "out-of-band" configuration
interface (USB, serial, etc.) and some kind of non-volatile storage for
firmware (although this may be incorporated in the cpu/etc.).  It may also be
worthwhile to include some sort of status indicator to communicate the
condition of the device when a fault occurs.

  

It's tempting to use off-the-shelf hardware, and there may be something out
there that fits the bill.  I'll have to do some research but I'm not willing
to compromise the goals of the device just to make it fit into an existing
design.

  

I'm planning to try another device just because time is of the essense, but
I'll continue to work this problem and update this post as progress is made.

  

  

Reference

  * [ https://www.seeedstudio.com/LinkIt-Smart-7688-Duo-p-2574.html ](https://www.seeedstudio.com/LinkIt-Smart-7688-Duo-p-2574.html)
  * [ http://www.pcengines.ch/ ](http://www.pcengines.ch/)
  * [ http://www.anandtech.com/show/6180/open-source-router-platforms ](http://www.anandtech.com/show/6180/open-source-router-platforms)

  

---
title: Learning the hard way to accept the easy way
date: 2016-08-31
tags:
  - evernote
---
It’s a bit humbling to admit it, but I may have been wrong about my fierce
resistance to third-party modules in Node.js.

  

I started working with Node.js before NPM, and before there were any third-
party modules (that I was aware of).  Much of my excitement about the platform
was based on the fact that you were starting from scratch, all the way down to
the webserver itself.  This was exciting to me because most of my work on
server-side code was, and still is, writing fast, stateless API’s.

  

My predisposition to avoid modules goes all the way back to my experience with
C.  C was my first experience with a compiled language (I don’t think an
assembler counts) and the thing that stood out to me was it’s simplicity.  The
language was so small that you could memorize the whole thing (there are
something like ~30 built-in functions?), but with that small set of tools you
could build anything.

  

Programming in Node.js was a flashback to this feeling, the ability to build
an entire server out of pure Javascript (and in my case, limiting it to [ The
Good Parts ](http://amzn.to/2bROJ4e) ), and know every line of code that will
run when a request is made.

  

However this was less exciting to the majority of developers who were used to
working in environments like Java or Ruby on Rails which had a rich ecosystem
of modules (“gems”) that could be drawn upon for almost any need that could
arise.  So these developers quickly set about creating modules for Node.js
that re-created the structures they were familiar with, notably the web
framework Express, which made Node.js work like a typical MVC-style web
application.

  

None of this interested me so I more or less ignored it.  Over the years I
would occasionally draw upon a third-party module to do something that
couldn’t be done natively in Javascript (modules which contained binary
functionality that can’t be replicated in Javascript running inside of
Node.js), but in each case I would read every line of the module’s source code
to maintain an awareness of what code was running when requests were made to
my servers.

  

I also ignored “convinience” libraries, modules that made Node.js work more
like other environments.  I considered each as it came to my attention but for
the most part they were either shortcuts to things that could be done directly
in Javascript, or they introduced changes to the processing model which traded
programmer convenience for runtime performance.  My position on this is that
it’s better to learn how to speak to the machine once vs. forcing the machine
to conform to the programmers predisposition on every incoming request.

  

It’s in this area where I’m learning I may have made a mistake.  In
particular, there are two modules that I now believe are worth the added
complexity of including them in a server project.

  

**Underscore.js**

I’ve heard [ Underscore.js ](http://underscorejs.org/) referred to as
"Javascript's missing standard library”, and that statement alone is probably
why I ignored it for years.  Going back to C, the standard library provides
functions that already exist in Javascript, and in particular the extensions
to Javascript that already exist in Node.js.  That said, looking at that claim
from another perspective makes it less dubious.

  

Underscore is a “standard library” in the sense that it provides a standard
way for developers to solve problems that commonly occur in Javascript
programs.  It is essentially a way to short-circuit arguments on the best way
to implement these common bits of code, as opposed to providing functionality
that doesn’t exist in the basic language.

  

The difference may be subtle, but that change in perspective made it a lot
easier for me to accept Underscore’s claim of being a standard, or otherwise
necessary.

  

**Moment.js**

Time and computers don’t get along.  Dealing with time is hard in any
language, but it seems particularly painful in Javascript.  This might be due
to Javascript’s origin as something that runs on a computer in the same
timezone as the user, but I think it’s more related to deficiencies in
Javascript’s type system.  Regardless of the reason, Javascript support for
working with time is really frustrating.

  

After years of fighting with this I’ve come to accept [ Moment.js
](http://momentjs.com/) as a solution to this problem.  Like Underscore.js,
this is less about Moment.js’s ability to do things you can’t already do, or
it’s performance, etc. and more about establishing a consensus between
programmers on the way to handle these things.

  

**Conclusion**

I’m still extremely cautious about including third-party modules in my
projects, and every day there are more and more poorly-implemented,
unnecessary modules uploaded to NPM, but I’m working on getting comfortable
with making exceptions for modules which are proven to reduce errors and
increase developer productivity (specifically when collaborating) so long as
they can be proven to have minimal impact on performance, reliability and
security.

  

---
title: Learn The Code
date: 1970-01-01
tags:
  - evernote
---
A lot of people ask me what programming language they should learn to write
web apps.  There are only two languages used on the web, and only one of them
is a programming language.

---
title: Lent on Reddit, vote us up! http -//www.reddit.com/r/iphone/comments/8uf8u/iphone_app_lent_always_get_your_stuff_back/
date: 2009-06-23
tags:
  - evernote
---
  
  

---
title: Let's take a moment to reflect...
date: 2000-11-09
tags:
  - evernote
---
It's almost 24 hours since the polls closed and we still don't know who the
next president will be. We have our results in (not exactly an impressive turn
out, I might add...but then again my line was down for the last day or so),
but it seems the ancient system used to tally the national votes could use
some help.  
  
In the meantime, here is the most interesting reply I recieved to my message
about our test poll. I've left out the authors email address, so please direct
all hate mail to me and I'll make sure he gets it.  
  
_I always knew..._  
  
_You were into politics. I hate all the side tracking bullshit topics that all
canidates use to win votes. I saw Al Gore in madison when he came. Everyone on
the stage behind him prompted everyone in the audience with flags and banners
when to clap and say, " Ya Gore you rock!" It reminded me of a brain washed
cult. America, the independent nation. I would love to be proud of america and
it's acheivements of dominating the globe with brutal tactics and greed driven
wars. Only we try to cover up these wonderful truths. If any president was as
cool to admit we rule 'cause we can, I would vote for him. I can handle the
truth, can you? What I'm saying is I'm voting for violence in the media, porn
on the internet, and school shootings._  
  
...and they said the american public was apathetic...

---
title: Let’s try this again
date: 2016-08-02
tags:
  - evernote
---
Years ago (looks like 2013?) I used [ Postach.io ](http://postach.io/) to
publish my blog.  I made a number of entries, and even wrote a [ Wordpress-to-
Evernote importing tool ](https://github.com/jjg/wp2evernote) to facilitate
the move, but for reasons I can’t remember it didn’t work out, and I abandoned
it, switching to yet another blog platform (maybe that was when I wrote [
Preposter.us ](https://github.com/jjg/preposter.us) ?).

  

Anyway, I received an apologetic email from the people at Postach.io and
decided to give it another try.  Since most of my writing originates in
Evernote anyway, if Postach.io works, it would save me considerable time and
effort, and would also get the desire to keep (re)building my own blog
platform off my plate.

  

So here’s to second chances.

---
title: Logger part 2
date: 2009-10-09
tags:
  - evernote
---
  
I've doing alot of thinking and not much building on the logger I mentioned
last week, but in the case that was a good thing.

As it turns out, I can use a simple voltage divider to wrangle the input
voltage (~0-15vdc, I hope) into something the arduino can deal with (0-5vdc).
Much simpler than I thought.

I probably will need to order a 0-5vdc meter though instead of using the 12v
one I have on-hand since I can't find an equally simple way to step-up the 5v
from the board to 12 (at least not using parts I have in stock).

I could forgo the meter (just use LED's) but what fun would that be?

So hopefully this weekend I'll get a chance to work on the case and such and
do a dry-run with a smaller solar panel I have in the basement. If that goes
well I'll get the rest of the parts ordered and get it ready for the real test
in the cabin in November.

I'm still planning on taking only one measurement but if this goes well (and I
can get some external memory working) I'll add a few more inputs to measure
pre-charge-controller voltage, amperage, temperature and maybe battery voltage
(as opposed to charging voltage) as well.  

---
title: Makerfaire Milwaukee 2016
date: 2016-10-26
tags:
  - evernote
---
I should have written this a month ago.

  

I learned a lot this year at Makerfaire Milwaukee.  It was the first time I
attended a Makerfaire as a presenter (really my daughter was the star of the
show, but we hauled Sux0rz along for the ride), and it gave me a new
perspective on the event.

  

The biggest take-away for me was the enormous amount of work put in by the
team putting on the event.  They were on-site working when we got there
Saturday morning and after we left Sunday night.

  

I also learned that attending as a maker is something I want to do again

---
title: Man, the zip line is only 60% done, but Jamie made me turn off the work lights (afraid the neighbors might call the cops)
date: 2009-06-19
tags:
  - evernote
---
  
  

---
title: Matt's Interview
date: 2008-01-07
tags:
  - evernote
---
Last weekend we shot [ Matt's ](http://mattscrudrundiary.blogspot.com/)
"progress report" interview and discussed his [ project bike
](http://mattscrudrundiary.blogspot.com/) and where things are at now that
we're about half-way to the spring crud deadline (since we began production on
the film).  
  
I have to say that Matt's bike looks a lot better than mine does right now,
and for all practical purposes he's a lot closer to being road-worthy than I
am.  
  
Shooting that interview definitely motivated me to get back in the garage, as
does the unseasonably warm weather we're currently experiencing here. The next
thing I need to do, re-assemble the carbs, I can do in the comfort of my
basement workshop so the only thing standing in the way there is finding the
time. After that it's back to the garage to re-attach the carbs, re-plumb the
fuel lines and try again to get the motor to start. Really not that much to do
before attempting to fire it up again if you think about it.  
  
After last weekend I can definitely say, like most things that require
discipline, fixing a motorcycle is definitely easier when you have a friend
doing the same. Fixing a motorcycle isn't all that hard, there is a financial
cost to it but if you do all the labor yourself (and fabricate/substitute the
less common parts) it's not terribly expensive. Technical skill is required,
but most of this you can acquire by reading and through trial-and-error. Bill
pointed out that the best way to learn these skills is by doing the work
yourself, making mistakes and then trying again until you get it. If you look
at the lost time and money of this process, it's still cheaper than going to
school and there is little chance of falling asleep working on your own bike.  
  
There is a part of [ Zen and the Art of Motorcycle Maintenance
](http://www.bartneck.de/work/researchProjects/pirsig/index.html) where the
narrator discusses "gumption", and says something like "without gumption the
motorcycle may never be fixed, but with gumption the motorcycle cannot resist
being fixed" (sometime I'll look up the exact quote) but I think the point is
that the only thing that will stop a project such as this is the mechanic's
choice to give up. If you can avoid this, the motorcycle will run again…  
  
…eventually

---
title: ...
date: 2016-09-20
tags:
  - evernote
---
Ahh... The Aleph...

  

I just had a realization that The Aleph is like the atomic bomb, the
realization of a theory that describes how matter can be converted to energy.
In the case of The Aleph, this conversion is from processing to memory, and
the ratio, the orders of magnitude, bear a resemblance.

  

In the case of The Aleph, the conversion is a very large amount of computing
power for an almost infinite about of memory.  The key however is that what
seems like a large amount of computing power by today's "conventional"
standards is but a drop in the bucket compared to what a quantum computer is
capable of.

  

This is another interesting coincidence, that the key to unlocking The Aleph
is the same key to unlocking nuclear weapons: action at the atomic level.

  

Quantum designs, using Aleph-type storage, will make the data centers of today
look like the mechanical computers of old.  We will look back at the
simplistic exponential process of bolting machine to machine akin to so many
cogs stacked on top of one another.

---
title: Measure twice, order once
date: 2008-09-09
tags:
  - evernote
---
Here's a bit of advice to all the budding " [ $1000.00 Motorcycle
](http://jasonscrudrundiary.blogspot.com/2007/05/price.html) " mechanics out
there: Anytime you order a part, make sure it will fit your bike by actually
measuring the mating parts and don't go by what "should fit based on the
model".  
  
I'm not sure why, but I fell squarely into this trap myself.  
  
I had to order a new set of tyres since the ones that came on the bike are of
unknown age but definitely old. So I go to [ Madison Motorsports
](http://www.madisonmotorsports.com/) and we look up in the book what kind of
tyres are available for a 197x CL350. As it turns out they are hard to get so
I place an order and forget about the whole thing for a couple of weeks.  
  
I get a call last week that the tyres are in, so I [ spend a few hours
](http://jasonscrudrundiary.blogspot.com/2008/09/better.html) getting the
wheels free from the bike and bring them down to the shop the next day.  
  
Later that same day I get a call from the shop because there is a problem with
the tyres I ordered. I ordered a set of 18" tyres as this is the size of the
stock wheels on the 197x CL350, however when the guys at the shop tried to
mount them they found out that my front wheel is not 18", it's 19".  
  
This brought into question weather or not I had a CL350 at all (apparently the
SL350 had a 19" wheel). I know it's a CL350 because that's what's stamped on
the frame, but then it all becomes clear:  
  
_When I was[ stripping and re-painting the tank
](http://jasonscrudrundiary.blogspot.com/2008/08/sunshine-of-my-hate-
part-1.html) I found out that there was some bondo filling in a few dents, and
there are other signs that the bike has been down. My guess is that when the
bike went down the original 18" wheel got bent and was replaced with this 19"
wheel. Maybe the whole front end was replaced? _  
  
Now let me say here that this is all my fault. Instead of going with "what the
book says" I should have measured the wheels (hell, just read the numbers off
the tyre) and this could have all been avoided. Based on this, the shop could
have justifiably charged me for this tyre as well as another that actually
fits, but they have been most understanding of the situation.  
  
Fortunately they were able to source a 19" tyre from a nearby supplier and
it's still possible that they will get it ready before they take off for the
races at the end of the week. This could have been a show-stopper for the fall
run, but it just goes to show the value of having a good relationship with a
professional shop.  
  
My old man was right...

---
title: megabytes for minutes
date: 2009-05-20
tags:
  - evernote
---
  
I figured out a way to (dramatically) reduce the memory footprint of my new
app (a necessity, because after too much use it would crash due to out of
memory conditions).

I held off because it was a pretty major change to the app, but now that some
of the other more annoying issues are resolved, the time came to address this.

I'm happy to say that after only about 45 minutes of coding and debugging not
only has the change been made, but the results are spectacular. I need to do
some more extended testing, but so far it's looking very good.

I also spent a little time working on the treehouse tonight, I think manual
labor helps debugging.  

---
title: Mesmorizing...
date: 2009-09-09
tags:
  - evernote
---
  
[ http://bit.ly/1319QD ](http://bit.ly/1319QD)  

---
title: Migration Update
date: 2013-03-21
tags:
  - evernote
---
I'm migrating the site to a new web host, so if you notice anything broken
please post a comment and we'll get it straightened out post-haste, thank-you.  
  

---
title: Millwork
date: 2016-08-03
tags:
  - evernote
---
A few months back I picked up a Lab-Volt CNC mill on Craigslist. It’s an
impressive machine, and seems to generally be in working order.  However the
software needed to drive it is proprietary, and only runs on Windows. Also I
don’t have a copy of it.

  

So I want to replace the “brains” with something I can work with, and since
I’m familiar with the open-source [ GRBL ](https://github.com/grbl/grbl) CNC
controller software, it seems like a natural choice for the job.

  

I spent a couple hours pulling the mill apart (most of which was simply
turning it around, it’s about 400lbs), examining the components and taking
readings. The good news is that it’s very modular, and I should be able to re-
use a lot of the components.

  

This is what the step drivers look like:

  

  

The bad news is that I wasn’t able to simply identify the control inputs and
drive the machine directly using a an Arduino running GRBL.

  

I’m fairly sure I’ve identified the inputs for the stock stepper-motor drivers
correctly. Using my little [ DSO Nano v3
](https://www.amazon.com/gp/product/B015X6LZFO/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&tag=jjg00-20&camp=1789&creative=9325&linkCode=as2&creativeASIN=B015X6LZFO&linkId=61acdd12a080cd281edbb9814e5bff1d)
oscilloscope, I captured signals on the various pins leading to the driver
boards while manually operating the X axis.

  

  

This trace was taken from one of the pins leading to the X axis stepper driver
while I was manually jogging the X axis. From the train of pulses seen here I
guessed that this was the “step” pin, which by process of elimination
identified the “direction” pin as well.

  

However once I connected the GRBL Arduino accordingly, I wasn’t able to move
the axis using GRBL commands. It’s not clear to me why this is exactly, which
I’m sure is due to the fact that I’m still learning how to use the scope and
read the trace captures. Driving the stepper board should be a matter of
feeding it pulses of the right voltage and of the right frequency, so if I can
positively identify what the stock computer is sending, I can figure out how
this is different from what the GRBL board is doing (and hopefully change it
to match the original computer).

  

I could just replace the stepper drivers as well, but that would make the
project considerably more expensive. Also I don’t know enough about the motors
to even spec. drivers for them, and I’m guessing they need something a little
beefer than the ones I use for my 3D printer.

---
title: MindPulse mentioned on healingbeats.com
date: 2009-06-06
tags:
  - evernote
---
  
I noticed these guys as a referrer in our HTTP logs, here's the source:

[ http://healingbeats.com/binaural-beat-applications-for-the-iphone/
](http://healingbeats.com/binaural-beat-applications-for-the-iphone/)  

---
title: More and Faster
date: 2001-01-22
tags:
  - evernote
---
No real news, I was going to do a review of _Crouching Tiger Hidden Dragon_ ,
but I thought I'd let you figure it out for yourselves, in the meantime, I'd
tell you we're working on some new stuff for the site, but you wouldn't
beleive me, but we are (confused yet?).  
  
In any event, I'll be adding an annoucement page where you can tune in and see
what's new, and what's really "coming soon", in the meantime, there's always
the current poll to waste time with.  
  
Oh yeah, buy.com ended up sending me the order and charging my credit card for
the experience I guess, oh well, at least I get to take a Monster out in
January, or something like that.

---
title: More Interviews
date: 2008-10-30
tags:
  - evernote
---
Last night we completed another key interview for the film and now we're down
to only one left on the list. That isn't to say we won't be doing more, or
doing additional interviews of subjects we've already spoken with but what it
_does_ mean is that we're one interview away from putting together what we're
calling the _"outline cut"_ .  
  
This is where we take everything we've shot so far, put it together in the
timeline and watch it end-to-end. The goal here is to see what we have,
identify the stories we want to focus on and determine what else we need to
tell these stories in a compelling way.  
  
So it's not so much the **end** of production in the traditional sense, but it
_is_ the **beginning** of post-production and a milestone that we've really
been looking forward to. Unlike a drama, where the story is written **before**
the camera rolls, **this** is where the story our film will tell really starts
to emerge.  
  
The other transition happening here is a shift in focus from working on the
motorcycle to one of working on the film. I have a few bike-related tasks in
store for the winter but the updates here will have more to do with **"shots-
and-cuts"** and less to do with **"pipes-and-jets"** (although bigger jets
_are_ on the top of my parts list). The primary challenge facing the bike at
the moment is getting the title and license squared away and from my
experience that doesn't make for very exciting writing. So unless something
particularly exciting happens, I'll spare you the gory details.  
  
We will also be annoucing new posts using our [ Slimey Mailing List
](http://2soc.net/crudrundiaries/) . If you'd like to receive these updates, [
follow me ](http://2soc.net/crudrundiaries/) to sign up.  
  
Look forward to more consistent (if not more frequent) updates and let us know
if you like what you're hearing or if there is something you think we're
leaving out.

---
title: More interviews scheduled
date: 2007-12-26
tags:
  - evernote
---
We've got two major production events coming up, the first is an interview
session that we'll be shooting with the two "mechanics" featured in the film
who will discuss their project bikes, their current progress and their
expectations for the next few months leading up to the spring 'run.  
  
Secondly we have secured an interview with Lyall Sharer (Sr.), one of the
Slimey Crud's and owner of [ Sharer Cycle Center in Verona
](http://www.sharercycle.com/) , WI. We are particularly excited about this
interview and hope to learn more about the origin of the run and the Slimey
Crud Motorcycle Gang.  
  
In related events, our last documentary release, " [ Breakdown
](http://2soc.net/breakdown) " has been selected by the [ Beloit International
Film Festival ](http://www.beloitfilmfest.com) and will be screening on
January 18th, 2008. You can find out more about "Breakdown" at BIFF by
visiting the production company's website at [ http://2soc.net
](http://2soc.net)

---
title: Music
date: 1970-01-01
tags:
  - evernote
---
Music existed for awhile before the recording industry however there are many
people who would have you believe otherwise. There were also musicians, and
they somehow made a living.  
  
In this time the only way to experience music was to attend a live
performance.  
  
With the advent of recording technology it, was finally possible for people to
make money from music they did not create. In the early days recording devices
were delicate and expensive and only worked under tightly-controlled
conditions. This made the process of producing a recording time consuming and
expensive, so to make up for this the recordings needed to be sold at a price
and volume capable of generating revenues that exceeded the cost of the
recording process.  
  
In order to meet these demands, additional people became involved in the
recording process. Entire companies sprung up simply to facilitate the
generating the volume of sales necessary to support the recording process.  
  
Conversely those who specialized in selling the music began to see patterns in
the types of recordings that were easy to sell (to their current audience) and
those that were not. These producers tuned their marketing system to run most
efficiently using these recordings, and this information was used to select
which recordings they would produce in the future.  
  
Of course not all artists met these criteria, in fact most did not. However at
first there were enough artists producing this "easy to sell" music (or were
willing to do so in order to secure recording contracts) that the supply met
the demand and the other musicians went on making a living the same way they
always had.  
  
As the audience for recorded music began to expand however, the demand for
different types of music grew and it became necessary for the recording
companies to offer more than the standard fare. This was not easy for them
because their production system and recording processes were optimized to
deliver a very narrow range of musical selections. This is where radio came to
the rescue.  
  
At first blush, broadcast radio would seem like the enemy of recorded music.
After all, the radio offered for free the music that the recording companies
were trying to sell. However, other than broadcasting live performances (which
was monumentally complex and expensive at the time), the radio could only play
music that was available via recordings produced by the recording companies.
Furthermore, radio's broadcast nature meant that everyone listening to the
same station heard the same music; this combined with a limited number of
station options (due to the high station operating cost, low selectivity of
receivers and geographical broadcast range restrictions), it was a
straightforward matter to ensure that the majority of radio listeners heard
the recordings that the recording industry was most interested in selling. The
result was an automatic classical conditioning training system for the music
recording consumer.  
  
This process continues today. Since the "golden age of radio", there have been
technological advances that have been capable of inflicting damage to this
system or rendering it impotent completely, however like any organism that has
lived a sufficient amount of time, this "recording system" has reacted to
these technological advances by engaging its immune system in the form of
psychological influence on artists, audiences and lawmakers. As the underlying
reason for the recording industry erodes (the high cost of recording music),
it is necessary for it to find new ways to justify its existence; first by
improving the lives of its customers and second by using threats and
punishment.  
  
It would be unfair to expect this creature to go down without a fight, and
given the size and power it has accumulated over the last century it would be
irrational to expect the fight to be over with quickly. However those of us
interested in reaping the rewards of progress need to make conscious decisions
to avoid feeding the beast or it will be through our own actions that it is
allowed to continue and squelch another generation of great musical artists.  
  
As creators of music we can begin by letting go of the idea that getting
"signed" is our ticket to the finacijal freedom we crave to allow us to spend
our time creating music and look toward more organic methods of making a
living from our creations. We must question even this notion that the value of
art can be corelated to it's revenue-generating potential.  
  
Over time this grew into an entire industry designed to

---
title: MyAppSales
date: 2009-08-04
tags:
  - evernote
---
  

Awhile back, maybe a few months ago I noticed a few apps showing up for iPhone
developers that make it easier to analyze app store sales.  Some of these apps
were desktop applications or web apps, but I was really only interested in
apps that ran on the phone itself.  If I wanted to use a "real" computer, I
could just use a spreadsheet.

  

  

One of the apps I found was [ MyAppSales ](http://www.drobnik.com/touch/my-
app-sales/) by [ Oliver Drobnik ](http://www.drobnik.com/touch/) (a.k.a. "Dr.
Touch").  As you may know, Apple provides no public API for accessing app
store sales data and as I understand it, it's "against the rules" to scrape
this data out of [ iTunes Connect
](https://itunesconnect.apple.com/WebObjects/iTunesConnect.woa) , so Dr. Touch
was unable to get his app approved for sale via the App Store.  So instead, he
found a clever way to work around the (dis)approval process by selling the
source code for the application to developers directly, who could then compile
and install the code on the iPhone themselves.  Since the only market for the
app are other iPhone developers, this model works just fine.

  

  

I decided to purchase a copy of the app, but during the checkout process (via
PayPal), I received an unexpected message (related to the international nature
of the transaction) and I chickened out, not knowing much about Dr. Touch at
the time.

  

  

A few days later I was relieved that the purchase didn't go through because I
found a lovely little app on the app store called " **Sales Tracker** ".  The
screenshots looked beautiful, and while the price was high I was pretty sure
this was the app I had been waiting for.

  

  

After purchasing the app, I was somewhat disappointed.  The graphics that
looked so good in the screenshots didn't necessarily do anything (notably a
cube on one of the screens seemed to serve no purpose) but it was giving me
some data and that was enough to put up with the limited functionality and
occasional crashes.

  

  

This wasn't the end of the disappointment however.  A month or so later, I
wanted to use the backup-and-restore feature because I was going to wipe out
my phone to install the OS 3.0 beta and I was unable to figure out how to use
the restore function (the URL provided by the application didn't work).  Worse
yet, when I emailed the developer I received no reply.  While I was having
trouble, so were other users as revealed by the increasingly negative reviews
on the app store, as well as rumors about questionable security and privacy
practices of the app's developers.

  

  

The final straw came about a week ago.  The day after my latest release of [
Lent ](http://www.gullicksonlaboratories.com/lent/) went live and I was very
curious about how it faired in the store.  When I tried to open **Sales
Tracker** and update my data, it was unable to connect.  As it turns out, this
was a common problem across all applications like this due to a change Apple
had made to iTunes connect.

  

  

All the while, I had been following Dr. Touch on [ Twitter
](http://twitter.com/Dr_Touch) and noticed that he too was having this problem
connecting with his **MyAppSales** app.  He described how he would address the
problem, but that he was on his honeymoon and it would have to wait until he
returned.  Not long after posting this message he wrote again, describing the
necessary fix and how you could modify the code yourself to address the issue.
Not more than a few hours later, the fix had been committed to the subversion
repository and that customers could fetch the patch immediately.

  

  

This was quite a contrast to the support provided for **Sales Tracker** .  I
decided right then to purchase a copy of **MyAppSales** .

  

  

I've been using **MyAppSales** for a few hours and I'm very happy with it.
It's not quite as "pretty" as **Sales Tracker** , but there's allot more
useful functionality and since I have the source, if I can't get Dr. Touch to
address my needs, I can do it myself (although I hope he does continue to
update the app, he does wonderful work!).

  

---
title: My conversation with AT&T
date: 2010-12-23
tags:
  - evernote
---
Our DSL suddenly dropped from about 5Mbps to 175kbps.  I contacted AT&T via
online chat, and after being told to restart the modem (and therefore the
conversation) twice, I had a long conversation with one of their "agents":  

  

System

  

Welcome Mr. Jason Gullickson.

  

System

  

Connecting to server. Please wait...

  

System

  

Connection with server established.

  

System

  

Technical Support Topic: DSL High Speed Internet

  

System

  

Frank has joined this session!

  

System

  

Connected with Frank

  

Frank

  

Thank you for contacting AT&T Internet Support, my name is Frank(ak880s).I see
that I am chatting with Mr. Jason Gullickson . Am I correct?

  

You

  

yes

  

Frank

  

I see that you provided XXXXXXXXXX as the number associated with your DSL/Dial
account.Am I correct?

  

You

  

yes

  

Frank

  

How are you doing Mr. Gullickson?

  

You

  

ok

  

Frank

  

Thank you for confirming the details. How can I make you a very satisfied
customer today?

  

You

  

I'm supposed to be getting betweeen 3mbps and 6mbps download but I'm only
getting about 175kbps (or 1.28mbps according to speedtest.net)

  

You

  

I just talked to another one of your agents who told me to remove my router
from the loop, so now I am jacked directly into the siemens speedstream modem
via ethernet

  

Frank

  

I am sorry you are experiencing this issue and will be happy to assist you.

  

Frank

  

I appreciate your efforts.

  

You

  

so it looks like it's not the router, since I'm getting the same numbers; what
do we do next?

  

Frank

  

Mr. Gullickson, did you power cycle the modem?

  

You

  

yes

  

Frank

  

Mr. Gullickson, in order to assist you further, may I place you on hold for
approximately two minutes while I check my resources.

  

You

  

ok, thanks for letting me know

  

Frank

  

Mr. Gullickson, please do not worry, I will do my best to resolve the issue.

  

You

  

thanks

  

Frank

  

Please let me know the speed that are you getting right now?

  

You

  

I'll re-run the test

  

You

  

1.31 mbps

  

Frank

  

Sure, I also want to ensure that you performed the power cycle in the given
sequence :-

  

Frank

  

Un-plug the power supply.

  

Un-plug the telephone cord from the modem.

  

Unplug the modem from all cables for 30 seconds.

  

Plug the telephone cord to the modem.

  

Turn on the modem.

  

Wait for the DSL light to become solid green.

  

You

  

yes, this is the sequence given to me by the previous agent

  

Frank

  

That's great.

  

Frank

  

I believe there should be increase in the power cycle of the modem.

  

Frank

  

I mean in the speed.

  

Frank

  

I am sorry for my typo.

  

You

  

so are you saying you want me to cycle it again?

  

Frank

  

Nope, if you performed the power cycle in the given sequence then it is fine.

  

You

  

good :) what's next?

  

Frank

  

Which Internet browser are you using? ( i.e. Internet Explorer, AT&T Yahoo
browser, Firefox or Google Chrome etc)

  

You

  

Chrome at the moment (on OSX)

  

You

  

however my original tests didn't involve a browser, I was reading SNMP data
from the router's WAN port, and now I'm reading it off the computers ethernet
interface

  

Frank

  

Mr. Gullickson, are you using Mac Operating System?

  

You

  

yes

  

Frank

  

Mr. Gullickson, the slow speed issue is might be with the computer or browser
settings.

  

Frank

  

I really apologize, we do not have expertise with the mac operating system,
this issue is best resolved when working together with one of our support
agents over a telephone line because it requires multiple troubleshooting
steps. Please call us at 1-877-722-3755 We look forward to speaking with you
to resolve this issue.

  

You

  

I'm certain it has nothing to do with the browser, I was experiencing this
download cap using other file download software

  

System

  

Connected with Frank

  

You

  

I'm certain it has nothing to do with the browser, I was experiencing this
download cap using other file download software

  

You

  

lost you for a second there Frank

  

Frank

  

Yes, we have appreciate your patience.

  

Frank

  

Yes, I am with you.

  

Frank

  

I am looking for your co-operation so that this issue can be resolve as soon
as possible.

  

You

  

so yes, I don't think this is a mac specific issue because I've noticed the
same problem with Windows machines on this network as well

  

You

  

It seems like it started a week or so ago, like somehow my account started
using a different, lower bandwidth plan

  

Frank

  

Please do not worry, we appreciate your business and time.

  

Frank

  

We want to resolve it with in time.

  

Frank

  

Please contact our voice support.

  

You

  

why can't we continue troubleshooting here?

  

Frank

  

Mr. Gullickson, we need to perform further trouble shooting steps to resolve
the issue and create the trouble ticket, I am documenting the case in such a
manner that you do not have to repeat yourself.

  

Frank

  

Please refer the case number :- 226343876

  

You

  

You're not answering my question Frank

  

Frank

  

Yes, we need to perform some troubleshooting steps ( browser optimize, LAN
settings, DNS flush etc), you are using the MAC Operating System and we do not
have expertise with the Mac.

  

Frank

  

Please contact our voice support, they will assist you further.

  

You

  

I have tried repeatedly to work with AT&T phone support with zero success. I
am a programmer and a network engineer, I can find any setting, etc. you want
to know about on the Mac

  

Frank

  

Yes, I appreciate your efforts. I apologize, the mac OS is not in our scope of
support and I can not perform troubleshooting beyond the scope of my chat
support limit.

  

You

  

How about I bring up a Windows XP machine, can we continue then?

  

Frank

  

Yes, we can continue but this will disconnect this chat session .

  

You

  

Actually it won't, I'm all set

  

Frank

  

Mr. Gullickson, do you want to start a new chat session or contact our voice
support?

  

You

  

No need to start a new session, I'm ready to troubleshoot on Windows XP

  

Frank

  

How did you change the computer?

  

You

  

I have an xp machine handy and plugged it into the modem, then connected the
mac (with the chat session) to a second ethernet port

  

Frank

  

Mr. Gullickson, the speedstream modem has only one Ethernet port.

  

You

  

the second ethernet is on the Windows machine, not the modem

  

Frank

  

I understand that there are two computer sharing the single Ethernet port, am
I correct?

  

You

  

not exactly (although I'm not sure how this is relevant to the troubleshooting
task); the XP machine is plugged into the single ethernet port of the modem;
the mac is plugged into a second ethernet port on the XP machine; the XP
machine is bridging the connection to the Mac, but the XP machine is connected
directly to the ethernet port of the modem

  

Frank

  

In this scenario, please contact our voice support, the issue is with the
multiple computer sharing the same modem issue, it is one of the reason for
getting the slow speed.

  

You

  

actually the performance tests were carried out with only one computer
connected, I only brought the second machine online at your request for a
Windows machine to troubleshoot

  

Frank

  

The benefit of the voice support is this we can restart the modem/computer as
per the troubleshooting steps without loosing the voice session.

  

Frank

  

Yes, I can see that the Mac computer is connected directly with the modem.

  

You

  

I understand the value of that flexibility. However has anything been done to
rule out a bottleneck between the modem and the internet? As several devices
on my network have poor network performance it is unlikely that the problem is
on one of these computers

  

Frank

  

Yes, I can understand that I appreciate your computer knowledge, I am here to
assist you.

  

Frank

  

We value your time as well, we can resolve the issue or create the trouble
ticket only after performing the all trouble shooting, we are not suppose the
line test before the trouble shooting ( power cycle the modem, disconnecting
the router or browser troubleshooting).

  

Frank

  

The slow speed issue is also may be due to the anti-virus applications or
maleware, router or modem firewall.

  

Frank

  

Mr. Gullickson, the speed range of your High Speed Internet service is based
on the distance between your home/office and our network location, as well as
the condition of your line.

  

Frank

  

The actual speed achieved will vary due to factors such as Internet congestion
and wiring inside your home/office. We need to perform various test to get the
root cause of the issue correctly.

  

You

  

Very good, let's perform some tests

  

Frank

  

Other reason are the temporary files and cookies of the Internet browser or
any EMI device near the modem.

  

Frank

  

Sure, as you are using the two computer with the same modem, I appreciate if
you contact our voice support. Everything is documented in my case.

  

Frank

  

It will be also best if you connect the windows XP computer with the modem and
perform the power cycle.

  

You

  

This is the second time I've been asked to re-start the troubleshooting
process with another agent. How about this, if you're willing to test the
line, and if that doesn't provide any insight, I'll attempt to contact AT&T
voice support, deal?

  

Frank

  

Mr. Gullickson, I apologize, in this situation, it is not possible to run the
speed test, I will not get the good and correct results.

  

Frank

  

I will be happy to chatting with you all day. We are here to resolve the
issue.

  

You

  

That's not the impression I'm getting Frank, if I understand correctly, the
only option you're providing me with is to end this conversation and attempt
to contact voice support

  

Frank

  

Mr. Gullickson, I appreciate your patience, I would love to help you and we
are here for the same.

  

Frank

  

However, we have some limitation over the chat support.

  

You

  

I believe we have arrived at an impasse; if you cannot (or are unwilling to)
test the line I don't think there is anything more we can do here.

  

Frank

  

We have some dedicated department for some specific technical issue like
wireless, Mac or gaming console etc.

  

Frank

  

They are expertise on these stuffs.

  

You

  

As I said before I can attest that the issue has nothing to do with these
individual devices, what I need is a network engineer or possibly a lineman.

  

You

  

I believe that the quickest way to resolve this issue is probably going to be
to find another internet service provider

  

Frank

  

Yes, I understand that our voice agent will surely arrange the line technician
for you if required.

  

Frank

  

Mr. Gullickson, you are valuable for us and you will be always valuable, we do
not want to loose any of our member. I appreciate your patience again.

  

Frank

  

Is there any thing else I may assist you with today?

  

You

  

Based on this conversation I believe there is not.

  

Frank

  

It was nice chatting with you.

  

Frank

  

Thank you for choosing AT&T where we value your business. You will now be
disconnected from this session. The chat window will remain open until you
close it. For quick answers, make the new AT&T Yahoo! Help site your first
stop. Visit [ http://support.att.com  ](http://support.att.com/) where you'll
find pages of product information to assist you. Again, thank you for choosing
AT&T Yahoo! Chat Support.

  

  

  

  

Connected with a AT&T representative.

  

  

Session ID: 3112700

---
title: My Recent Experience with Square
date: 2012-05-10
tags:
  - evernote
---
I have been a customer of [ Square ](https://squareup.com/) since they were in
beta and up until this experience have happily paid hundreds of dollars to
them in transaction fees.  Their software is excellent and when things are
going well, the experience is the best their is.  I have recommended Square
repeatedly to others and have even gone out of my way to promote the service
because it has been so valuable to me in the past.  It is because of this that
the horrendous treatment I have received as of late is all the more surprising
and painful.

  
  

On April 18th I received an email from Square:

  
  

\----

  
  

_Jessica_

  

_APR 18, 2012  |  12:05PM PDT_

  
  

_Hi Jason,_

  
  

_We have received notification of a chargeback to your account. If you would
like to dispute this chargeback, please provide a detailed description of the
goods or services that were sold for the transaction below. Also, if this
transaction was manually entered (card not present), please provide signed
card holder authorization and proof of delivery for the transaction. There
will be a hold or debit (via your bank account) on this transaction until the
dispute is resolved. We have outlined the process below for your convenience._

  
  

_1\. The buyer (cardholder) requests a chargeback/dispute from their financial
institution._

  

_2\. The respective financial institution notifies Square and debits the funds
from Square._

  

_3\. Square places a hold on the seller's funds related to the chargeback._

  

_4\. Square notifies the user (merchant/seller) through email and requests
information that will facilitate in the challenge process._

  
  

_Amount: $180.00_

  

_Type: Card Payment_

  

_Date: Mar 2, 2012 at 8:22pm CST_

  

_Payment card: *REDACTED*_

  

_Statement description: Jason Gullickson_

  
  

_As this case is time sensitive, if you would like to challenge the
chargeback, please respond back within 15 days from the date of this email._

  
  

\----

  
  

I followed-up immediately with as much detail as I could put my hands on
without going back to the paper records:

  
  

\----

  
  

_APR 18, 2012  |  12:17PM PDT_

  

_We will would like to dispute this chargeback._

  
  

_This transaction consisted of four items won during the *REDACTED* auction.
We are looking into our records to determine what additional details are
available about the specific items that were purchased._

  
  

\----

  
  

At this point I contacted the company I had processed the payment for to
marshal any additional information that could be had from the paperwork
(should Square require more details) and waited to hear back from Square.

  
  

The next morning I received this email from Square indicating that they had
withdrawn $175.05 from the bank account linked to my Square account:

  
  

\----

  
  

_Hello, Jason Gullickson._

  
  

_We just initiated a debit in the amount of $175.05 from your *REDACTED*
account. We are debiting your account because you have a negative Square
balance. The funds should be withdrawn in 1–2 business days._

  
  

_For more information, please read our terms of service:[
https://squareup.com/legal/sign ](https://squareup.com/legal/sign) _

  
  

_If you have any questions, please contact support:[ https://squareup.com/help
](https://squareup.com/help) _

  
  

_Thank you!_

  

_[ https://squareup.com/ ](https://squareup.com/) _

  
  

\----

  
  

This was a problem.  The reason being that I don't keep a balance in this
account (ironically to minimize my loss should the account become
compromised).  This being the case I imagined one of two outcomes:

  
  

1\.  The transaction would fail

  

2\.  The transaction would go through and incur overdraft penalties

  
  

First I checked the balance of the linked account and it was negative due to
the charge placed by Square.  I immediately contacted my bank to let them know
what was going on and that I would be transferring sufficient funds to my
account to cover this withdrawal. I then transferred $180.00 from another
account, however due to the speed of EFT this would take a few days.

  
  

I then replied to the original email I received from Square explaining the
situation and why debiting this account was unacceptable, and that I would be
happy to fund my Square account through other means until the disputed charge
was settled.

  
  

\----

  

_APR 19, 2012  |  04:03AM PDT_

  
  

_Good morning,_

  
  

_I received an email from Square this morning indicating that you will be_

  

_withdrawing $175.05 from the bank account associated with my Square account_

  

_due to a negative balance (which is due to this chargeback). There are two_

  

_problems with this:_

  
  

_1\. As I understand it I have 15 days to dispute this charge, and have_

  

_already initiated the dispute process, so it is unacceptable to have funds_

  

_withdrawn from my account until this process has been completed_

  

_2\. I do not keep a balance in the account associated with my Square_

  

_account as I use Square only to accept payments. Your attempt to withdraw_

  

_from this account will result in either a rejection of the transaction due_

  

_to insufficient funds or worse, you will receive your payment and I will_

  

_receive insufficient funds fees._

  
  

_This is completely unacceptable and I require that you immediately stop_

  

_this transfer to avoid any chance that I will incur fees. If it is_

  

_necessary I can pay the negative balance in my Square account from other_

  

_sources, however as this chargeback dispute is still in progress I don't_

  

_understand why I should have to pay anything until the matter is settled,_

  

_and should that be the case I will be closely reviewing your terms and_

  

_considering further action._

  
  

_For the record I have never received such "guilty until proven innocent"_

  

_treatment from a company before and until now have sung the praises of_

  

_square and happily paid the fees associated with your service. If this_

  

_issue is not resolved immediately (today, 04/19/2012) I will no longer use_

  

_or recommend Square and will close my account as soon as this situation has_

  

_come to an end._

  
  

_I will also attempt to submit this request to your "support" system if I_

  

_can figure out how to find a contact email address there._

  
  

_Your prompt response is appreciated,_

  
  
  
  

_Jason J. Gullickson_

  
  

\----

  
  

I waited a few more hours for a response (at this point I had still not heard
back from my first response to their initial message) and then sent one more
reply to the original chargeback email and indicated that I required a
response by 3:00PM CST on April 19th.

  
  

\----

  
  

_APR 19, 2012  |  10:23AM PDT_

  

_This is my third attempt to contact Square and I have not yet received a
response._

  
  

_If I do not receive an email response to these messages before 3:00PM Central
Standard Time today I will seek alternate forms receiving a response._

  
  
  
  
  
  

_Jason J. Gullickson_

  
  

\----

  
  

When 3:00PM had come and gone, I worked my way through Square's "support"
system until I could find a contact email address and contacted them again:

  
  

\----

  
  

_Subject: Disputed Transaction_

  

_Email Message:_

  

_One day ago I was contacted by Square regarding a disputed transaction. I
provided the requested information and did not receive a response._

  
  

_This morning I received an email indicating that Square had withdrawn the
disputed amount from my bank account without warning. I again responded to the
original message explaining why this was unacceptable and requested a response
from square today._

  
  

_A third time I responded to the original message with a request for response
by 3:00PM CST. Again I received no response from Square by this time._

  
  

_In addition to costing money, this mistake has now cost a considerable amount
of time and has undermined any confidence I have had with Square. I have been
using Square since beta and have recommended it repeatedly as well as having
paid hundreds of dollars in transaction fees. This lack of response and
manipulation of my accounts without notice is unacceptable and I write this
with some hope that approaching the communication via a separate channel will
not fall upon deaf ears as all of my other correspondence as so far._

  
  

_Please respond to this message today if you have any interest in resolving
this matter amicably._

  
  
  
  

_Jason J. Gullickson_

  
  

\----

  
  

I promptly received this automated reply:

  
  

\----

  
  

_Square Support Notification for Case #: 597069_

  

_Case Updated:   APR 19, 2012  |  08:23PM UTC_

  

_Thank you for contacting Square._

  
  

_We are committed to responding within 24 hours, so you will hear back from us
shortly._

  
  

_In the meantime, feel free to return to the Square Help Center for answers to
frequently asked questions._

  
  

\----

  
  

Another day goes by.

  
  

At this point my bank has levied an insufficient funds fee on my account,
however I was able to contact them and get the charges reversed (a welcome
surprise).  Interestingly enough my balance  had returned to it's original
state as Squares' charge was declined, which made it easier for me to be
patient and wait for Squares' response as they did not have my funds and it
looked unlikely that their actions would result in any additional fees from my
bank.

  
  

Well over 24 hours after my inquiry above, I received this message from
Square:

  
  

\----

  
  

_APR 20, 2012  |  04:11PM PDT_

  
  

_Hi Jason,_

  
  

_Thanks for writing in. Please be advised that we respond to tickets in the
order that they are received. If you write in repeatedly prior to receiving a
response, your correspondence will be understandably marked as more recent,
may be seen as spam, and you will naturally inhibit our ability to address
your case in an efficient manner._

  
  

_As a point of clarification, all funds associated with disputed transactions
are automatically held in your Square account pending the resolution of said
dispute. If there is an insufficient balance in your Square account to cover
the total of the dispute, your linked bank account will be debited. Note that
this does not mean we are deciding the case against you and these funds are
not remitted to the cardholder unless we receive an unfavorable resolution._

  
  

_It is our pleasure to inform you that this inquiry has been preliminarily
closed in your favor. While this is promising, it is also conditional. Your
funds are eligible for automated release on or around 30 April 2012 once
additional confirmation is secured. No further action is required from you at
this time. Unless you receive further correspondence from us regarding this
dispute, you may consider the matter closed upon receipt of your funds._

  
  

_Thanks,_

  
  
  
  
  
  

_Jeanette_

  

_Square Chargeback_

  

_[ help.squareup.com ](http://help.squareup.com) _

  
  
  
  

\----

  
  

While the tone was a bit disrespectful from a customer service perspective, I
was happy to hear that it sounded like we were out of the woods, and that it
would just be a few more days before everything was back where it should be.
I was certain that I would no longer be a customer of Square but I had no
intention of expending any more effort on the matter.

  
  

Then the next morning I received another email from Square indicating that
they had again withdrawn funds from my bank account.  Frustrated, I once again
replied:

  
  

\----

  
  

_APR 24, 2012  |  05:46AM PDT_

  

_Jessica I received another email from Square this morning that said you_

  

_withdrew $175.05 from my *REDACTED* account because of a negative Square_

  

_balance, however since the beginning of our conversation the Square website_

  

_has never indicated that I have a negative balance._

  
  

_Either this is a bug or something fishy is going on._

  
  

_Please return my funds as soon as possible and close my account._

  
  
  
  
  
  

_Jason_

  
  

\----

  
  

Another day goes by, and this time the funds had been successfully removed
from my bank account (fortunately the transfer I initiated on the 18th had
completed so I wouldn't have to beg for mercy from my bank again to avoid
additional fees).  Finally I get a response from Square regarding the
withdrawal on April 25th:

  
  

\----

  
  

_APR 25, 2012  |  03:44PM PDT_

  
  

_Hi Jason,_

  
  

_Thanks for writing in. The debit you referenced occurred because the first
failed due to an insufficient level of funding in your linked account. Per the
network regulations, the debit process is automated and will continue to
attempt to hold the funds associated with the dispute until it is resolved._

  
  

_Because the debit failed, release eligibility has been shifted to 4 May 2012
in order to verify that the most recent debit was successful. Understandably,
when we receive notification that your linked account is under-funded, we will
become concerned about your ability to meet the performance obligations
associated with your past/present use of the service._

  
  

_Thanks,_

  
  

_Jeanette_

  

_Square Chargeback_

  

_[ help.squareup.com ](http://help.squareup.com) _

  
  

\----

  
  

Let's just stop here for a moment and consider the facts:

  

  

  

  * I used Square to accept a legitimate payment for a product which was delivered in person at the time of purchase 
  

  * Square informed me of a chargeback 
  

  * I immediately responded with information which later appeared to resolve the chargeback issue 
  

  * Square repeatedly debited my bank account to cover the chargeback (which at this point looked to them to no longer be an issue) 
  

  * Square fails to respond to any of my requests in a timely fashion 
  

  * Square fails to meet even their own responsiveness promises 
  

  * Square offers no consideration for the effects their actions may have on their customer (me) 
  

  * Square accuses me of spammer-like behavior 
  

  * Square considers me suspect of not meeting performance obligations 
  

  

  

…and now I'm being told that the funds (which are no longer needed) which have
been removed from my bank account (with no advance notice) will be held longer
due to a transaction result (which I attempted to warn them about) that I had
no control over.

  
  

Given this, I reply:

  
  

\----

  
  

_APR 25, 2012  |  04:32PM PDT_

  

_This situation has moved from frustrating to bizarre._

  
  

_I have explained throughly and repeatedly the situation from my perspective_

  

_yet you refuse to offer any understanding or consideration of my position._

  

_I have complied with all of your requests (and unlike you, in a timely_

  

_fashion) and yet you choose to respond to my requests for service with_

  

_thinly veiled threats and accept zero responsibility for what has_

  

_transpired._

  
  

_I understand that you are under no legal or contractual obligation to make_

  

_any concessions or demonstrate any compassion or interest in your customers_

  

_but I am stunned that this is a course of action you consider acceptable or_

  

_a component of a successful business model._

  
  
  
  
  
  

_Jason_

  
  

\----

  
  

Eventually, Square responds:

  
  

\----

  
  

_APR 30, 2012  |  12:39AM PDT_

  
  

_Hi Jason,_

  
  

_Thanks for writing in. As a point of clarification, chargeback/dispute
policies are set by the network and are strictly regulated. Please review
article 31 of the User Agreement ([ https://squareup.com/legal/ua
](https://squareup.com/legal/ua) ) for more information regarding the
transactional hold process. When a transaction is disputed, the funds that are
the subject of said dispute must be held until a final resolution is received.
As the merchant who processed the transaction, you retain the responsibility
to ensure that your linked account is sufficiently funded to cover your
obligations. Please note that when you receive a chargeback, Square
effectively floats you a loan for the duration of the dispute process.
Therefore, the transaction level hold is an industry standard regulation
imposed for the merchant's protection. _

  
  

_No further action is required from you at this time. Unless you receive
further correspondence from us regarding this dispute, you may consider the
matter closed upon receipt of your funds._

  
  

_Thanks,_

  
  

_Jeanette_

  

_Square Chargeback Services_

  

_[ help.squareup.com ](http://help.squareup.com) _

  
  

\----

  
  

At this point I give up and decide to wait it out.  On May 5th I log in to my
Square account and see that I have a positive balance of $175.05 (FWIW, the
balance never indicated a negative number).  I excitedly look for the means to
withdraw this amount.

  
  

After hunting around a bit I find this statement:

  
  

_"Payments taken during normal business hours (before 5pm PST) are available
in your bank account the next business day."_

  
  

OK, according to the last email from Square the funds should have been
released on May 4th, so why haven't they been deposited in my bank account
yet?

  
  

Ah, "business day"

  
  

Looking further down the page, since the "payment" happened on a Friday (at
what time I do not know) the funds could be sent to my bank as late as Sunday
(May 6th), and will be available on Monday (May 7th).  That's fine, I can wait
a little longer.

  
  

May 7th comes and goes and the funds do not show up in my bank account.  I
wait one more day (perhaps it's some strange timezone issue?) and again no
deposit.  I send yet another response to Square:

  
  

\----

  
  

_It is now May 8th._

  
  

_Release my funds to my linked bank account within 24 hours or I'm going to
the press._

  
  
  
  

_Jason_

  
  

\----

  
  

Today is May 10th and unsurprisingly I have not yet heard back from Square,
nor have my funds been returned to my bank account.  Obviously some more
aggressive measures will be required on my part to get my money back, but I
wanted to share this experience with anyone who is considering using Square as
a credit card processor, as an example of how you can expect to be treated
when something even slightly unusual occurs.

  
  

I have had poor customer service elsewhere, and often it is almost an
expectation but given just how good Square was, I was shocked that I received
worse treatment from their support staff than even AT &T could dish out (at
least AT&T didn't take money from me and then fail to return it when
promised).

  
  

Worst of all is their complete unwillingness to make any attempt to compromise
in order to provide a shred of customer service or resolve the situation
amicably.  I don't like doing this, but at this point they have given me
little choice but to share with you my experience in the hope that others will
become aware of the possibility and choose to do business elsewhere before you
find yourself almost literally held for ransom.

  
  

[ http://www.squareup.com ](http://www.squareup.com)

---
title: NASLite
date: 2004-07-13
tags:
  - evernote
---
NASLite turns any i386 box into a Network Attached Storage (NAS) device. In
essense, it is a box of disks (BOD) that you can access via the network.  
  
While this could be accomplished through any number of means, wWhat makes
NASLite interesting is that it is contained on a single floppy disk.  
  
So, to use NASLite, you take a box (even a crusty old 200mhz Pentium), pop
this floppy in it and any disks installed in it can be used as network
storage.  
  
In addition, NASLite can do hardware-independent large-disk access, so for
example, chances are the machine above has an old BIOS and IDE controller
which cannot deal with say, a 160GB disk. NASLite will deal with this on its
own, so you can take your P200, slam four 160GB disks in it an have yourself
500GB+ of fast, reliable network storage.  
  
NASLite provides SMB, CIFS and HTTP network filesystem support and is managed
via a web browser or TELNET.  
  
I haven't used this yet (I just found out about it today), but I will be
making a disk and experimenting with it over the weekend. The only snag that I
see so far is that it uses a floppy formatted for 1.72MB. This is easily
created on a Linux system but there is no way (that I know of) to do this
under Windows (once I get it up and running and you would like one of these
disks, let me know).  
  
The actual requirements (from the manual):  
  
486DX  
PCI (no ISA or EISA support)  
16MB RAM  
1 to 4 IDE hard disks  
PCI NIC  
3.5" HD Floppy Disk Drive  
  
The entire manual (PDF) can be found here, and is much more useful than the
website for information:  
  
http://www.serverelements.com/bin/NASLite-Manual.pdf  
  
The only thing I would have liked to see added to this (and I only say it
because it's already built into SAMBA, a core component of NASLite) is the
ability to hang a printer off the NAS box printer port and use it as a network
print server. That aside, I think this is a pretty amazing tool and I'll add a
second part to this post when I have had a chance to actually test it out at
home.  
  
So, using my Fedora partition I create the 1.77 floppy as described on the
website.  
  
I dig out a Dell XPSM200s from my garage and haul it in the house (I picked
this one because it was cleaner than the others).  
Here's the specs for this box:  
  
Pentium 200  
32MB RAM  
2.4GB HDD  
10/100 3Com PCI NIC  
  
The first time through I want to see exactly what this thing does, so I hook
it up to a keyboard and monitor (turns out this is necissary unless the
default IP is valid on your network). The NASLite disk boots like any other
Linux boot disk, various kernel messages scrolling along, untill it comes to a
login screen. The first time I booted the system I received an error
reguarding the first fixed disk, which was accurate. I had trashed it with
another experiment long ago.  
  
So, once I logged into the NASLite menu, I selected option #4. There was a
warning about lost data (which I ignored) and entered 1 to indicate that I
wanted to setup the first fixed disk. This then formatted the disk and
requested a reboot before it would be avaliable. I rebooted, and this time, no
fixed disk error.  
  
Just out of curiosity, I opened "Network Neighborhood" on one of my other
machines to see if I could see the NASLite box yet; nope.  
  
OK, so I log back into NASLite and take a look at the menu. There is a number
of network-related options (network settings, name, workgroup, etc), so I'm
going to run through that next.  
  
First, I changed the IP from the default (192.168.1.1) to something reasonable
for my network and outside of my DHCP scope.  
  
After making this change, I'm prompted that I must save the configuration, so,
I select "save config" from the menu and after confirming the save, it writes
the config back to the floppy. I think I could have changed the remaining
settings as well and saved them all at once, but I'll try to follow the rules
first (not that this did not require a reboot).  
  
After saving the config and changing the other parameters (name, workgroup,
etc.) I shut the thing down and parked it in it's permanent home (behind the
couch). Booting it without a monitor, it became avaliable in about 2 minutes
(remember, we're reading this from a floppy).  
  
Managing the system remotely is done via a telnet interface which is identical
to the menu displayed on the console. Fireing up "Network Neighborhood" now I
can see the NASLite box.  
  
The box has a default share for the disk I setup along with an INFO share. The
INFO share has a number of webpages that display system information, disk
information and system logs. This was useful in troubleshooting the beeping.  
  
Yes, the beeping.  
  
NASLite has a neat "audible feedback" feature. Since it's assumed that this
box will be hidden in a closet somewhere without a monitor, it uses the PC
speaker (no soundcard required) to provide feedback on the device's status. It
has different beeps (described in the manual) for things like "ready",
"shutdown", etc. It also provides notification when something is wrong with
one of the disks, using S.M.A.R.T. pre-failure testing (which is performed
every five minutes).  
  
In my case, I'm using a disk that is so old it doesn't provide any S.M.A.R.T.
data, so NASLite issues a set of warning beeps, every five minutes. I looked
around in the config and didn't see any way to turn this off (maybe I should
read the ENTIRE manual?)r.  
  
Other than that I have been very, very impressed with NASLite. Within one hour
(including the time-outs I took to take these notes) I have a chunk of
networked disk to dump all of my media and other backup junk. I want to point
out that the manual (PDF) is excellent, easily the best I've ever seen for a
GNU project, and the rest of the system works as advertised. I have some
larger disks I'm going to throw in the system this week and really beat it up
(transfer all of my music and some uncompressed video to the thing) and see
how it holds up, but so far my testing has revealed no significant weakness.  
  
I highly recommend this software.

---
title: New Crud Run Diary post
date: 2009-05-07
tags:
  - evernote
---
  
[ http://jasonscrudrundiary.blogspot.com/2009/05/hmm.html
](http://jasonscrudrundiary.blogspot.com/2009/05/hmm.html)  

---
title: New Handgun Could Be Used For Self-Defense, But Not Crime
date: 2016-05-03
tags:
  - evernote
---
  

New Handgun Could Be Used For Self-Defense, But Not Crime

[ https://www.washingtonpost.com/archive/business/1984/05/28/new-handgun-
could-be-used-for-self-defense-but-not-crime/337e287c-
64e9-40a3-9f39-95dbb4c94563/
](https://www.washingtonpost.com/archive/business/1984/05/28/new-handgun-
could-be-used-for-self-defense-but-not-crime/337e287c-
64e9-40a3-9f39-95dbb4c94563/)

New Handgun Could Be Used For Self-Defense, But Not Crime The inside track on
Washington politics. Be the first to know about new stories from PowerPost.
Sign up to follow, and we’ll e-mail you free updates as they’re published.
You’ll receive free e-mail news updates each time a new story is published.
You’re all set! Sign up *Invalid email address Got it Got it By Barry Gross;
Gross is an editor for the Business & Finance section of The Washington Post.
May 28, 1984 A Louisiana man has applie...

  

---
title: New stuff, etc
date: 2001-01-25
tags:
  - evernote
---
Not alot to report on this week, however we have added a new feature to the
site, the special annoucement page (if you got an email telling you to come to
the site, you wanna go there, either click the link above, or the fish on your
left).  
  
Oh yeah, we're also working on a little something you might want to check out
if you're into all this digital media stuff going on.  
  
More next week, untill then...

---
title: Nine Days
date: 2008-09-26
tags:
  - evernote
---
I took the bike out for an extended ride last night (on a closed course of
course…) and even was able to get a high-speed (60mph) run in there. There
seems to be some sort of missing/backfiring problem between 0-.25 throttle
which got worse toward the end of the ride. At first I thought it was maybe
the battery conking out but I checked it right away when I got home and it was
12.25 (which is impressive since it was 11 something when I left...the
charging system is working!). It also resembled a lack of fuel, but I think
there was plenty in the tank and in the lines as well.  
  
I need to do some reading but it might be that the air screws are too tight,
making for a too-rich mixture around the crossover point where the carbs
switch from the idle to the main jets (and both are open), so it might be just
a matter of adjusting those screws, but you would think it would backfire at
idle if that was all there is to it.  
  
In any event the bike went 60 without ejecting any parts, and then it stopped
as well. I have to say the handling is awesome (not that I pushed it too hard,
but it feels really good at speed). If I can get this one running problem
cleared up, and find out what's up with that back brake…I think she's done for
this phase.

---
title: No Joy
date: 2012-12-04
tags:
  - evernote
---
After much fiddling with connections I was able to get the replacement QU-BD
extruder wired up and ran a couple of extrusion tests.  
  
![20121204-063255.jpg](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/20121204-063255.jpg)  
  
The short version of the story is, it didn't work.  
  
![20121204-063319.jpg](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/20121204-063319.jpg)  
  
The long version is that after several attempts, I was never able to get any
material to appear at the end of the nozzle. The filament would feed briefly,
then jam. After disassembling the extruder, the end of the filament looked
like this:  
  
![20121204-063330.jpg](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/20121204-063330.jpg)  
  
It occurred to me that perhaps the extruder was tested with ABS instead of PLA
(which is what I use) so I ran the temperature all the way up to an indicated
240C, but to no avail.  
  
My last guess at what is wrong is that the indicated temperature is incorrect.
I'm going to double-check the thermisistor tables in the firmware and I'm
considering picking up an infrared thermometer to measure the actual nozzle
temp (although I'm not sure reading the external temperature is useful).  
  
I'm also planning to order a [ J-Head ](http://reprap.org/wiki/J_Head_Nozzle)
to get my old Makergear extruder ready to go again.

---
title: OctoPrint - Control your 3d printer from anywhere using a web browser
date: 2013-01-19
tags:
  - evernote
---
[ OctoPrint ](https://github.com/foosel/PrinterWebUI) (available on [ github
here ](https://github.com/foosel/PrinterWebUI) ) is an alternative "host"
program (the program that controls the 3d printer, typically a desktop
application running on a desktop or laptop computer) which is controlled
entirely via a web interface.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Main.png)

_OctoPrint Homescreen displaying status and temperature graph_

  
This means that instead of having to be physically nearby the printer to start
and monitor print jobs, you can submit them from anywhere* and keep an eye on
the progress of the print from any device with an internet connection and a
web browser.  
  
I've been looking for something like this ever since I started using [ my
Reprap ](http://www.gullicksonlaboratories.com/projects/reprap/ "RepRap") , as
the machine is located in the laboratory and I'm not always able to hang out
for hours to keep an eye on things.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Controls.png)

_Manual printer controls_

  
In addition to basic host features like loading files, monitoring print
progress and manually operating the various axis of the printer, OctoPrint
includes a few additional features to address the fact that you're not in the
same room with the printer to keep an eye on it.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Webcam.png)

_Monitoring a print job remotely via live video feed_

  
The first is the ability to display a live video feed inside the OctoPrint web
interface.  Essentially this passes-through a feed from an external program
that operates the web cam, but having it right in the same page makes it very
convenient to monitor all of the critical elements of the print job while it's
running and intervene immediately if you see something going wrong.  OctoPrint
uses an http request (just a web link) to get the images from the camera,
which has the added benefit of working with dedicated stand-alone web cameras
as well as cameras connected to other computers.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Timelapse.png)

_OctoPrint can automatically create timelapse movies of your print jobs_

  
In addition to displaying a live video feed, OctoPrint has a timelapse feature
which captures still images from the camera feed at regular intervals (time-
based or triggered when the printer begins a new layer).  OctoPrint then
collects these stills and assembles them into a video file automatically.  
  
Aside from being cool to watch, these videos can be extremely valuable when
tuning and troubleshooting the printer (or seeing what went wrong with a
failed print in the middle of the night).  
  
http://www.youtube.com/watch?v=8EbIO71Bi-M  
  
OctoPrint also keeps a copy of each file it's printed so you can easily re-
print something without having to upload the file again (important since
devices like the iPad can't upload local files via web page), and since the
design of the web interface is "responsive", it works well on any browser
regardless of screen size (at least every one I could test).  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Files.png)

_OctoPrint keeps a copy of the files you print for easy reproduction later_

  
Another nice thing about OctoPrint is that it was designed from the beginning
to work well on the [ Raspberry Pi ](http://www.raspberrypi.org/) , which can
be easily attached to the printer and make the whole setup self-contained.  In
my case I'm using WiFi on the Raspberry Pi as well, so my entire printer can
be operated with nothing but a power cable (and perhaps in the future, on
batteries alone).  
  
I've been using OctoPrint in its various incarnations since I first heard
about the project in a [ post to the Google+ 3D Printing Group
](https://plus.google.com/u/0/106003970953341660077/posts/GQmn9tSgfGP) by it's
author [ Gina Häußge
](https://plus.google.com/u/0/106003970953341660077/posts) around Christmas
2012.  I've been using it as my exclusive printer host for the last week or so
without incident.  I've used it for prints lasting more than 7 hours, and I'd
say at this point it's stable enough to be used as a replacement for my old
standby [ Pronterface ](https://github.com/kliment/Printrun) .  
  
As with anything, there are [ room for improvements
](https://github.com/foosel/PrinterWebUI/issues) , and while I can do things
like load filament by entering the extruder commands directly into the app's
terminal, it would be nicer if these functions were exposed via buttons on the
control tab.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Terminal.png)

_ When all else fails, you can always get it done in the terminal _

  
I'd also like to see the ajax hooks documented (so I could have other devices
interact with OctoPrint via http) and have the ability to have "callbacks" in
the form of URL's that are called when certain interesting events occur like
the completion of a file parsing, target temperature reached, end of print,
etc.  The good news here is that the developer has been extremely responsive
to both bug and feature requests while maintaining the discipline to keep the
program focused on its key objectives (i.e., avoiding "bloat").  
  
OctoPrint is a great example of the advantages of [ Open Source
](http://en.wikipedia.org/wiki/Open_source) software development and in a
later post I plan to outline some of the non-technical aspects of its
development to illustrate why I think OctoPrint could have only existed in an
Open Source environment and why it's important that we keep the future of 3d
printing (and all other technology) open.  
  
  
  
  
  
_*to access OctoPrint outside of the network your printer is attached to
you'll have to open up your firewall/router to let those requests in.
Currently OctoPrint doesn't provide a means to authenticate requests, so until
it does, this would probably be a "bad idea"._  
  

---
title: Ok, goldfish. Delivered, now to work on the zip line...
date: 2009-06-19
tags:
  - evernote
---
  
  

---
title: Once Upon a Time I Wrote a Book
date: 2016-09-03
tags:
  - evernote
---
A while back I wrote a book; well... _most_ of a book.  

  

It started with a Tweet from a publisher, re-tweeted by a friend, that showed
up in my feed.  The publisher was looking for authors interested in writing
technology books for kids.  I had been considering writing a tech book for
years, even started a few, and I had recently posted a detailed blog post on
how to 3D print objects created in Minecraft, so I thought I'd reach out and
give it a shot.  

  

I had a great conversation with the publisher on Twitter, which turned into an
email conversation and eventually I was taking the next step and submitting a
proposed table of contents for the book.  The original title was "3D Printing
with Minecraft", and I designed the chapters around the idea of teaching the
basics of 3D printing through exercises that could be carried-out using
Minecraft.  The approximate page-count came out to about 100 pages, and I felt
like it was something I could do well, essentially expanding on my original
blog post.  The book would be structured like my favorite electronics book,
Forrest M. Mimm's " [ Getting Started in Electronics ](http://amzn.to/2bXWscq)
" with the first part covering theory, and the second part containing a
collection of exercises.  

  

The publisher thought this was OK but they were looking for something more
substantial, so we went through a process of notes and re-working the TOC.
The end result was something more like 200-300 pages and was a lot more like a
typical tech book.  It was now evolving into the eventual title "3D Printing
with Reprap", and the audience was no longer kids but beginners in general.  

  

At this point I probably should have stopped.  The book was no longer the book
I wanted to write, and there were other constraints that came up which I
wasn't comfortable with and I knew would slow down my work, but this is the
first time I'd written a "real book", and so I trusted the professionals and
pressed-on.  

  

There was also the money.  I was promised an "advance" that I thought I could
put toward parts  & equipment I'd need for the exercises part of the book.  To
me this was the big difference between writing the book on my own and working
with a publisher, I wouldn't have to carry the financial weight of getting it
done alone.  

  

Ultimately, I talked myself into continuing and began submitting chapters to
my editor.  This was when I realized another benefit of working with a
publisher: someone else would be editing my work!  I could focus on putting
words and pictures on the page and someone else would make sure that my output
would compile-down into proper English.  This elation didn't last long
however, because the feedback I received resulted in more than simple grammar
and formatting notes and ultimately, changed the book radically.  

  

The overall push of the editors notes was to make the book integrate exercises
with theory, again making it more like a typical tech book.  Additionally
there was a push to use different art, to move away from the hand-drawn style
I had  included (created by Jamie) and replacing it with primitive 3D models
and screenshots, something like what you might see in "Windows XP for
Dummies".  Again this felt wrong, but  I reminded myself "they are the
professionals", and anyway, one more chapter and I should reach the threshold
to receive part of my advance, and I can focus on the fun part of building &
writing the example projects.  

  

So I furiously typed away, pushing through OpenOffice crashes (did I mention I
was forced to use OpenOffice?), ripping apart chapters and re-structuring,
including the new art and emailing changes to the editor.  After turning in
more chapters than my contract defined as necessary to begin receiving the
advance, I didn't hear anything about it, so I asked.  The response was that
they chapters didn't count until they were _accepted_ , at which point I'd
start to receive the advance.  

  

At this point I started hearing less and less from my editor, and responses
that came took longer and longer.  Excuses were made and to fill the gaps I
started working on later chapters until the book was almost complete.
Eventually a month passed between submitting my latest edits and receiving a
response, so I decided to reach-out to the editor and see what was going on.  

  

I never found out exactly what was going on, but I received a long apology and
was informed that the editor was being taken off my project.  In her place the
publisher would take over and delegate where necessary to a team of editors so
we could get the project back on track.  I was excited about this, I was
starting to think that things were falling apart because of me, the quality of
my work or lack of experience.  It was a relief to know that things felt wrong
because something _was_ going wrong, and now it was going to  be made right.  

  

The publisher reviewed the work so far and a week or so later responded with
an email that left me aghast.  Long story short, he wanted to throw out
everything that had been done up to this point and suggested a new structure,
_which happened to be just like my original one_ .  In a nutshell, everything
that the editor had made me change since the beginning of the project took the
book down the wrong path, and we essentially wasted a year and countless hours
doing what I felt was wrong.  I tried to explain to the publisher that I had
pushed against these changes, but it basically fell on deaf ears, and at this
point it became less about fixing what the editor had done wrong and more
about how I was providing unusable content, and would essentially have to
start over.  

  

Coincidentally, this also meant that I'd be going back to square-one in terms
of receiving any advance.  Bear in mind that this is over a year after the
start of the project, and I've made significant personal sacrifices in terms
of time and money to work on the book, and I've yet to see a dime of
consideration from the publisher.  This was also around the time that Jamie
received a cancer diagnosis, and I had been hoping that I could recover some
of the money I had put into the book project to help cover medical expenses.  

  

I made a valiant attempt to re-work the first chapters to meet the
requirements of the new structure and hit the mark to receive part of the
advance.  I went around-and-around with the publisher-editor who changed his
mind about the direction of the chapter (or entire book) each time I submitted
an edit.  This, coupled with endless technical difficulties exchanging
documents resulted in another few months passing by with no end in sight.  

  

I was still hopeful that we could complete the project successfully, in a way
that would benefit everyone involved, but the external pressures on me reached
a point where I needed some support from the publisher to continue.  I wrote a
heartfelt email to the publisher explaining all the details of my personal
situation, and a explanation from my perspective about how we reached this
point, and requested the initial payment of the advance.  What I received back
could be summed-up as:

  

_That sucks, but your book is now a mess, and I can't tell you when we'll be
ready to compensate you._  

  

At this point I walked away from the project for awhile and thought about my
options.  In filmmaking (in particular, editing), I'd been introduced to the
term "kill your darlings".  The idea is that you can fall in love with work
you've done on the film, that will make it harder for you throw them out
during the edit, even if you should to make the entire film better.  This was
probably one of those times, and I probably should have burned then entire
book and chalked it up to experience, but it was too hard, I had sunk so much
time and thought and care into the project I couldn't bring myself to walk
away.  I was also too optimistic, and I had a mission in mind for this book,
to turn newcomers to 3D printing on to the [ Reprap project
](http://reprap.org/) , and I thought this was too important to throw away.  

  

The one upside of having received zero compensation from the publisher is that
it makes a good case for our contract being invalid.  Armed with this, I
decided to go it alone, request cancellation of my contract and publish the
book on my own.  I wrote an email to the publisher explaining the situation,
and received a gruff and curt response, including a signed letter ending our
contract.  

  

I converted the entire contents of the book to Markdown for publishing via [
LeanPub ](https://leanpub.com/3dprintingandreprap) .  I roped Jamie into
editing the book, and we started working on turning into something worth
reading.  This continued for awhile, but it became apparent that much of the
content would have to be re-done because it was written two years ago, and so
much had changed in 3D printing, it was no longer relevant.  I believe I made
a valiant effort at this, but combined with everything else happening around
me, it became intractable.  

  

That brings us to today.  It's been several months since I committed any
changes to the book, and every day the relevancy of the existing content
rusts.  What I've learned from the process is indispensable, but the body of
work is probably less so.  In the off-chance that it is of any value to
others, I have made the repository containing the manuscript public, and I'm
happy to accept pull requests.  

  

If you'd like to contribute, you can find the source for the book on Github
here:  

  

[ https://github.com/jjg/3d-printing-with-reprap ](https://github.com/jjg/3d-
printing-with-reprap)  

  

I have also published the book in its current state and made it available for
free on LeanPub.  You can find and download the current version here:  

  

[ https://leanpub.com/3dprintingandreprap
](https://leanpub.com/3dprintingandreprap)  

  

My advice to other authors-to-be is, if it's at all possible to publish your
book without a publisher, do so.  If you are writing a book that requires
capital up-front to pay for research, equipment, ramen noodles, etc., make
sure any publisher you engage with intends to pay you up-front.  We have so
many alternatives to the traditional publishing industry that it would be a
shame to ignore them, and I hope if nothing else this post helps aspiring
authors avoid experiencing what I have, and they are able to share their work
with the world.  

  

  

  

  

  

  

  

  

  

---
title: One Hour Cinema
date: 2011-07-20
tags:
  - evernote
---
One Hour Cinema (OHC): write, shoot, edit and release a film in one hour or
less.

  
  
  
Around 2005 I started working in Milwaukee which was a change from Madison
where I had worked previously for over ten years.  I was writing software for
a living but I had a renewed interest in becoming a filmmaker.  
  
  

With the extended commute and a young child at home the time I had to dedicate
to filmmaking was very limited, however I didn't know anyone in Milwaukee so I
spent my lunch hours for the most part alone.  This is where One Hour Cinema
was born.

  
  

I was reading several books on filmmaking at the time but the one that stood
out was called [ Practical DV Filmmaking ](http://www.amazon.com/Practical-
Filmmaking-Second-Russell-
Evans/dp/0240807383/ref=sr_1_1?ie=UTF8&qid=1311168731&sr=8-1) .  The book
provided a series of exercises which taught the skills necessary to produce a
film shot on digital video.  I didn't have the time (or resources) to carry
out most of the exercises however I adapted the lessons to fit within the one
hour lunch break I had and used the equipment I had on-hand (a cheap digital
still camera and a laptop).

  
  

I did this for about a month and shot about one film per week.  This resulted
in three films (the fourth was not completed due to equipment failure),
"Neutron Man", "The Old German Church" and "$1 Lunch".  "The Old German
Church" went on to be the first film that I screened in front of a public
audience.

  
  

The experience taught me how to be resourceful and recover from problems
quickly, as well as how to prioritize the work to meet a deadline.  While I
can't say absolutely that this improves all artistic processes, for me it was
a way to move toward my goal of becoming a filmmaker and gain the necessary
skills within the time and equipment restrictions I had available to me.

  
  

From time to time I have considered producing another series of OHC films.
The capabilities of mobile devices such as the iPhone dramatically exceed the
tools I originally worked with in both image capture quality and editing
facilities (not to mention capacity and battery runtime), so the format may be
exceptionally well-suited for these devices.  In fact I am in the planning
process of teaching another filmmaking class and perhaps this will be part of
the curriculum.

  
  

If you make a OHC film please let me know, I'd love to see it!

---
title: Oracle 1.1
date: 2001-02-07
tags:
  - evernote
---
We've added one (count 'em, one!) new feature to the Oracle. In response to
several requests, you can now see the last five questions that were answered.  
  
We're planning on releasing Oracle 2.0 within a week or so, depending on how
the server rebuild goes (oh yeah, we're rebuilding the webserver this weekend,
so they site won't be here, etc). The next version will be even smarter, have
some cool gadget features, and will even be able to answer your questions in
realtime! This is going to take some considerable hardware upgrades to happen,
so I've been thinking of adding a tip jar to the site, what do you think?  
  
As always, with a new post comes a new survey, so be sure to plunk down your
two cents on the current poll page and let us know how you really feel.

---
title: Parts, parts parts!
date: 2007-11-28
tags:
  - evernote
---
The carb kit arrived (I decided to go with the carb kit) and seeing all those
bright, shiny bits gets me all revved up to be working on the bike. Now that I
can finally re-assemble the carbs I'm thinking about doing some kind of crazy
stop-action animation of the assembly or a dramatic montage of the process...  
  
...or maybe I should just get it done...  
  
As far as the rest of the movie goes, things are going well and we've got the
first interview under our belts. I should be able to post a few stills from
that day shortly, Mike was busy with the clicky-clicky camera that day so we
might have some real photographs (as opposed to frames swiped from the film).  
  
The next thing I need to concentrate on is setting up the remaining
interviews. From some of the emails I've received and the statistics for this
and our company website ( [ http://2soc.net ](http://2soc.net) ) it looks like
interest is growing in the project and we've received many kind words from
motorcyclists out there anxious to see the finished product. As of now we're
still on schedule for 2008 and if everything goes as planned we should be
screening the film in the late summer/early fall. It's wayyy to early to be
talking about release dates but anyone reading this blog should already be
accustomed to seeing things change from week to week.  
  
If we have time we may begin posting some video from the film as it
progresses. It's allot more work than just typing and posting random pictures,
but if there is interest from the audience that might motivate us to exert the
effort. Drop us a comment here if that's something you'd like to see.  
  
For now, I should get back to those carburetors...

---
title: Photographs
date: 2007-07-12
tags:
  - evernote
---
Finally got around to adding the pictures to the last post.  
  
So far I'm loving the VQ1005, and so is Libby:  
  
[
![](http://lh3.google.com/jason.gullickson/RokEx6TiBzI/AAAAAAAAAcg/hvZ2QnmU9VM/s400/IMG_0010_5.JPG)
](http://picasaweb.google.com/jason.gullickson/VQ1005Photos/photo#5082598909975594802)

---
title: Pimpin' (it actually is easy) the apps
date: 2009-06-20
tags:
  - evernote
---
  
  

---
title: Portals Part Two
date: 2012-12-03
tags:
  - evernote
---
[ Awhile back ](http://www.gullicksonlaboratories.com/windows-portals-and-a
-replacement-for-the-internet/) I noodled on the idea of a device that was
simply a portal to the Internet and nothing more. Yesterday I was reading [
this post ](http://m.linuxjournal.com/content/swap-your-laptop-ipad-linode)
and that combined with my own reassessment of my portable computing needs
brought the idea to mind once again.  
  
To some degree these devices exist. The [ Litl Webbook
](http://litl.com/webbook/meet-webbook/overview.htm) (and later, the
Chromebook) function in this way however both have the trappings of their
personal computer ancestors and therefore disregard any innovation that could
be had by eschewing the old forms and re-engineering the device and experience
in light of this new way to interact with information via the device.  
  
[ Boot 2 Gecko ](https://wiki.mozilla.org/B2G) is a project closer to this
ideal, and while it's currently feasible to do so, I believe that marring this
device to the Web is too limiting for my application (additionally B2G is
designed with the limitations of current devices in mind, which make sense for
its goals, but which make it non-ideal for for what I am describing).  
  
In the early 90's I was drawn to the work of [ Jaron Lanier
](http://en.wikipedia.org/wiki/Jaron_Lanier) and other pioneers of [ Virtual
Reality ](http://en.wikipedia.org/wiki/Virtual_reality) (in fact that was what
I told the counselor I wanted to work on after high school, and they suggested
technical college...?). For a couple of years I worked on a personal computer
operating system centered around the Virtual Reality experience (Virtual
Reality Operating System or VROS, creative name I know...) but continually
bumped-up against the cost and limitations of interface hardware at the time.  
  
The cost of head-mounted displays has been steadily dropping since the initial
excitement about them in the 1999's and the quality has been improving well
(in fact I was very close to [ backing a Kickstarter project
](http://www.kickstarter.com/projects/1523379957/oculus-rift-step-into-the-
game?ref=search) for a very nice display and now I'm kicking myself for
missing it).  
  
Non-physical interface devices are also becoming more viable, there has always
been experimental interface devices that use EEG or other biological signals
for input to computer software but recently there have even been [ consumer-
grade devices ](http://www.emotiv.com/) of this sort. Additionally, speech and
gesture recognition have improved to the point where they are good enough to
be used on a daily basis and are incorporated in many consumer devices.  
  
Finally, starting with the iPhone, embedding an array of sensors (proximity,
acceleration, gyroscopic, etc.) has become commonplace in many devices and
developers are getting very creative with ways to leverage these inputs in
software applications. I think everyone was surprised at how quickly software
developers embraced these new forms of input and the impact it has had on user
interface design.  
  
The availability of these interface technologies, combined with a market
interest in in the idea of a device that provides a transparent connection to
the Internet indicates to me that the time may be drawing near for a personal
computing experience akin to what I was working on in the early 90's but was
beyond both the technology and psychology of the time. It may finally be the
right time to resume that work.

---
title: Pretty Good Privacy
date: 2016-08-16
tags:
  - evernote
---
Perhaps inspired by upcoming events, I decided to get things in place to
facilitate secure communications with my compatriots.  After spending years
considering various options, I've settled on [ The GNU Privacy Guard
](https://gnupg.org/) (GPG), an open-source implementation of the [ OpenPGP
standard ](http://www.ietf.org/rfc/rfc4880.txt) .  

  

Many systems offer secure communications but most rely on services or software
which belong to, or run on, third-party systems.  Most of these systems are in
turn owned by private corporations.  Regardless of how thick your tinfoil hat
is, even under the best of circumstances these companies can be bought, sold
or simply fail, which leaves your ability to communicate (and the privacy of
your previous communications) in a dubious position.  OpenPGP -based privacy
is a bit more effort, but all you need to use it is a personal computer and
the GPG software, which runs on almost any working computer you can find.  

  

First, some nomenclature:  

  

  * **OpenPGP** is a _standard_ that defines the way data is encrypted and decrypted, and how the associated keys, identities, etc.   

  * **GPG** is an _implementation_ of the OpenPGP standard, a piece of software that can be used to encrypt and decrypt messages.   

  * A **public key** is something you can share openly with people you want to communicate securely with   

  * A **private key** is something you use to decrypt encrypted data sent to you, and you must keep it secret   

  

Besides GPG, there are other implementations of the OpenPGP standard.  Any
piece of software that implements the standard can decrypt data encrypted by
another, so long as the proper keys are in place.  So, if your friend uses a
commercial encrypted email package that uses OpenPGP-compliant encryption, you
can read their messages using GPG and vice-versa.  

  

**Using GPG**  

There's a wide-range of tools designed to make using GPG easier, but I
recommend starting with the basic command-line tools so that you have a more
complete understanding of what's going on.  One of the key problems with
creating secure communications is the ability to loose track of what's
happening to the information you want to secure.  Once the cat is out of the
bag, you can't put it back in.  

  

I won't go into great detail about setting up GPG, plenty of other people who
know a lot more about have done so already (I'll provide some links below).
What I will describe are the steps involved in sharing an encrypted document
with a college so you can get a feel for what's involved.  

  

A very common situation where you need to encrypt information is when you want
to share account information with someone else*.  How this is often done is by
writing down the information and sharing it in person, or sending the user
name and password via separate means.  In both these cases, the credentials
are potentially stored somewhere along the way, which makes them vulnerable.
On the other hand, if you use OpenPGP, you can encrypt a file containing the
account information and share it via any means you like with no fear of it
revealing the secret information.  

  

So for example, let's say I want to share my Netflix account with a trusted
friend; here's how I would do it:  

  

First I create a file containing the user name  & password for my account:  

  

  

Second, I encrypt this file specifically for my friend:  

  

gpg --output netflix.txt.gpg --encrypt --recipient ralph@nader.org netflix.txt  

  

Finally, I email the encrypted file.  

  

When my friend receives the email, he downloads the attached file, and
decrypts it  

  

gpg --output netflix.txt netflix.txt.gpg  

  

Then he can watch some shows, change my credit card information or sell the
account to the Russians**  

  

At this point you might be asking _"How come Jason's friend can decrypt the
file but random Internet people can't?"_ .  This is where the "trusted friend"
part comes into play.  

  

Before you can share an encrypted file with someone, you will need their
public key.  Once you have someone's public key, you can add it to your GPG
keychain and create encrypted files which can only be decrypted by the
intended recipient.  This is what is meant by a "trusted friend".  

  

It's worth pointing out that it's not necessary for both parties to share keys
in order to send a message.  Since there is no harm in sharing public keys,
many people include their public keys in email and other communication, or
post them on their websites, etc.  If, for example, you wanted to send a
secure message to a reporter and they share their public key on their blog,
you can add that key to your keychain, encrypt a message for only that person
and safely send it over the Internet.  If the reporter decides to reply to
you, they may request your public key in order to encrypt the response so that
only you can read it, but this isn't a prerequisite for you to send the
initial message.  

  

It's worth pointing out again that you need to protect your private key.  If
anyone were to get a hold of it, they can decrypt any data that you ever
encrypted with the key.  There are numerous ways to avoid this, or to minimize
the damage if it happens, but the important thing is that you are aware of how
critical it is to keep your private key private.  

  

Conversely you need to keep track of the private key as well, because if you
loose it, you can no longer decrypt any data that was encrypted with the key.
Arguably this is better than someone else getting access to your private key,
but not much better.  

  

Used correctly, OpenPGP is extremely effective at keeping secrets.  Possibly
more important than the encryption itself is the fact that communicating
securely using OpenPGP relies only on the two trusted parties involved in the
conversation.  It does require some premeditation in order to establish trust,
and that's not necessarily a bad thing, but that means it's a good idea to get
things setup before you need them.  

  

There is a common misconception that encryption is only needed by criminals or
perhaps the press or the government, but as you can see from the example
above, there are everyday situations where having the ability to send private
information between trusted individuals is handy and necessary.  We tend to
delegate responsibility for our privacy to others and expect them to provide
secure means of communication, but with OpenPGP we can guarantee privacy
ourselves to a degree far beyond what is possible by depending on an outside
entity, company, etc.  

  

If you'd like to get started using GPG here's a few links to more detailed
information.  If you'd like to send me an encrypted message to test your
setup, I've included my public key below.  

  

  * A somewhat terse introduction directly from the source: [ https://gnupg.org/gph/en/manual.html ](https://gnupg.org/gph/en/manual.html)   

  * A slightly less terse introduction aimed at Windows users: [ http://www.glump.net/howto/cryptography/practical-introduction-to-gnu-privacy-guard-in-windows ](http://www.glump.net/howto/cryptography/practical-introduction-to-gnu-privacy-guard-in-windows)   

  * A video aimed at reporters: [ https://www.youtube.com/watch?v=CU861f5szsQ ](https://www.youtube.com/watch?v=CU861f5szsQ)   

  * A high-level guide to setting up encrypted email (this relies on tools that avoid the command-line interface described above, I don't recommend starting here but once you understand OpenPGP it's a convenient workflow): [ https://emailselfdefense.fsf.org/en/ ](https://emailselfdefense.fsf.org/en/)   

  

  

My public key, should you like to get in touch:  

  

\-----BEGIN PGP PUBLIC KEY BLOCK-----  
Version: GnuPG v1  
  
mQENBFeyZxYBCADD1Cvw8GlmDf3t3gVor/+97jKrjhE1t79U7ysRpTcX4HARN5DU  
Zt+uGZHsMC/SBfhJGkDxnarOeIdfKKdqNgKE6HcjGP92z+h8RGFR0qoj5wxX3qns  
54fJARjSfxnJHKbmoV5hHU8AGiB293a2p44vwlO7S/dVFxglzZ/kVTG9vVtYhlCL  
dxIPALDhMtkL2qnJJgXlGvGdNkU66LI5flKOAvcXFaWxalxmyvBneQ6fve/fFkQd  
VAqY336/bIJ4Qq1RsK0KgKTnidgW+1iZeep3mZS/vPH51iSQ1pSYDcXx0yX3tQtH  
aSFoZmAvmY4sE0KtBqgfjih+wXw3ExepOQT7ABEBAAG0P0phc29uIEouIEd1bGxp  
Y2tzb24gKG9uZSBtYW4gU2t1bmt3b3JrcykgPGpndWxsaWNrc29uQDJzb2MubmV0  
PokBOAQTAQIAIgUCV7JnFgIbAwYLCQgHAwIGFQgCCQoLBBYCAwECHgECF4AACgkQ  
9qxpfgzEoNrzSQgAoccj9KNq1i4MoPw57FXFO5+5mvYxMUjBnkSa5EE3izvLZnzb  
Pu2jWjDtflCOPh5cVUJUSQU1PVS25eCpfVoupMCiZBeYBMhhMVTun7YvK5Q5DoiK  
MwWEaTSi/YyVk+2lRdVtAM5YkGw4nppK3ibDd0lOEwprOb4OPWFHDubvR588QPhU  
lcg7d24oE+UhpxjydaFCHDmJlnN8CSdmh9XzaBkljzfLznb0zgOtwk5Pn9sYJYKs  
uWd+lyD0csn9I2wJAsg4eR3gJUxiaSfwBIXlrRymLSjIFoBxiWp7ACDY+uaowG1y  
b0CUrZG2UpXGSaRHjuYfCt+a3bVNCiWAf9RB5bkBDQRXsmcWAQgAvHmDiaE8PKyB  
dV7FxMCmETnUXuM0/sXvpC6WIStWcDDvW41FdDhGNQQtSKnO9Z+owJ2HFV1Mqp2A  
PtYPZNMQ9+6JUSC8pNipKFgSynKC8SplPeXIxyUYq/hBD3A4UQ7zFNsMDLwwtsbS  
FMjAn7YeH0crmwpFxjvIgNbbi9eVUokgUvDKqZr0VHmP8fmJiDX8QUCNq8J23wz8  
Ua6rEH5gwMRabwCxInn83BPBCtxDFm8EEW+KwJHawaONQcSjnpGfqewTUZhZ6sXq  
rfie8MHWfdrKeidsgNmJ7Bt78mnANsFUybntcsGoE4KKqkMVZoOzxG9Y4SCPSPmv  
G3MQja7dtwARAQABiQEfBBgBAgAJBQJXsmcWAhsMAAoJEPasaX4MxKDa+NwH/A4a  
xv3cQsmW+9WsPWVXVV/CpDf6PhLxjYMka9MgpFCXy9ZQFT6D3liol9p9bjT3iFnH  
vXoQxH8yoTOcadFmm2jaxiuj4mYplq+Iz1TP4dqNOQtOJytiZ8qCOKwL2rAMErl0  
t2voTsTBIX+C60uT5WCdm38LZsAKQsZGm96MFRtolI7al6HcHfq5GYD3AV9eQa2n  
ybnsxi1/BS5cFUK781dpfubf9O78g7JFlHJRaRGl3tks7ybKo+DKgxj7vJFhJhp6  
Ur5XuMkda8V6rDCDdv7fHdtHgqAQewL0LYLYJ4RBLKDB+IC6DMWKdhrUi66/2tuz  
beMs4u8mxFBNt91GUsw=  
=Oeiz  
\-----END PGP PUBLIC KEY BLOCK-----  
  

  

* It goes without saying that sharing accounts is in general a bad idea, but there are times when it's necessary.   

  

**This raises an important point about using encryption.  Even though the data
is secure in transit, once it's decrypted anyone can use it, so it's important
that the people you share with understand this and don't store this decrypted
information in vulnerable places.  

---
title: Print quality is getting better and more consistent
date: 2013-01-08
tags:
  - evernote
---
Here's a few recent prints from my [ Reprap
](http://www.gullicksonlaboratories.com/projects/reprap/ "RepRap") .  After a
fair amount of tuning the quality has dramatically improved, but more
importantly, the consistency and repeatability is getting to a point where I
can print many simular and different parts, and they fit together with simular
tolerances.  
  
![Ball-chain pulley for upcoming drawbot
project](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/01/IMG_20130107_241901_118-300x225.jpg) Ball-chain pulley
for upcoming drawbot project  
  
The Ultimachine filament I picked up seems to have greater dimensional
stability than the Faberdashery samples I have, although the colors of the Fab
filament are nicer as the texture is a little more pleasant as well so I can
see why it's popular.  For my purposes engineering accuracy is more important
so I'm happy the Ultimachine filament is fitting the bill in that regard.  
  
![Open-Source Hardware logo, upside-
down](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/01/IMG_20130107_131819_981-300x225.jpg) Open-Source
Hardware logo, upside-down  
  
One thing that came to my attention through the last tests is that I have one
of my axis inverted.  This seems like something you'd notice but since a lot
of the things I print are symmetrical  it wasn't apparent until I printed some
objects with letters on them (or this iPhone case with the camera opening on
the wrong side...).  
  
![Domestic Violence Awareness iPhone case \(prototype,
apparently\)](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/01/IMG_20130107_070152_297-225x300.jpg) Domestic Violence
Awareness iPhone case (prototype, apparently)  
  

---
title: Public Service Announcement
date: 2000-11-17
tags:
  - evernote
---
NOTICE OF REVOCATION OF INDEPENDENCE

  
To the citizens of the United States of America, In the light of your failure
to elect a President of the USA and thus to govern yourselves, we hereby give
notice of the revocation of your independence, effective today. Her Sovereign
Majesty Queen Elizabeth II will resume monarchial duties over all states,
commonwealths and other territories. Except Utah, which she does not fancy.
Your new prime minister (The rt. hon. Tony Blair, MP for the 97.85% of you who
have until now been unaware that there is a world outside your borders) will
appoint a minister for America without the need for further elections.
Congress and the Senate will be disbanded. A questionnaire will be circulated
next year to determine whether any of you noticed. To aid in the transition to
a British Crown Dependency, the following rules are introduced with immediate
effect:  

  

  1. You should look up "revocation" in the Oxford English Dictionary. Then look up "aluminium". Check the pronunciation guide. You will be amazed at just how wrongly you have been pronouncing it. Generally, you should raise your vocabulary to acceptable levels. Look up "vocabulary". Using the same twenty seven words interspersed with filler noises such as "like" and "you know" is an unacceptable and inefficient form of communication. Look up "interspersed". 
  

  2. There is no such thing as "US English". We will let Microsoft know on your behalf. 
  

  3. You should learn to distinguish the English and Australian accents. It really isn't that hard. 
  

  4. Hollywood will be required occasionally to cast English actors as the good guys. 
  

  5. You should relearn your original national anthem, "God Save The Queen", but only after fully carrying out task 1. We would not want you to get confused and give up half way through. 
  

  6. You should stop playing American "football". There is only one kind of football. What you refer to as American "football" is not a very good game. The 2.15% of you who are aware that there is a world outside your borders may have noticed that no one else plays "American" football. You will no longer be allowed to play it, and should instead play proper football. Initially, it would be best if you played with the girls. It is a difficult game. Those of you brave enough will, in time, be allowed to play rugby (which is similar to American "football", but does not involve stopping for a rest every twenty seconds or wearing full kevlar body armour like nancies). We are hoping to get together at least a US rugby sevens side by 2005. 
  

  7. You should declare war on Quebec and France, using nuclear weapons if they give you any merde. The 98.85% of you who were not aware that there is a world outside your borders should count yourselves lucky. The Russians have never been the bad guys. "Merde" is French for "shit". 
  

  8. July 4th is no longer a public holiday. November 8th will be a new national holiday, but only in England. It will be called "Indecisive Day". 
  

  9. All American cars are hereby banned. They are crap and it is for your own good. When we show you German cars, you will understand what we mean. 
  

  10. Please tell us who killed JFK. It's been driving us crazy. 
  

  
Thank you for your cooperation.  
  

---
title: Pullstarter - Crowd-sourced funding for ideas
date: 2013-04-06
tags:
  - evernote
---
You're probably familiar with [ Kickstarter ](http://www.kickstarter.com) (if
not, click over there and we'll be here when you get back).  Kickstarter is a
prime example of a Crowdfunding site used to raise money for projects without
the overhead of traditional investment.  
  
This is a good thing, and many cool projects have received funding through
Kickstarter campaigns, and it's been great for people who want to make
something and just need the funding to see it through.  But what if you just
have an idea?  
  
Conversely, what if you just love to build stuff but don't know what you
should build, how to find an audience or how to value one idea over another?  
  
I've had a number of people come to me with these same questions, and after
conversations with [ trusted ](https://twitter.com/southpolesteve) [ advisors
](https://twitter.com/wiscoDude) the idea behind [ Pullstarter
](https://github.com/jjg/pullstarter) was born.  
  
**How Pullstarter works:**  

  

  1. You list and idea for something, and how much you'd be willing to pay for it 
  

  2. Other users who are interested in the same idea bid on it as well 
  

  3. When the bids get high enough, a Maker takes on the project 
  

  4. When the project is delivered, the Maker is paid and the bidders get what they wanted 
  

  
  
  
**An example:**  
  
I want a flying bicycle, and I'd be willing to pay $500.00 for one.  So I
create a new idea on Pullstarter and provide some details about what I want.  
  
[ ![Who doesn't need a flying bicycle?](http://www.gullicksonlaboratories.com
/wp-content/uploads/2013/04/Screen-Shot-2013-04-05-at-4.58.22-PM-300x264.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/04/Screen-
Shot-2013-04-05-at-4.58.22-PM.png) Who doesn't need a flying bicycle?  
  
My idea gets added to the list of ideas displayed on the site:  
  
[ ![Screen Shot 2013-04-05 at 4.58.41
PM](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/04/Screen-
Shot-2013-04-05-at-4.58.41-PM-300x35.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/04/Screen-
Shot-2013-04-05-at-4.58.41-PM.png) Let the bidding commence!  
  
Then Bob logs in and sees my idea.  He likes it too, but it's only worth
$300.00 to him.  He bids on the bicycle idea and now it's worth $800.00 to
whoever can complete it.  
  
![Screen Shot 2013-04-05 at 5.01.00 PM](http://www.gullicksonlaboratories.com
/wp-content/uploads/2013/04/Screen-Shot-2013-04-05-at-5.01.00-PM-151x300.png)  
  
Other users may join in the bidding, some may just "follow" the project to see
what happens, and a summary of ideas, following and bidding is displayed while
they are logged into the site.  
  
Eventually, the value of the project reaches $1M which gets the attention of
Julia, who just happens to want to build a flying bicycle but never thought
anyone else would want one.  She looks at the number of bids (586) and divides
this by the value of the idea and decides that yes, she could produce 586
flying bicycles with $1M.  
  
Now, placing a bid is a legal commitment, so Julia knows if she can deliver
the bikes she will receive the funds so she takes on the risk of securing the
necessary credit, etc. to product the product and fill the orders.  
  
Julia develops the bikes and delivers the first one to me (since it was my
idea).  I love it and consider her product acceptable, Julia delivers the
remaining bicycles and the funds are released to her account.  
  
This is of course a rather dramatic example, and clever makers will take on
ideas that they can carry out without having to beg or borrow too much to put
together the product.  But I used big numbers because they are more exciting.  
  
If you think this is an interesting idea, leave a comment and we can discuss.
Awhile back I spent a couple hours and threw together a prototype for the
system which you can check out on Github:  
  
[ https://github.com/jjg/pullstarter ](https://github.com/jjg/pullstarter)  
  
I'd love to hear what you think about it.  
  

---
title: QU-BD Experiments
date: 2013-02-18
tags:
  - evernote
---
I blew-up the heating resistor in my J-Head so while that was down awaiting
parts I thought I'd take another stab at getting the QU-BD extruder to work.  
  
The first step was a modification to the filament drive designed by [
whosawhatsis ](http://www.thingiverse.com/whosawhatsis) ( [
http://www.thingiverse.com/thing:15718
](http://www.thingiverse.com/thing:15718) ).  Fortunately I happened to print
these parts before my J-Head croaked so I had them around to test with.  
  
Next I was happy to find that since my last attempt at getting these extruders
to work someone had published a resistor table that they could demonstrate
good results with.  
  
I updated my firmware settings with this table and made the necessary steps-
per-mm adjustments and then re-ran the PID autotune.  
  
After several tests, the exact same problems from my earlier attempts to get
these to work exist.  I tried one more time with a more extensive resistor
table to make sure I exhausted every option.  
  
I ran the PID autotune again and flipped over to Google to see if I could find
any more nuggets of wisdom that I might use to get something useful out of
this hardware when I noticed a stream of smoke pouring out of the nozzle.
Apparently the resistor table I used wasn't quite right and the hot-end was on
it's way to meltdown.  This is when I noticed that setting the temp to 0
doesn't override the PID tuning procedure and it was time to hit the E-Stop
(i.e., pull the plug).  
  
Thus ends another chapter in the futility of trying to do something useful
with the QU-BD extruder.  I guess I'll be setting the printer aside until my
replacement parts arrive...

---
title: QU-BD Extruder Adventures
date: 2012-11-26
tags:
  - evernote
---
Awhile back I picked up a pair of [ QU-BD extruders ](http://store.qu-
bd.com/product.php?id_product=19) as part of their [ Kickstarter campaign
](http://www.kickstarter.com/projects/qu-bd/open-source-universal-3d-printer-
extruder-dual-ext) .  I purchased these after destroying my [ Makergear hot-
end ](http://www.makergear.com/products/operators-pack) (twice!) on my [
RepRap Prusa ](http://reprap.org/wiki/Prusa_Mendel_\(iteration_2\)) .  After
half a year of wresting with the machine, the plan was to "buy my way out" of
extruder problems and get on with the printing.  
  
Needless to say, things didn't go as planned.  
  
I won't go into details here, but suffice to say the QU-BD extruders were not
a "bolt on and go" operation for me.  After struggling with them for a few
weeks and discussing simular issues with other users on their [ support forum
](http://www.fabric8r.com/forums/forumdisplay.php?6-Official-QU-BD-
Troubleshooting-amp-Support-Forum) , I arranged to send my extruders back in
exchange for a pair that had been tested and known-good.  
  
I am currently awaiting the arrival of these replacements.  Unfortunately the
pair I sent in checked-out as working by QU-BD staff so there's probably no
chance of the new ones performing any differently unless I make some changes;
that's what this series of blog posts will be about.  
  
I'm in the process of assembling a list of things to try to get these
extruders to work with my setup.  I've found a number of things to change
about the way I was working with them before and a lot of ideas have been
gathered in the support forum since the last time I took a stab at it, so
there is hope.  
  
I'll be posting each experiment here under the [ qu-bd tag
](http://www.gullicksonlaboratories.com/blog/?tag=qu-bd) in hopes that this
information will be useful to other QU-BD users in the future (as well as a
way for me to keep track of what works and what does not).

---
title: Quietshop
date: 2016-08-24
tags:
  - evernote
---
A coffeshop (or equivalent) where communications devices are checked at the
door.

  

The goal of a Quietshop is to provide a shelter from the distraction and
stress of constant connectivity.  When someone is visiting a Quietshop, the
expectation is that they are not immediately available, but they _can_ be
reached in case of emergency.

  

A Quietshop is broken into at least three distinct spaces:

  * A common area where visitors can meet and converse 
  * A quiet area where library rules apply 
  * A reception area where communication devices are checked and monitored by the attendant 

  

Additional spaces of the same or other types may be available, but to meet the
definition of a Quietshop these three areas must be present and acoustically
isolated.

  

When visiting a Quietshop, you enter through the reception area where you are
greeted by an attendant who safely stores your communication devices in a
check area.  If your device demands attention during your visit (you can
define what that means for the attendant) the attendant will make a note of
the request and deliver it to you.  If you need to immediately respond to the
request, you can do so in the reception area.

  

  

---
title: RC1 of the new iPhone app ready...
date: 2009-06-08
tags:
  - evernote
---
  
I just sent RC1 (Release Candidate 1) off to my beta group. I think it's in
good shape, all known issues have been addressed and I'm very pleased with how
it's turned out. I have allot of ideas for future releases but I'm trying to
stick to my original design goals for the first release to prevent the app
from getting stuck in continual beta mode.

If all goes well, I will be submitting the app to Apple in the next few days
and then it's just the waiting game until I hear back. I guess I'll take that
time and go to the bahamas* or something...

(* by Bahamas I mean, work on a bookshelf or something)  

---
title: Remarkable transparency - http -//groups.google.com/group/google-appengine/msg/ba95ded980c8c179?pli=1
date: 2009-07-10
tags:
  - evernote
---
  
  

---
title: Reprap back online (again)
date: 2013-07-02
tags:
  - evernote
---
About a month after seeing smoke pouring from the hot-end of my Reprap Prusa,
we're printing again.  
  
[ ![IMG_0166](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/07/IMG_0166-300x224.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/07/IMG_0166.jpg)  
  
  
  
The crazy part is that the repair was little more than two hours of work (1
hour of diagnostics + 1 hour of actual fixing).  What took a month was
recovering the gumption lost watching the machine fail.  
  
Building and operating a RepRap has taught me so many things.  Most of them I
had already known to some extent (electrical, mechanical and software
engineering, etc.) but also many other more abstract things that I had less
deliberate knowledge of.  
  
The value of community support, of collaborating without overhead and how
determination doesn't mean crashing into the same wall relentlessly.  That
somehow stopping to stare at the wall for a month gives you a chance to see a
crack, a cleavage point where you can strike with 1/1000th of the force and
the wall just falls away.  These are the ideas I want to encapsulate and
communicate in my RepRap book.

---
title: RepRapRevisited
date: 2012-12-31
tags:
  - evernote
---
Over a year ago I started building a RepRap.  During that time I'd say it was
operational for about four months; the rest of the time was spent
troubleshooting, learning, waiting on parts and fixing things (mostly the hot-
end).  
  
In the last two weeks things have accelerated tremendously.  The key to this
success was replacing the Makergear hot-end that came with my kit with a
J-Head; after that things started to come together.  
  
![2012-12-21_14-45-10_178](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-21_14-45-10_178-300x225.jpg) Finally back to
where we left off before the "headaches"  
  
Once I was able to print for more than a couple minutes I stared to find other
areas to improve, and this is where the RepRap really comes into it's zone,
because the RepRap is born to be hacked.  
  
Here's a list of modifications I've made so far:  

  

  * Added these thumbwheels to make leveling the bed easier (possible?): [ http://www.thingiverse.com/thing:39459 ](http://www.thingiverse.com/thing:39459)
  

  * This fan mount: [ http://www.thingiverse.com/thing:13343 ](http://www.thingiverse.com/thing:13343)
  

  * These z-rod isolators to eliminate "ripples" (more on this later): [ http://www.thingiverse.com/thing:20147 ](http://www.thingiverse.com/thing:20147)
  

  * A better z-stop adjuster (attempted twice but still failed to print): [ http://www.thingiverse.com/thing:25383 ](http://www.thingiverse.com/thing:25383)
  

  * An even better fan (the other mount doesn't seem to actually get the air where it needs to go): [ http://www.thingiverse.com/thing:34387 ](http://www.thingiverse.com/thing:34387)
  

  * Another thing to address z-rod wobble: [ http://www.thingiverse.com/thing:34140 ](http://www.thingiverse.com/thing:34140)
  

  
![2012-12-19_08-33-54_447](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-19_08-33-54_447-300x225.jpg) The first mod: a
thumbwheel to make leveling the bed possible...  
  
  
  
I'm still working on addressing the z-wobble.  My prints are starting to turn
out pretty good but I'm still getting a rib pattern on the walls that seems to
come from a bend in the threaded rod that raises and lowers the nozzle.  The
first thing I printed seem to make this better, but it was still present so
I'm trying something more drastic and replacing the threaded rod in the z-axis
alltogether.  We'll see how that goes but I may end up going all the way to a
proper set of leadscrews to fix it once and for all.  
  
![2012-12-22_14-57-07_295](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/2012-12-22_14-57-07_295-300x225.jpg) Not bad
but...still a bit wavy on the sides  
  
I'm also troubleshooting a problem printing circular overhangs, they tend to
get ahead of the nozzle and from the discussions I've had this has to do with
laying a new layer down while the previous layer is still cooling.  I've
printed an improved fan mount/duct that is supposed to help with this but I'm
also going to try and slow down a bit while printing shapes like this to see
if it improves.  
  
![ACA9C548-B415-403B-87BA-E92E31C22018
\(1\)](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/ACA9C548-B415-403B-87BA-E92E31C22018-1-300x225.jpg)
...one fan duct to rule them all!  
  
This might sound like a bunch of problems but really it's been a blast to work
on.  The cool thing about the RepRap is that the more you are willing to do
the better it gets; the printer is capable of producing useful parts as-is
(otherwise it couldn't improve itself) so I could just stop and use it (and to
some degree I have), but I love to hack and this device, more than anything I
can remember working on provides such an awesome reward for someone interested
in putting in the extra work.  
  
![IMG_20121228_151909_960](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/12/IMG_20121228_151909_960-300x225.jpg) Of course not
*all* of the prints are serious engineering improvements  
  
I have an album on Google+ where I'm collecting all of my RepRap related
photos if you'd like to follow along the adventure visually: [ RepRap Album
](https://plus.google.com/u/0/photos/103331171897037694195/albums/5822923079196979745)
.

---
title: Re-store
date: 2007-06-18
tags:
  - evernote
---
I'm re-considering a frame-up restoration.  
  
[ ![](http://digitalcarversguild.com/products/toonpro/jetsonscartoon160.jpg)
](http://digitalcarversguild.com/products/toonpro/jetsonscartoon160.jpg)  
  
On one hand I think doing so will definitely blow my deadline of getting the
bike rideable in time for the fall crud. On the other hand, maybe having it in
boxes in all over the garage will motivate me to make more time for the
project.  
  
Six of one…  
  
[
![](http://lh3.google.com/image/jason.gullickson/RnpkfUs7-YI/AAAAAAAAAaQ/RW5qCPEH3As/s144/IMG_0013_2.JPG)
](http://picasaweb.google.com/jason.gullickson/VQ1005Photos/photo#5078482019109566850)
I blame [ Brillo ](http://www.brillo.com/) , because I finally got around to
giving them a try on the bike's chrome (one of the mufflers and the gas cap)
and I was shocked to see that it actually worked to remove the decades of rust
from the surface, leaving it as brilliant as a mirror. Seeing those funky
Jetsons-esque cans glimmer in the daylight made me re-think the whole "get it
done now and make it pretty later" attitude of this project so far.  
  
Also the girls picked out some great colors for a new paint job and it would
just be easier to do a good job of that with the parts all…apart.  
  
I'll give it a week to decide.

---
title: Robots for Peace and the Million Bot March
date: 2016-11-11
tags:
  - evernote
---
Imagine an organization whose members are robotocists (robot designers &
builders, AI programmers, operators, etc.) who refuse to allow their work to
be used for non-peaceful purposes.

  

This may not appeal to all roboticists, but for those it resonates with, it
could provide a supportive community and an awareness that they are not alone.

  

The Million Bot March is a protest akin to other marches on Washington,
carried out by autonomous or teleoperated robots built by Robots for Peace
members.  The purpose is to raise awareness for the group, and to show people
and industry that there are enough roboticists who believe in peace that it is
not necessary to work with or support those who do not.

  

Of course leading up to a MBM, RfP bots could be used for other demonstration
purposes, appearing a rallies, allowing human protesters to access
demonstrations they could otherwise not attend, etc.  

  

The march has a secondary value.  It is a show of force that demonstrates that
the time has passed where peaceful people can be oppressed through violent
means.  The march shows that the group can meet the enemies of peace on their
own playing field, and that they can meet both the intellectual and physical
challenges that have destroyed peaceful movements in the past.

---
title: Rolling
date: 2007-08-31
tags:
  - evernote
---
The [ Crud Run Diaries ](http://2soc.net/crudrundiaries/) has finally been
"greenlighted" by [ the second society ](http://2soc.net/) . What this means
is that it will now occupy an "official" spot on the production schedule and
things should really start rolling from here.  
  
What happens next is we will be scheduling interviews with anyone involved in
the Run's history as well as time to record major mechanical work on the
project bikes (which means I need to get a list of to-fix items for the CL).
The first of these sessions will be recorded on **September 9th** so I'll have
more to report on after that.  
  
A few other decisions have been made and due to scheduling conflicts (and
frankly, lack of progress on the project bikes) we're targeting the spring Run
of 2008 to get the bikes running and on the road. This will give us most of
the winter to get the work done, and since we live in Wisconsin, there's not a
lot else to do, really…  
  
In the meantime we are looking for stories from anyone who participates or
even knows something about **The Slimy Crud Run** . If you or someone you know
falls into this category drop a comment on this post and we'll get in touch.

---
title: Rosie - A first step toward shrinking the world through telepresence and robotics
date: 2013-04-04
tags:
  - evernote
---
About two years ago I put together a [ proposal for a solution
](https://docs.google.com/document/d/1eg2YgIuBx-
gVpKT1hB3_rL_OwfpoevLcFBpKYiJqY7Q/edit?usp=sharing&authkey=CNSdqu8P) to the
array of problems facing the domestic robot.  You know the kind, the robotic
maid that was part of " [ Tomorrowland
](http://en.wikipedia.org/wiki/Tomorrowland) " in the 1960's (hence the
project name " [ Rosie ](http://www.imdb.com/character/ch0000696/) ") and
"just around the corner" in the 1980's but instead the best we could manage
what the [ Roomba ](http://en.wikipedia.org/wiki/Roomba) .  
  
There's a number of reasons we never got our robot butlers.  Over time some of
these have been addressed by dramatic improvements in technology and the
reduced cost of components.  In other areas however there has been little
applicable progress; machine vision, artificial intelligence and dynamic
feedback control systems (responding to changing conditions, etc.), all of
these areas have improved since the 1980's but not enough to make building a
domestic robot practical, and even with an unlimited budget, such a machine
would have limitations that would disappoint.  
  
What has improved dramatically since the 80's though is communication
networks, and in many ways these networks have made it possible for people to
work together around the world, regardless of physical location and to some
degree, without the encumbrances of social-political borders or economic
limitations.  These networks allow work to be distributed globally, from
programmers to telemarketers to drive-through attendants.  
  
However for the most part taking advantage of the ability to work globally has
been limited to work that can be transmitted electronically (there are a few
exceptions).  Wouldn't it be cool if people who work in the physical world
could enjoy the same power and convenience that telecommuters have enjoyed for
decades?  
  
This is where Rosie comes in.  Initially replacing the role of the household
cleaning service, Rosie is a telepresence robot that combines the mechanical
and electrical advances in robotics with the availability of high-speed
communications networks to provide the safety and convenience of a domestic
robot with the control and intellect currently only available via a human
operator.  
  
By abstracting physical presence from the housekeeping work, Rosie provides
increased value to the housekeeper in the form of reduced cost, scalability,
increased safety and tangental educational benefits as well.  
  
For the housekeeping customer, Rosie increases convenience, reduces cost-per-
utility and allows for greater control and management of the service with
lower personal overhead.  
  
Additionally, Rosie provides housekeeping professionals with training and
experience beyond the work at hand, creating a growing number of experienced
professional operators who's skills could be transitioned to maintaining and
operating robots for more diverse applications.  
  
The nature of housekeeping work strikes a nice balance between providing a
challenging engineering task for the design of the robot while at the same
time remaining simple and focused enough that designing and implementing such
a machine is within the means of current technology and within reach of a
small team with a relatively short development timeline (1-2 years).  
  
The up-front and operational cost of such a robot is such that it could employ
a mobile-phone-style pricing structure, baking the unit price into a fixed
contract resulting in a monthly cost that is equivalent to a traditional
housekeeping service but with the potential to provide better service.  
  
A side-effect of this pricing structure is that off-contract robots become the
property of the housekeeping customer, allowing them to be sold off-contract
and potentially applied to additional uses, which leads to "phase two" of this
project.  
  
By underwriting the development of a basic but functional telepresence
chassis, the Rosie product funds the development of more sophisticated
machines that can then be dispatched for more challenging tasks.  Evolution of
the design is part of this proposal, and within a year or two of rolling out
the housekeeping models, new models designed for other jobs will become
available.  It's not hard to imagine other areas of work that would benefit
from decoupling the work from a human operator, allowing for more flexible
scheduling (since workers could be available from other parts of the world),
reductions in safety-related costs (specialized machines can accommodate work
that is unsafe or uncomfortable for on-location humans).  
  
It's also worth pointing out that manual-labor is not the only application for
such robots, that any profession which  currently requires (or benefits from)
physical presence could be improved by specialized telepresence robots or by
the availability of general-purpose robots that could be dispatched around the
world quickly and conveniently (it's not hard to imagine offices or factories
maintaining a collection of such machines the way that they currently maintain
other office equipment, allowing instant on-site visits from employees from
other locations).  
  
Rosie (and descendants) can operate anywhere that can be reached by the
control network, and without the life-support requirements of living workers,
this includes extra-terrestrial operations.  
  
In the years since I first proposed Rosie, interest in telepresence has been
on the rise.  Several companies have introduced [ telepresence products
](http://www.doublerobotics.com) at increasingly accessible price-points,
evidence that a market is emerging for these machines.  However, the machines
currently available seem to be an evolution of telecommunication devices and
lack the ability to interact with their environment beyond sight and sound.
For these reasons I believe that Rosie is now a viable product and that
investment in such an effort now will lead a generation of such robots, laying
the foundation for future machines with more automatic, autonomous
capabilities, and I would be happy to lend my experience to such a project.

---
title: Scatagories
date: 2012-12-05
tags:
  - evernote
---
I need to pair-down the categories, looks like the imports built a bunch of
them and really a fair amount of categories I have should really be tags.  
  
This is more a reminder for me than an update for you, but it's also to let
you know I'm aware that it's ridiculous and I'm working on it.

---
title: Sneak Peek!
date: 2008-12-11
tags:
  - evernote
---
Just a quick note;  
  
Myself and Matt will be appearing on Wisconsin Public Television's "Directors'
Cut" tonight at 9:30PM Central. The show focuses primarily on our last
documentary "Breakdown" but near the end it also includes a sneak peak of the
Slimey Crud Run film as well (You can also view the entire episode online if
you can't catch it on TV).  
  
[ http://www.wpt.org/directorscut/111gullickson_cribben.cfm
](http://www.wpt.org/directorscut/111gullickson_cribben.cfm)  
  
Let us know what you think!

---
title: Some encouraging news...
date: 2009-06-29
tags:
  - evernote
---
I got an email over the weekend from the bike's previous owner. He has all the
paperwork done and in the mail to get the title squared away. I'm not sure
what this means time-wise, but at long last it looks like I'll have a clear
title for the bike, which means I need to get my "rear in gear" and get these
carb issues under control.

---
title: Some Progress
date: 2008-05-12
tags:
  - evernote
---
I finally got some time to work on the bike instead of the movie!  
  
I've had the pipes in the basement with the intention to cut off the rusted-
out mufflers for months. Last weekend I had a couple of hours to kill and I
decided to head downstairs and take a crack at them.  
  
[
![](http://lh4.ggpht.com/jason.gullickson/SCWIX1oj8iI/AAAAAAAABO0/gGi8QwZHbYI/s400/IMG_0195.JPG)
](http://picasaweb.google.com/jason.gullickson/Pipes/photo?authkey=sRbcUyakg-w#5198711287984681506)  
  
Probably the most distinctive feature of the CL350 (as opposed to the CB350)
is the mufflers. Unfortunately they are also one of the most troublesome. Due
to their shape and position, they collect water on the inside which is never
good for steel, and most any un-restored example you find in the wild has
major to complete rust in this area. To make things worse, the mufflers and
the headers are one-piece, so there's no "bolt-on" option for replacing them.  
  
Since this was one of my favorite parts of the bike I spent a lot of time
trying to replace/repair them while retaining the stock look, thinking I could
find some on Ebay (I've had luck with so many other parts). As it turns out,
since they all had this problem non-rusted out examples fetch a nice sum and
more than I'm willing to invest (this isn't a restoration project, I had to
keep telling myself that). Finally a few months back I decided to damn the
torpedoes and just slice off the mufflers and experiment. I figure the worst
thing that can happen is I'll need to pick up a replacement set, which is what
I was looking at in the first place.  
  
So I set out with Dremel in hand and attacked the pipes. This makes for great
movie footage as sparks fly and smoke rises, but it doesn't make for the
cleanest cut possible. After seeing the results of this first attempt, I
decided to give the hacksaw a try, which worked so well that I went back over
the first pipe with the saw just to get a clean edge.  
  
[
![](http://lh3.ggpht.com/jason.gullickson/SCWIploj8mI/AAAAAAAABPU/WMCLwVXIb-0/s400/IMG_0199.JPG)
](http://picasaweb.google.com/jason.gullickson/Pipes/photo?authkey=sRbcUyakg-w#5198711592927359586)  
  
I had a little more time left and the pipes were so rusty that I decided to
try taking a wire brush (attached to a power drill) to them and see if I could
clean them up a bit. 30 minutes later they looked great (compared to before at
least) and other than a few spots where the chrome had been removed (from a
fall or something) they looked almost new. Well not new, but I think with some
Brillo work they will look great. While I was at it I gave the clamps the wire
wheel treatment as well and there was a huge difference (it didn't come out so
much in the photo but trust me, it's very nice).  
  
I spent a lot more time on this than I think I had to, but there was something
so satisfying about making these parts look so nice. It makes me re-evaluate
my approach to the bike, and now that I have a few months more to work on it's
I'm considering taking a more thorough approach to cleaning the rest of the
machine up.  
  
...Then again it took me three months to get around to this one job so maybe I
should just keep my ambitions low until the engine runs...

---
title: Some Updates
date: 2000-10-13
tags:
  - evernote
---
There is a new section to the site called Projects (avaliable also from the
menu above) that has some info on a few things I've been working on lately.
Nothing ready for prime-time but maybe interesting nonetheless.  
  
As you may know we moved the 'zine site www.fish-zine.com to a new hosting
location and so far everything is going fairly well (Except that creepy
feeling that A's telepresence is in my loft while I'm sleeping...ok so I don't
sleep but it's still creepy). I have a new server on the way for the 'zine and
hopefully it will find its way here by the end of the week (Probably just in
time for me to leave town for the weekend). I will dedicate an entire article
to the building of this box as it should provide entertaining to anyone who
laughs at futility.  
  
In addition to the Projects section mentioned before I'm also planning on
adding a few more sections to the site; primarily I want to get some pix of my
skooter up here and some info on the bike (and what I plan to do to it this
winter), and I also want to setup a online DVD database so I can keep track of
what I have, what I've borrowed, and what I need to pick up (read: gift list).
None of which is real priority, so I wouldn't get too excited about seeing it
anytime soon.  
  
That's it for this episode, I missed the debates tonight but I guess it
doesn't matter since only 2/3rds of the candidates were represented anyway,
but I'll try and post something more interesting by next week.  
  
Over and out

---
title: So, what do you think about Wave?
date: 2009-05-29
tags:
  - evernote
---
  
I'll share my thoughts when I'm at a real keyboard, but in the meantime I'd
like to see what the rest of you have to say.  

---
title: Step 1 - Decide where to start
date: 2007-05-22
tags:
  - evernote
---
As I said before, one of the hardest parts of a project like this is deciding
where to start. After spending the last two days going back and forth between
two extremes (a complete frame-up restoration or a quick-and-dirty reassembly)
I've decided to go with the latter.  
  
[ ![](http://lh6.google.com/image/jason.gullickson/Rk9qJc6JBMI/AAAAAAAAASY
/iFxOOFQo-GQ/s288/IMG_2008.JPG)
](http://picasaweb.google.com/jason.gullickson/CL35051907/photo#5066384816426517698)  
The biggest reason for this is that my primary goal (to get the bike running
in time for the fall Crud Run) is unlikely to happen if I completely overhaul
the bike at this point. It's definitely something I'd like to do (and maybe
I'll end up doing it anyway) but for now I'm just going to focus on getting
the bike running and ridable and troubleshoot specific problems as they arise.  
  
So, when I picked up the bike it came with a stock-pot full of parts in
addition to the exhaust, fuel tank, seat and sissy bar separate from the rest
of the motorcycle. This pot was filled with various fasteners, clamps and
washers as well as the carburetors.  
  
[
![](http://lh3.google.com/image/jason.gullickson/Rk9qDs6JBLI/AAAAAAAAASQ/4u28Vbb_ozc/s288/IMG_2007.JPG)
](http://picasaweb.google.com/jason.gullickson/CL35051907/photo#5066384717642269874)  
I think I'm going to begin the re-assembly with the exhaust if only because
it's the area that's going to need the least attention. The fuel system needs
to be re-attached as well but since the carbs are already detached I'm going
to pull the bowls off just to make sure everything is clean before I bolt them
back on. I'll also be replacing all the fuel plumbing as well as treating the
tank (I haven't looked yet but I assume it's rusty on the inside), so hanging
the pipes will probably be the easiest thing to do first.

---
title: Step 2 - Hanging the pipes
date: 2007-05-24
tags:
  - evernote
---
[
![](http://lh4.google.com/image/jason.gullickson/RlTuSs6JBTI/AAAAAAAAATY/Jv4cQYBaNck/s288/IMG_2013.JPG)
](http://picasaweb.google.com/jason.gullickson/HangingThePipes/photo#5067937485758727474)
Tonight the girls and I decided to get started.  
  
Let me just start out by saying that this bike has the most unusual exhaust
plumbing I've ever seen. We spent at least 15 minutes just trying to decide
which pipe went where and the assembly order of the exhaust clamps (I'm still
not sure it's right).  
  
The mufflers are quite rusted out (you can see the red remains of the lower
muffler in a pile near the rear wheel) so my original plan was to remove them
for now and replace them with baffles. However now that I've actually spent
some time with them I find out that one is clamped on, the other welded. So, I
won't be making any changes here until I get out the angle grinder...  
  
[
![](http://lh3.google.com/image/jason.gullickson/RlTu4c6JBZI/AAAAAAAAAUk/7EW4DgoEci0/s400/IMG_2019.JPG)
](http://picasaweb.google.com/jason.gullickson/HangingThePipes/photo#5067938134298789266)
With a little help we were able to figure it out though and I even had enough
time afterward to do an oil change (maybe for the first time in the last 15
years?). With some fresh lube in the case I felt more comfortable turning over
the engine and I was able to do so without any unusual noises.  
  
I also picked up a few feet of fuel line today. The next task will be to re-
attach the carburetors (I inspected them today and they look surprisingly
good) and hook them up to an external fuel supply (I have other plans for the
actual gas tank, but I'll leave that for another post) so we can move closer
to that first attempt at firing up the beast.  
  
...now I need to go googling to make sure I clamped the headers on
correctly...

---
title: Sux0rz
date: 2016-08-06
tags:
  - evernote
---
_Note: I'm going to try something different with this post and update it as
the project progresses.  We'll see how that goes...)_  

  

Sux0rz  is a robotics collaboration between Jamie and myself.  The name (which
may change) was inspired by the fact that it started it's life as a vacuum
cleaner.  

  

For my part of the project the starting place is making it move.  It came on
wheels, which is a good start.  The rear wheels look like old lawnmower wheels
and are mounted on a solid axle, here's the basic measurements:  

  

  * Axle 9.53mm 
  * Wheel ~7in 
  * 6 spokes   

  

  

The front wheels are pivoting casters, so a tank-style two-motor setup should
prove adequate for drive and steering.  

  

After mulling over the drivetrain options I settled on attempting to design  &
print it myself.  This lets me recycle a pair of motors left over from the
Power Wheels autonomous truck/hovercraft project instead of having to buy off-
the-shelf gearmotors, which are kind of expensive and take awhile to get here.  

  

This might not pan out since I'm still learning about designing gears, ratios,
power requirements, etc. and also working with undocumented motors, but this
is more fun than just spending money.  

  

Turns out the gear's center hole should be 10mm ID (post-print) to fit over
the axle.  

  

  

  

  

  

The custom gear has been designed to work with the pinion that came with the
motors.  I'd prefer twice the gear ratio (currently 15:1) but that would
require more gears, so we'll try this for now.

  

  

  

  

  

  

Turned out that the motor nestles in almost perfectly between the deck and one
of the existing axle clamps (?).  Gear engagement seems satisfactory, so all
that's necessary is something to hold the motor in place.  

  

  

  

  

  

  

After considering a few options, I remembered I had a bar of aluminum stock
that was close to the width of the motor.  With a few well-placed holes, the
motor could be mounted through a piece of this bar and the whole things could
be bolted to the side of the deck.  

  

  

  

  

  

For whatever reason I seem get spacing like this wrong often (perhaps a
"stack-up" problem.  Regardless it seems to hold, so I'm planning to use this
as a template to cut a pair of more accurate mounts later.  

  

  

  

  

I wanted to have some adjustment in the mount, so that the engagement could be
experimented with, but for the first pass I decided against it.  I wasn't sure
how everything was going to come together and I didn't want to spend a lot of
time working the mount if it was going to have some fatal design flaw.

  

After fitting things together it looks like that's not the case, so I might
put some adjustment ability into the next version, but for now there's enough
slack in the way the assembly is mounted to the deck that there some
adjustment available.  

  

  

  

Once things were together I did a test-run using a 9v battery and everything
seemed to work, but I wanted to see how it looked driving the actual wheel.
My long-term plan is to modify the gear model so that it can engage a standard
lawn mower wheel directly, but in the meantime zip ties suffice.  

  

  

  

  

Coincidentally, this setup creates a sort of ghetto "cush-drive".  This might
be advantageous, so I'm considering running with it and printing another gear
like the first.  

  

  

After adding the gear, and spacing it enough to avoid interfering with the
motor mount screws, a longer axle is necessary.  A piece of  3/8" x 22" smooth
rod should do it.

  

  

  

  

New axel being sized, second motor mount and gear can be seen on the right.

  

  

  

Both motors mounted, preparing for drive test.

  

  

  

Final step for the new axel, drilling holes and placing pins to keep the
wheels on.

  

  

**Motors**  

For now we're recycling the motors from the Power Wheels truck.  They seem to
be some RS540 variant, 16k RPM and 12vdc I guess.

  

Current (har har har) thought on driving  the motors  is to use a simple
MOSFET + Arduino PWM setup for variable speed drive in one direction.  This is
largely based on the fact that I have all the parts for this on-hand, so it's
cheaper and faster than buying/building something more sophisticated.
Additionally, I'm just guessing at the max power requirements for these motors
(40 amps?) so I'd rather burn-up a couple dollars worth of MOSFETS than an
expensive (both in dollars and time) controller while I figure things out.  

  

First pass at motor driver (controller is kind of an overstatement)

  

  

  

Setup a test rig on the bench and it worked end-to-end with one channel, so
hopefully the second one works just as well.

  

  

  

Next step is to hook the whole mess up to the real drivetrain and see what
happens...

  

First live test with the actual drivetrain:

  

  

  

Found out that heatsinks are necessary on the MOSFETS, and there's suddenly
more friction on the wheel than before.

  

Todo:

Heat sinks  

File-down wheel to reduce friction  

Drill-out gear hub  

Mount electronics  

Find cotterpins  

Solder other diode  

  

  

Heatsinks, hmm...  

  

  

  

  

  

  

The heatsinks appeared to help, but unfortunately I noticed a problem with the
solder joints on one of the MOSFETs.  I thought I could repair it, but the
traces on the board were actually lifted and broken, so it looks like this
board it toast.  

  

I might be able to make another one but I'm going to need some parts.  Since
I'm ordering parts anyway, I decided to pick up a couple of cheap RC motor
controllers as well.  I really wanted to build my own drivers for the robot
because I wanted to work with parts-on-hand and learn about motor control in
the process, but the clock is ticking for [ Milwaukee Makerfaire
](https://makerfairemilwaukee.com/) and using off-the-shelf controllers is one
less variable at this point.  

  

I guess I still did build my own driver, even if only half of it worked, and
I've learned enough that I might take a swing at making a more advanced
driver/controller in the future for another project.  

  

Now I need to find out if I can use the existing pyfirmata - > standardfirmata
setup to drive servos instead of just PWM.  

  

Motor control V2

  

  

  

  

  

  

  

  

  

  

The new motor controllers basically work, and with that it was possible to
assemble the entire robot (temporarily in some areas) and test the system end-
to-end.  Functionally everything worked, but there were some control problems.
Here's what we know now:

  

  * The drivetrain is capable of moving the entire system at a high enough speed 
  * The control system works as expected 
  * All of the parts can be assembled in a form that can move about 

  

Here's what we need to look into:

  

  1. The robot does not always respond immediately to commands 
  2. Braking and reverse 
  3. The robot can "fail deadly" in that the drive can become engaged and doesn't disengage when control signals are lost 
  4. There is significant lag (seconds) in the video feed 

  

I have a few things to try in order to address these issues:

  

  * Network disconnects may be to blame. Configure the robot to use an on-board wifi access point so distance to a stationary AP isn't a factor 
  * Add a iterator thread to the Python code to avoid overflowing the serial port 
  * Use a queue to send commands from Python to the arduino to throttle command rate 
  * Use a timeout on commands received at the arduino so motors stop if no additional commands are received within a specific window of time 
  * The motor controllers supposedly support braking and reverse, but they came with no documentation so I need to find that and configure them, as well as add support for reverse to the controller software 
  * Use a more sophisticated video streaming method and protocol 
  * Consider switching to WebRTC to kill latency, etc. (https://janus.conf.meetecho.com/docs/index.html)   

  

**Software**  

At the moment it looks like a combination of Python modules will be providing
remote control via WiFi (flask - > pyfirmata).  A REST API will provide
endpoints for motor control input that will come from a simple client-side
javascript remote control (that can run on any device).  

  

Screenshot of current (working!) controller:

  

  

  

  

**Next Steps  
**

  

Get the Raspberry Pi mounted securely in the brainpan

Configure the motor controllers  & add "arm" control to UI

Configure Raspberry Pi for mobile hotspot WiFi network  

Update UI to include reverse, and configure for forward, braking and reverse
signals  

  

**Notes**

  * Could the gear include a 608 bearing (or two)?  Would that fit on these axles? 
  * Could ssh tunnels be used to expose the control interface (and associated API) to the public Internet via a gateway server? 
  * Consider picamera to handle video stream via python instead of mjpg_streamer 

  

**Reference**  

  * [ http://bildr.org/2012/03/rfp30n06le-arduino/ ](http://bildr.org/2012/03/rfp30n06le-arduino/)   

  * [ http://bristolwatch.com/arduino/arduino_pwm_hb.htm ](http://bristolwatch.com/arduino/arduino_pwm_hb.htm)   

  * [ http://bristolwatch.com/ele/h_bridge.htm ](http://bristolwatch.com/ele/h_bridge.htm)   

  * [ http://flask.pocoo.org/ ](http://flask.pocoo.org/)   

  * [ https://github.com/tino/pyFirmata ](https://github.com/tino/pyFirmata)   

  * [ https://raspberrypi-aa.github.io/session3/firmata.html ](https://raspberrypi-aa.github.io/session3/firmata.html)   

  * [ http://code.tutsplus.com/tutorials/an-introduction-to-pythons-flask-framework--net-28822 ](http://code.tutsplus.com/tutorials/an-introduction-to-pythons-flask-framework--net-28822)   

  * [ http://picamera.readthedocs.io/en/release-1.12/recipes1.html ](http://picamera.readthedocs.io/en/release-1.12/recipes1.html)   

  * [ http://www.html5rocks.com/en/mobile/fullscreen/ ](http://www.html5rocks.com/en/mobile/fullscreen/)   

  * [ http://www.hongkiat.com/blog/responsive-for-mobile-screens/ ](http://www.hongkiat.com/blog/responsive-for-mobile-screens/)   

  * [ https://developer.apple.com/library/safari/documentation/AppleApplications/Reference/SafariHTMLRef/Articles/MetaTags.html#//apple_ref/doc/uid/TP40008193-SW1 ](https://developer.apple.com/library/safari/documentation/AppleApplications/Reference/SafariHTMLRef/Articles/MetaTags.html#//apple_ref/doc/uid/TP40008193-SW1)   

  * http://scruss.com/blog/2012/10/28/servo-control-from-pyfirmata-arduino/    

  * [ http://www.rcworld.com.au/media/downloadspdf/TY1%20%20-%20TY1%20Brushed%20%20ESC%20instructions%20May%2008%20EE.pdf ](http://www.rcworld.com.au/media/downloadspdf/TY1%20%20-%20TY1%20Brushed%20%20ESC%20instructions%20May%2008%20EE.pdf)   

  * [ http://techvalleyprojects.blogspot.com/2012/06/arduino-control-escmotor-tutorial.html ](http://techvalleyprojects.blogspot.com/2012/06/arduino-control-escmotor-tutorial.html)   

  * [ https://www.linkedin.com/pulse/faq-rc-esc-calibration-programming-using-arduino-boura-cissp-mphys ](https://www.linkedin.com/pulse/faq-rc-esc-calibration-programming-using-arduino-boura-cissp-mphys)   

  * [ https://janus.conf.meetecho.com/docs/index.html ](https://janus.conf.meetecho.com/docs/index.html)   

  * https://www.rs-online.com/designspark/building-a-raspberry-pi-2-webrtc-camera   

---
title: Sync & Backup your files without servers with BitTorrent Sync
date: 2013-07-23
tags:
  - evernote
---
[ BitTorrent Sync ](http://labs.bittorrent.com/experiments/sync.html) is
something I've been looking for for a long, long time.  
  
[ ![Once installed, BitTorrent Sync provides a simple interface to sharing
directories](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07
/Screen-Shot-2013-07-22-at-9.23.29-PM-300x213.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-22-at-9.23.29-PM.png) Once installed, BitTorrent Sync provides a
simple interface to sharing directories  
  
I use a lot of computers and often find myself needing a file that I have
stored at home when I'm at the office, or remoted into a server somewhere on
the Internet, or... you get the idea.  Over the years I've used dozens of
different things to access these files from FTP servers to rsync to Dropbox,
on and on and on.  
  
Dropbox was my favorite for awhile because it thought ahead for me and sync'd
files before I needed them, but then I experienced one of the biggest problems
with trusting your data on someone else's server; [ they got hacked
](http://news.cnet.com/8301-31921_3-20072755-281/dropbox-confirms-security-
glitch-no-password-required/) .  
  
[ ![With no server in the middle, files synchronize very
quickly](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07
/Screen-Shot-2013-07-22-at-9.35.01-PM-300x213.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-22-at-9.35.01-PM.png) With no server in the middle, files
synchronize very quickly  
  
Since then I've been looking for a way to get that level of simplicity without
having to rely on other people's servers, but they all leaned on a server at
some point, or required some sort of account, or transmitted data using non-
secure methods, etc.  Enter [ BitTorrent Sync
](http://labs.bittorrent.com/experiments/sync/technology.html) .  
  
I've used [ BitTorrent ](http://www.bittorrent.com) to transfer files on a
case-by-case basis, but this is awkward and requires forethought (you need to
publish the files before you forget them), and it also puts your files out in
the open which isn't always acceptable.  
  
BitTorrent Sync uses the same "swarm" mechanism for moving data round
efficiently, but in the form of a sync application that lets you share folders
on your computer and automatically sync their contents with other authorized
machines.  The transmitted data is encrypted, and since authorizing other sync
points requires exchanging an encryption key, security is preserved.  
  
[ ![BitTorrent Sync provides several configurable methods of allowing nodes to
find eachother without servers \(don't worry, the defaults work great out-of-
the-box\).](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07
/Screen-Shot-2013-07-22-at-9.32.01-PM-278x300.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/07/Screen-
Shot-2013-07-22-at-9.32.01-PM.png) BitTorrent Sync provides several
configurable methods of allowing nodes to find eachother without servers
(don't worry, the defaults work great out-of-the-box).  
  
Other services provide encryption but what if their servers get hacked?  This
is where BitTorrent Sync really shines because there are no servers to hack.
BitTorrent Sync uses the same serverless mechanisms of the regular BitTorrent
protocol to connect computers and sync data across the Internet.  This is done
through various discovery mechanisms which are configurable depending on the
level of privacy and anonymity you require.  
  
My only beef with BitTorrent Sync is that it's currently closed-source, and
there's no public API either.  It's still in beta so this may change, but I'm
not going to count on it.  I have some very interesting application ideas for
BitTorrent Sync, but I'm holding back a bit because I'm cautious to build on
top of closed software, but hey maybe we'll get lucky and they will open it
up, or perhaps an open alternative will come along.  
  
In the meantime it's an excellent way to synchronize your data in an very
secure and private way which also happens to be free (as in beer), at least
for the time being.  
  
BitTorrent Sync has clients for Mac, Linux, Android and Windows.  The Linux
version, while lacking an X-Windows interface has a cool http-based front-end
that makes using it on "headless" servers and NAS boxes a piece of cake.

---
title: Test the market for your tech with Tindie Fundraisers
date: 2012-12-18
tags:
  - evernote
---
You're probably already familiar with [ Tindie ](https://tindie.com/) (a sort
of [ Etsy ](http://www.etsy.com) for electronics), well they just announced a
feature today that might interest you hardware hackers: [ Tindie Fundraisers
](https://tindie.com/shops/fundraisers/) . [  
](https://tindie.com/shops/fundraisers/)  
  
[
![](https://s3.amazonaws.com/tindie_prod/cache/ee/d5/eed5b4e3e3ea378cc151e027d34cc780.jpg)
](https://tindie.com/shops/rajbex/pre-order-trh-meter-kit-3/) TrH Meter -
Measure hotness & wetness with a cool display for only $25, who could turn
that down?  
  
In a nutshell, it's a way for someone with an idea or a prototype to feel out
the market before they order a bunch of parts. It's kinda like a [ Kickstarter
](http://kickstarter.com) , except tied to a specific deliverable item (as
opposed to a range of "rewards") and it's unambiguous about the fact that
you're plunking money down to pre-order on something not just an idea or the
intent to deliver an item (if this sounds familiar I wrote about [ a related
topic ](http://www.gullicksonlaboratories.com/this-is-how-products-of-the-
future-will-be-born/ "This is how products of the future will be born") a few
days ago).  
  
[
![](https://s3.amazonaws.com/tindie_prod/cache/0d/39/0d3944ac63d385f309a0e5dd6c669a92.jpg)
](https://tindie.com/shops/ElectricLaboratory/chainable-intelligent-stepper-
driver-l6470/) Let me tell you, you can never have too many chainable,
intelligent stepper motors...  
  
It's all pretty geeky stuff right now, but it's not hard to imagine more
consumer-oriented devices filling this space soon. I for one have several
ideas I've considered taking to market but I wasn't sure if the cost to build
would match up with what the market would bear. Now I have a chance to find
out!

---
title: That was quick
date: 2009-05-19
tags:
  - evernote
---
  
Fixed the bug Tone reported over the weekend in the new iPhone app. Now to see
what comes back next...excellent.  

---
title: Thawing Out
date: 2008-03-17
tags:
  - evernote
---
So there is a lot of exciting developments around the film these days;
unfortunately none of them have to do with my motorcycle.  
  
I've had the carb rebuild kits for at least a month and the pipes in the
basement for weeks, but with the schedule lately I've had zero time to spend
down there getting things together.  
  
With any luck things will wind down here in the next week or so and I'll get a
chance to do some work on the bike. The weather around here has been
improving, making it a lot easier to get work done out in the garage. Once I
get the carbs and pipes done I'll be digging the rest out and moving
operations back outside for the warmer months.  
  
In the meantime I'll be focusing on my "Producer" role of the project and less
on the "Mechanic" one; but hopefully that will change soon.

---
title: The App Cobbler
date: 2012-11-18
tags:
  - evernote
---
So many Apps in the App Store that seem to burst onto the scene but over time
wither and die.  
  
Sometimes because they went bust, sometimes because their creators ran out of
steam and certainly many times for many other reasons.  
  
What if there were an App Cobbler, a skilled and clever software artisan who
would take these apps and resole them, like a traditional cobbler would a
quality shoe, returning or perhaps surpassing their original value?  
  
Of course there are some shoes that are cheap and not worth fixing, and some
apps like this as well; but for those who bear quality, but perhaps suffer
from wear or neglect, perhaps an app cobbler should exist?

---
title: The Arduino
date: 2012-06-05
tags:
  - evernote
---
What can I say that hasn't already been said about the [ Arduino
](http://refer.ly/aanO) ?

---
title: The Bookstore Idea
date: 2012-09-08
tags:
  - evernote
---
Imagine a traditional bookstore, however when checking out, the shopper
receives an ebook instead of the physical book, and the physical book is
returned to the shelves.  
  
Since the books never leave only one copy of each book needs to be stocked,
allowing more variety on the shelves. This combines the pleasure of the
bookstore experience with the selection of an online store and the convinience
(and ecological advantages) of electronic books.

---
title: The Evil Thing
date: 2013-02-20
tags:
  - evernote
---
So the other night my daughter had the audacity to attempt to create a [
YouTube ](http://youtube.com) channel.  
  
She went to [ youtube.com ](http://youtube.com) , signed in with her [ Google
](http://google.com) email & password and then was prompted to enter birth
date.  After supplying this and clicking "next", she received a big red
message that said she was too young to have a Google+ account, and that her
account was now locked, which means no access to any Google services (gmail,
docs, etc.).  
  
The message said, If she had specified her birthdate incorrectly she could use
one of the three links provided which could be used to unlock the account by
providing a credit card, a federal ID or something involving a fax machine.  
  
The message went on to say that if this was not a mistake, that the account
and all data associated with it would be deleted in something like 19 days.  
  
I understand that maybe there are now laws prohibiting children under a
certain age from having accounts on social networks or some such thing,
however she has been using these services for _years_ and has hundreds of
documents stored in Google Docs.  Perhaps her age was requested back when her
original gmail account was setup, but then why was it requested again?
Regardless, at this moment years worth of her work was now locked away and
queued up for deletion, and the only way to unlock it is essentially to
falsify identity information.  
  
After giving this some thought, I came to the conclusion that the only thing
to do was to attempt to recover the account, even if that meant lying to
Google about her age.  If nothing else, this appeared to be the only way to
have a chance to extract her work from their systems before it was destroyed.
So I went ahead and took the credit-card route, specified my birthdate and
provided the credit card information (all using my name, obviously not he same
name used for the Google account).  
  
Surprisingly this just worked.  
  
We could talk about the questionable nature of a system that would be so
concerned about your age as to threaten to delete your data, but is willing to
release its grip as soon as you can provide _someone_ _else's credit card_ ,
but the more significant thing to me here has to do with Googles ability to
legally destroy the documents and information you have trusted them with.  
  
I say legally because I assume that they have arranged their terms of service
in such a way that it grants them the legal right to do so, after all if not
then they would be breaking the law by deleting the data in your account.  
  
This being the case, it made me think long and hard about who actually owns
the stuff you store in Googles systems.  While the legal definition may vary,
I would say that from a practical perspective, if someone can legally destroy
something you own, _then you don't really own it_ , and you should put some
thought into how many hours you've spent creating the content stored in your
Google Docs, or on your Google Drive, or in your Gmail archive, or your
YouTube videos, etc., etc.  
  
This is why I can no longer in good conscious use Google systems to create,
store or distribute anything of value, knowing that at any moment it could
simply all go away, that my access could be revoked and that the work itself
could be wiped from existence.  
  
I know that most of you reading this will disregard it, or choose to ignore
the warning (I probably would have too), but I I felt compelled to share it if
only to explain why I'll no longer be investing time in these services.

---
title: The Fishbowl
date: 2004-08-21
tags:
  - evernote
---
My best memories of Valcom are when I worked in the fishbowl.  
  
The fishbowl was a corner office in the 3113 building (at the time the
education center). It was a place for the I2 team to get away from the
disruption of the main office on Pinehurst.  
  
I started working with the team somewhat covertly while I was a "regular tech"
at on the Rayovac contract. After sucessfully deploying MRTG on WindowsNT,
something happened and I was magically transported from the bowels of "the
third ranked battery" company to the shiny digs of the Fishbowl.  
  
Well, it wasn't exactly shiny. The room was filled with six-foot folding
tables (we reffered to them as "lunch room tables") and crummy office chairs
that were crept from unsuspecting classrooms. The rest of the room contained a
dizzying array of discarded PC's, ethernet cable and various bits of
networking hardware.  
  
Reguardless, this was heaven compared to where I had come from. My original
charter with the company had me driving to Land's End in Dodgeville (about a
hour commute, which at the time seemed long) every day for about six months.
After that I filled various roles "in the field", untill landing my more
permanent seat at Rayovac. I wrenched on that account for what seems like ten
years, but it was more like two. After living in the white-noise-corporate-
hell that was Rayovac, I was ready to move on to something that challenged
more than my patience and sanity.  
  
The fishbowl just felt right. The only people with access were the most elite
of elite engineers, and our rock-star boss Bob (the first in a long series of
managers I would have named Bob). At this point in my life I had little to do
with myself beyond my work, so I spent day and night in the 'bowl, tweaking on
ASP, Linux, SQL, whatever we could find a way to sell and was interesting
enough to loose sleep over.  
  
We made a regular habit out of bailing out the "other" engineers in the
company. I would probably be retired in the Bahamas now if I had actually
gotten paid for the hours that we spent on-call, or in the office, or out in
the field recovering some abortion of a server upgrade that one of our
employees managed to get themselves into.  
  
I remember one day looking out over the aurboritum from the second floor of
3113 and thinking about the future; I did this alot then, more than I do these
days. I thought about ways to take our little company and turn it into
something big, something that would leave a beautiful stain on the technology
world.  
  
I had no idea.

---
title: The future is written in Javascript
date: 2013-05-04
tags:
  - evernote
---
There are a lot of programming languages, but there is only one that runs in
your web browser, and that's [ Javascript
](http://en.wikipedia.org/wiki/Javascript) . [ Ruby
](http://en.wikipedia.org/wiki/Ruby_\(programming_language\)) , [ Java
](http://en.wikipedia.org/wiki/Java_\(programming_language\)) , [ C#
](http://en.wikipedia.org/wiki/C_Sharp_\(programming_language\)) , [ Python
](http://en.wikipedia.org/wiki/Python_\(programming_language\)) , [ PHP
](http://en.wikipedia.org/wiki/Php) and [ dozens more
](http://en.wikipedia.org/wiki/Web_development#Server_side_coding) are used to
write software for the web but in the end they all produce [ HTML
](http://en.wikipedia.org/wiki/Html) and Javascript, and HTML is not a
programming language. That means that there's only one programming language
for the web, and that's **Javascript** .  
  
If you're writing software for the web, you're programming in Javascript, it's
just a matter of how many layers of "comfort zone" there is between you and
your application.  
  
**But what about the servers?!?**  
Fine, great, the future of _user interface_ might be written in Javscript, but
what about the server code?  
  
If you're a web developer you have of course heard of [ Node.js
](http://en.wikipedia.org/wiki/Node.js) (if you haven't, you can get the scoop
[ here ](http://nodejs.org) ). The short of it is that you can now write your
server code in Javascript as well as your client-side code, and Node.js has
been around long enough to have proven that it has the necissary ingredients (
[ performance ](http://zgadzaj.com/benchmarking-nodejs-basic-performance-
tests-against-apache-php) , [ scalability
](http://lanyrd.com/2012/dibi/sqtyb/) and [ security
](http://wegnerdesign.com/blog/why-node-js-security/) ) to function in a
production environment.  
  
That being the case, why would you spread your focus accross multiple
languages to write a web application? Some will argue that Javascript isn't
appropriate for writing server-side code, that there is something in the
nature of the language or the runtime that just isn't right. I argue that this
is patently incorrect, and the only way to arrive at such a conclusion is to
have an inaccurate or incomplete understanding of the Javascript language.  
  
Javascript has been designed from the beginning to work in the event-driven
environment of the browser and excels at asyncronous processing. There was a
time where this was not a useful feature for server-side web software, but
since the proliferation of [ AJAX
](http://en.wikipedia.org/wiki/Ajax_\(programming\)) techniques the old "page
at a time" method of server-side processing has gone by the wayside.  
  
Beyond the technical or implementation-level featurs that make Javascript
suitable for these tasks is the philosophy of the language and its constructs
that lead the developer into an event-driven way of thinking. Once embraced,
this mode of thought changes the way a developer designs applications and
opens them up to new ways of providing better, more responsive and more
engaging experiences to users.  
  
**But the future isn't all on the web right? What about mobile?**  
The future of mobile is Javascript as well.  
  
This isn't a radical idea, in fact Apple knew this when the first iPhone was
introduced back in 2007. Until 2008, the only way to write software for the
iPhone (unless you were an internal Apple developer) was to write it in
Javascript in the form of " [ Web Apps ](http://www.apple.com/webapps/) ".
Early on, there was no indication that Apple would ever open up the iPhone to
allow third-party developers to write "native" applications for the iPhone,
and the fact that Apple provided Javascript API's to access device features in
these early versions of iOS is evidence that they saw the potential of
delivering fully-functional Javascript-based applications to mobile devices
(why did Apple change directions? That's [ another story
](http://9to5mac.com/2011/10/21/jobs-original-vision-for-the-iphone-no-third-
party-native-apps/) ).  
  
Since then, the number of device features that have been made accessible via
Javascript API's has grown, and technologies like [ Websockets
](http://en.wikipedia.org/wiki/Websockets) and [ WebRTC
](http://en.wikipedia.org/wiki/WebRTC) make writing Web Apps that rival native
applications even easier, and that's just the tip of the iceberg. With the
introduction of [ FirefoxOS ](https://developer.mozilla.org/en-
US/docs/Mozilla/Firefox_OS) (also known as "Boot to Gecko" or B2G), there is
now a platform where Javascript and native code applications are on equal
footing (a predecessor with a similar philosophy is [ WebOS
](http://en.wikipedia.org/wiki/Webos) ), and by equal I mean feature parity,
however Javascript has several distinct advantages.  
  
**Developers, Developers, Developers**  
There is no other language with more developers using it than Javascript
(remember the first few paragraphs where we talked about how all web
developers are writing Javascript?). This means that, all else being equal,
the potential of Javascript applications is orders of magnitude higher than
"native" applications based on the sheer number of developers with decades of
experience behind them. What's more is that these same developers have been on
the cutting-edge of creating compelling user experiences (much of the original
use of Javscript was to provide realtime feedback and event-driven user
interface elements). These developers have been sidelined on mobile platforms
by restricted subsets of device features or second-class integration into the
user experience. With B2G, a whole new army of developers will be able to
build great applications for mobile, and as the iPhone has proven, great user
experiences drive the platform.  
  
**Developer productivity and other excuses**  
There is a case to be made that other languages, frameworks, etc. are more
than up-to-the-task of creating compelling experiences on the web (and
beyond), and that these tools and platforms lend themselves to increased
developer productivity. I could dispute these points as well but instead I'll
agree, that any language can be used to generate HTML and Javascript and
return it to a browser for processing, and that frameworks and toolkits and
the like can make creating applications more convinient for programmers
unfamiliar with raw Javascript and it proper use. I'll even go so far to say
that anything you can find on the web today could probably be created using
any of these toolsets, and if you have those tools (or those trained in those
tools) at your disposal then you can probably get more done faster by using
what you know.  
  
However, this essay is about the future, and if there's anything certain about
the future it is that you can't build it within the restrictions of models
from the past. If you're using a toolset it is a fact that creating something
that wasn't considered when the toolset was designed will be difficult if not
impossible, and in either case will doing so will compromise your vision.  
  
Javascript is the most complete programmatic interface avaliable to web-based
applications. Instead of mastering analogies and metaphores that insulate the
developer from the platform (and often mask its most exciting features), time
spent embracing this single language allows a developer to not only create
what is known, but to invent future applications and techniques that have not
yet been imagined by the authors and architects of today's convinience
frameworks.  
  
This, coupled with the ever expanding list of [ exciting new technology
](https://github.com/leapmotion/leapjs) that is programmable using javascript
is what I mean by "the future is written in Javascript".  
  
**Easier said that done (or, "how do we get there from here?")**  
Understanding and even accepting this philosophy is one thing, but how to
transition from an existing application, or from previous experience, to
Javascript-based development? The path will varry for every developer and each
application, but what I can do is share the path that I have personally
embraced.  
  
_Learn the code_  
Most web developers have used Javascript code directly, and many have even
written it, but few have taken the time to learn Javascript as a language unto
itself. Read a good book and learn Javascript as if from scratch. Take care to
learn the [ good parts ](http://amzn.to/108GJJg) , and learn how to avoid the
[ bad parts ](http://amzn.to/10xIwri) .  
  
Write Javascript programs in an environment where you won't get hung-up on
dealing with the browser and its DOM or third-party Javascript libraries
(Node.js is a great environment for this).  
  
_Draw a line in the sand_  
Javascript fits more naturally on the front-end than on the back end, and the
proliferation of REST and JSON-based API's makes implementing user interfaces
in Javascript much easier than it was in the past. Begin here by building
directly against these interfaces using client-side Javascript code, and
resist the urge to leverage frameworks and libraries whenever possible.  
  
Partition your existing applications by implementing a [ Hypermedia API
](http://amzn.to/13a4eWC) . Use this as a "firewall" to push existing code
behind a clean surface to attach a Javascript client to. Refactor the code
behind the API over time and eventually replace it with service calls built on
Node.js.  
  
_Learn a new platform_  
Start developing for FirefoxOS, or develop Web Apps for other platforms. iOS
provides decent support for making Web Apps behave like native apps (including
launcher icons and full-screen modes that hide browser chrome). Android
provides a rich set of Javascript-accessible device API's as well, but for the
full experience FirefoxOS is really the way to go (in fact all of the built-in
software is written this way).  
  
There are as many paths over the mountain as there are developers, but the
first step is to look at Javascript not as an inconvinience or a compromise,
but as an elegant solution to a complex problem. A language which is
encumbered with demons from its past, but which are only harmful to those who
know not how to avoid them.  
  
Any of the steps above can be the beginning of this understanding, and all of
them can lead to building the applications of the future, with Javascript.

---
title: The Goodlight Project and taking Quirky for a spin
date: 2013-07-29
tags:
  - evernote
---
Heard about [ Quirky ](http://www.quirky.com) while reading [ an interesting
post about the future of hardware startups
](http://www.trueventures.com/2013/07/27/the-hardware-revolution-is-upon-us-
and-why-it-matters/) from the investment perspective. So I dug out one of my
simpler ideas to take it for a spin, see what you think:  
  
**Hate how you look on camera?** What you need is some **Goodlight** : [
http://t.co/Xw3f6mjP08 ](http://t.co/Xw3f6mjP08)

---
title: The Library
date: 2001-01-29
tags:
  - evernote
---
So this week's topic is a new project we're working on we like to call the
library. The idea behind the library is to provide a place to keep track of
your digital media (currently dvd, cd, playstation and minidisc). My original
motivation for this was to make it easy for people who like to buy me this
stuff to know what I already have ; ), but it panned out into some more
interesting possibilities.  
  
I think the primary usefulness of the library right now is that it provides
you a way to see if someone you know has something you'd like to borrow, and
also allows the borrower to keep track of who has what. We are also working on
some more features to make this more useful, and possibly more automated.  
  
So in the meantime, hit www.jasongullickson.com/library/ and take a look at
what we're working on now, and if you have any ideas, feel free to drop me a
line at mr@jasongullickson.com.

---
title: The Natives are Restless
date: 2012-12-05
tags:
  - evernote
---
_note: this was originally going to be a response to my friend Dave on
Facebook but then he convinced me it should be a blog post (because that takes
a lot of coinvincing after all...)_  
  
The question essentially is, have you considered Mono for cross-platform
mobile development, especially when cost and maintainability are a
consideration?  
  
This discussion (native-vs-cross-platform) has come up a few times since I
started doing mobile development a few years ago and the answer is definately
a fourth-dimensional thing, so the best I can do is answer it in the context
of now, and note that I'm continually reviewing this reasoning myself every
few months or so.  
  
**Favor the User's Experience**  
  
As a developer in general (but more importantly as a mobile developer) my
choice of tools is always based on providing the best possible user
experience.  This is particularilly important for mobile devices because these
are treated much more intimately than a desktop computer or even a laptop and
so an application must adopt that directness and personalness as well.  This
tends to favor native app development, because it allows the developer to
leverage familiar interface paradigms in a more seamless way than any cross-
platform tool can.  It also means that the resulting application will be able
to take advantage of the latest platform features (cross-platform tools
usually require time to catch-up to new hardware and operating system
features, if they adopt them at all).  Of course the best performance is
obtained using native tools, and responsiveness is a key element to getting
that natural feeling and one-ness expected from mobile applicatons.  
  
**If it's all the same to the user, go HTML5**  
  
If you can provide a natural, personalized experience using an HTML5
interface, and if you get get at the device resources you need via HTML5, and
you want cross-platform portability, go with HTML5.  
  
(did I say HTML5 enough?)  
  
If you can live with the compromises of speed, hardware access and integration
that come with building your app as an HTML5 "web app" and you have a need for
cross-platform support then above all other options I recommend going HTML5.
Most other cross-platform tools have simular limitations and few (if any) of
them provide the benefit of providing recyclable experience (anything you do
in HTML5 can be leveraged in non-mobile web projects as well).  Furthermore,
most devices have some sort of method of treating HTML5 web apps almost as
well as native ones (allowing you to create an icon on the dashboard, etc.) so
while not quite as seamless as native, HTML5 web apps can integrate pretty
well with the platform.  
  
The last thing I'll point out in this section is Mozilla's [ FirefoxOS
](http://www.mozilla.org/en-US/firefoxos/) ; this is an entire platform based
on HTML5 web apps (all the way down to the phone dialer).  Personally I think
this has huge potential, since it leverages a huge existing HTML developer
base and provides these developers with first-class application treatment and
platform integration.  
  
**Conclusion**  
  
I'll wrap up with saying that I really recommend against trying to build a
single cross-platform application.  Users select a platform (be that a phone,
tablet, desktop computer, etc.) because they have an affinity for the
environment which that device provides, and good user experiences come from
software that embraces the context of that platform.  If you are compelled to
maximize your codebase, choose a service or API-based approach that allows you
to re-leverage your code on the back-end but provide a unique and appropriate
experience for the user on the front-end by embracing the native tools and
design patterns to present the information and processing that is shared via
the API across platforms.  
  
**Dude you didn't answer the question, what about Mono?**  
  
Honestly I haven't looked into using Mono on mobile yet, maybe that should be
a blog post...

---
title: The new app has been submitted
date: 2009-06-09
tags:
  - evernote
---
  
After Apple's announcement today that iPhone OS 3.0 will be available 06/17, I
decided to skip the 2.2 version and release the 3.0 features immediately.

So, tonight I re-enabled the 3.0 features, gathered the various media and bits
required for submission and sent the app up.

I imagine they are swamped, more than usual, so I don't expect to hear
anything anytime soon, but with any luck we'll be one of the first "build for
iPhone 3.0" apps in the store...

...wish me luck!  

---
title: The next step for my project bike - TTXGP
date: 2009-06-23
tags:
  - evernote
---
  
  

---
title: The Next Thing is Telepresense
date: 2016-10-26
tags:
  - evernote
---
I've been spending a lot of time lately thinking about what I want to work on
next.  There's a lot of possibilities, and some are certainly easier or more
practical than others.  I realized this morning that telepresense is what I
want to work on next.

  

There's a lot of challenges in this space, and it's more ambitious than doing
something that I'm more familiar with (software development, etc.).  It's also
riskier because it involves hardware, which has more up-front cost and risk of
loss than writing code (failures are more expensive and less reversible).
It's also more challenging because it requires skills that are not as
developed as my programming skills (although extensive programming is a part
of it).

  

However what I came to realize is that solving telepresense reduces the
friction and overhead of a lot of other areas I'd like to work in.  It removes
the geographical constraints and would allow me to work on anything, anywhere,
anytime.  This, in addition to many of the other problems that telepresense
can solve make it seem like the right choice for the next thing.

  

Work on [ Sux0rz ](http://jjg.2soc.net/post/sux0rz) has taught me a lot about
what it takes to build a good telepresense robot, especially the experience of
taking it to [ Makerfaire ](http://jjg.2soc.net/post/makerfaire-
milwaukee-2016) .  From this experience, I've already started to draw-up
designs for a next-generation machine, and I'm excited to get started on that
build.

  

Whether or not I can find a way to turn this work into something self-
sustaining remains to be seen.  I believe that there are many practical, high-
value applications for affordable, high-quality telepresense machines (even at
the consumer level), but before I can prove that I need to develop some
machines that can be used to test these applications, as well as discover new
ones.

---
title: The Post-Twitter Economy
date: 1970-01-01
tags:
  - evernote
---
I'll be honest, this has nothing directly to do with economies but it just
rolled off the "tongue" nicely.  
  
It appears that Twitter's decline into "burning the brand" is no longer a
prediction but a meaurable fact.  Naturally this leads to thoughts of "what's
next?".  
  
There's certainly no shortage of Twitter work-alikes out there (I'm a user of
at least two) but to [ paraphrase Tyler Durden
](http://www.imdb.com/title/tt0137523/quotes) , "...I wonder if another
microblogging service is really the answer we need?".  
  
Indeed, stepping back a bit could the things we use Twitter for be better
served by something radically different?  Twitter was originally designed for
SMS after all, having bolted-on additonal functionality to the point where it
became useful for more than just letting your pals know which session you're
attending at SXSW.  
  
At this point it's worth noting that for a long time this additonal
functionality was provided by third-party developers exclusively; the same
group that Twitter now chooses to alienate.  
  
This perhaps provides the most direction for what might truely surpass
Twitter.  If Twitters fall was brought on by the choice to become hostile to
the developers who created its value in the first place, perhaps a successor
would have no such weakness by design; not just a benevolent overseer who
promises to play nice forever but an architecture which makes it impossible to
exclude developers from access to the system?

---
title: The Price
date: 2007-05-22
tags:
  - evernote
---
In 1995 I declared that you can own a motorcycle for $1000.00. This was in
response that I gave anyone who was interested in getting started in
motorcycling but thought it was too expensive based on the going rate of new
motorcycles at the time.  
  
The principle is this: If you spent $1000.00 on a used bike you could find one
that ran and was in good enough shape to ride at least for one season. If you
spent $500.00 on a used bike, you'd probably need to put another $500.00 into
it (typically battery, tires, etc.) to get it ready for serious riding. The
same goes for $250.00/$750.00, $0.00/$1000.00, etc.  
  
I think this general rule still holds true today, although I might bump the
base cost to something like $1200.00.  
  
I've started [ Google Spreadsheet ](http://spreadsheets.google.com) (still
trying to figure out how to share it with the general public) to track the
cost of my little project here to see if this theory still holds true in the
year 2007.

---
title: The problem with almost every (messaging) application
date: 2013-02-25
tags:
  - evernote
---
When you suddenly remember you need to send that email, or feel inspired to
tweet a message or share that photo on Facebook what is the first thing you
see when you open that application or website? Chances are it is a list of
other photos, updates, messages or posts.  
  
How often do you fire up your mail application with the intention of writing a
message but find yourself distracted by a list of new messages? It happens to
me daily, to the point were I often forget to write the message I intended to
write in the first place.  
  
This is a design flaw.  
  
Our marketing folks spend a lot of time determining our websites visitors
intent in order to present the visitor with the most appropriate parts of our
service, and I think other applications could benefit from this technique as
well. This might not seem like an easy task, especially when we are used to
combining multiple functions (the reading and writing of messages, for
example) into a single application, but perhaps the solution is to decouple
these activities; after all you don't open your notebook to read the (snail)
mail, and you don't sit down at a piano to listen to your favorite
recordings...

---
title: The RepRap Page
date: 2013-01-06
tags:
  - evernote
---
Finally getting around to putting a page together with details on my RepRap 3D
printer project:  
  
[ **The RepRap Page**  
  
](http://www.gullicksonlaboratories.com/projects/reprap/ "RepRap")  
  
I'll be adding more details soon as well as information about building,
calibrating, testing and operating the machine.

---
title: The waiting game
date: 2013-12-04
tags:
  - evernote
---
I'm excited to make the switch to postach.io.  My original plan was to wait
until their Wordpress migration tool was ready, but I might alter that course.

  

Evernote has become more central to my workflow, and while I don't love having
non opens-source software in a critical role, it's the best option at the
moment and they have a good API if I need to eject.

---
title: The wait is over!
date: 2013-12-09
tags:
  - evernote
---
After giving it a few weeks, I decided to spend some time today learning the
Evernote API and writing something that could import my old Wordpress blog
posts so I could start using Postach.io.

  

The results of this work can be found here: [
https://github.com/jjg/wp2evernote ](https://github.com/jjg/wp2evernote)

  

It’s incredibly primitive, and if you follow me on Twitter you know that I’ve
already run it once (you might want to disable auto-tweeting new posts while
you run the script).  It’s incredibly buggy, and can have nasty side-effects
if you don’t read the directions, but it was “good enough”, so I wanted to
share it before it sat on the shelf waiting to become “good”.

  

A side-effect has been that I’ve learned some more Python tricks as well as
enough of the Evernote API to be dangerous.  Since I’m leaning pretty heavily
on Evernote these days this is a good thing, and I’m excited to find ways to
leverage this newly acquired knowledge.

  

In the meantime I’ll be sorting out why 419 of my posts failed to migrate, and
if I get them working, I’m sure you’ll hear about it...

---
title: The Web We Were
date: 2011-09-08
tags:
  - evernote
---
[ Pete's post ](http://rasterweb.net/raster/2011/09/01/reclaim-blogging/) got
me thinking a lot lately about my first experiences with the web as both a
consumer and a producer (it also renewed my interest in blogging but I'll
leave that discussion for another time).  I honestly can't remember the first
time I used the web but I can remember the first website I created, I called
it "Bachelor Pad Online".  
  
  

It's hard to say exactly when I made this site but it had to be before 1997,
it was right around the time I was getting into lounge music (hence the theme
for the site, inspired almost entirely by the [ Ultra-Lounge
](http://en.wikipedia.org/wiki/Ultra-Lounge) compilation album " [ Bachelor
Pad Royale ](http://www.amazon.com/Bachelor-Pad-Royale-Ultra-
Lounge/dp/B000002TZM) ").  This was when I taught myself HTML, and I
distinctly remember printing off the entire W3C specification on a
surprisingly small number of pages and binding it in a thin red binder (I
believe the most recent addition to the specification was "frames"...oh my
yes...).

  
  

The site was hand-coded HTML of course and edited in "SaintEdit" on my [
PowerBook 5300 ](http://en.wikipedia.org/wiki/PowerBook_5300) .  Images were
created care of "ColorIt!" (an ironic choice in light of the 5300's greyscale
display...) and the site was hosted by my Madison-based ISP "IntraNet" (a name
that has always perplexed me).  At the height of the sites sophistication it
even sported a Java-applet-based chat room (dubbed "The Viper Lounge") which
had a few regular visitors but more often than not an endless stream of
confused surfers who thought the sites black & white nature was a problem with
their monitor.

  
  

Unfortunately no remnants of this early example of my web handiwork remain
(that I am aware of).  The last physical copy existed on a 1.44MB floppy disk
which has long been lost and even if I still had it I'm not sure what I'd read
it with.  I have attempted to locate the site in the [ Wayback Machine
](http://web.archive.org/) but to no avail.

  
  

Since that time I've built several "personal" sites, hosted my own servers in-
house (even a [ gopher server
](http://quux.org:70/Archives/Mailing%20Lists/gopher/gopher.2002-01%7C/MBOX-
MESSAGE/32) for a brief time) during the beginning of the "home broadband age"
and like everyone else have used countless third-party services to publish to
the web.  More recently I have been relying on these third-party services for
most of the information sharing that I take part in but based on Pete's post
(and the conversations he links to) as well as some encouragement by recent
failures and shortcomings in these services I'm in the process of returning to
systems I have built myself, whose code I control and whose data I can easily
move from place-to-place if need be.  I'll be sharing more about these changes
in future posts but in the meantime consider what you are doing with the
content you create and share and think about where it lives, who owns it and
what might happen if there were a sudden change in the companies you currently
rely on to share your information.

---
title: Thingimultiverse
date: 1970-01-01
tags:
  - evernote
---
_This started as a long post to the G+ 3D Printing Group.  Here I attempt to
elaborate on that post by providing more details about architecture,
implementation and engaging an audience._  
  
I think we're getting to a point where a distributed, un-censorable object
repository system will become a necessity Thingiverse is great, but we've
already seen that they are willing to compromise content due to external
pressures.  
  
I believe any single site, system or company is vulnerable to this sort of
pressure and furthermore I believe it is their right to choose what they host.  
  
However, if history has taught us anything it is that allowing information to
be at the mercy of any level of tyranny can lead to loss of knowledge; due to
accidents, coincidence or in worse cases deliberate destruction.  
  
What I propose as a remedy to this is a system of federating this information
instead of relying on any single entity, vulnerable to the systemic weaknesses
present in any type of organization known today. A federated system,
distributed across physical and political regions in order to avoid
constraints imposed by any type of external method of control or coercion.  
  
At a less abstract level, this amounts to object repositories with
standardized access methods and standardized catalog data that can be easily
replicated to allow for indexing and searching across the entire federation of
repository sites.  
  
The technology for this certainly exists (either in parts or perhaps in
whole), and I'm looking for feedback from this group to determine if perhaps
this work is already underway. I am aware that there are other sites like
Thingiverse out there, but at this point I'm unaware of any deliberate effort
to publish their content in a way that allows for global search across sites
(without relying on other centralized corporate entities like search engines)
or replication designs intended to distribute models across sites in an
automated fashion (necessary to prevent things from disappearing when a site
or object is taken down voluntarily or by force).  
  
I believe now is the time to design these standards, while the proliferation
of "thing" repositories is small and the control mechanisms are still in their
infancy. If we wait too long, we'll all become too attached to our personal
favorites and implementing a federated system will become more difficult.
Doing something now also makes it harder for those who would desire to curtail
such an effort if it has already demonstrated its value to a worldwide
community.  
  
I'm planning to elaborate on these ideas and investigate implementation
options in the near future, so any information about simular or overlapping
projects you can share is appreciated.

---
title: This is how products of the future will be born
date: 2012-12-15
tags:
  - evernote
---
_**Update:** http://www.gullicksonlaboratories.com/test-the-market-for-your-
tech-with-tindie-fundraisers/: [ Tindie ](https://tindie.com/) is doing
something very cool to encourage just this type of behavior, I've written a
little about it here: [ Test the market for your tech with Tindie Fundraisers
](http://www.gullicksonlaboratories.com/test-the-market-for-your-tech-with-
tindie-fundraisers/ "Test the market for your tech with Tindie Fundraisers") _  
  
[ IR-Blue ](http://www.kickstarter.com/projects/andyrawson/ir-blue-thermal-
imaging-smartphone-accessory?ref=card) is a spot-on example of how products of
the future will be created.  It goes a little something like this:  

  

  1. I need a product that is unobtainable (doesn't exist, too expensive, etc.) 
  

  2. I build one for myself 
  

  3. Other people want one 
  

  4. I crowdfund a production run (if the crowd supports it) 
  

  
  
  
Rob needed a thermal imaging camera to use around the house but commercial
units (designed for commercial applications) were too expensive, so Rob
figured out how to [ build one himself ](http://www.instructables.com/id
/Thermal-Imaging-Phone-Camera/) using more-or-less off-the-shelf components.  
  
http://youtu.be/pIb1scnD67o  
  
As [ word got out ](http://hackaday.com/2012/11/12/building-a-thermal-imaging-
sensor-from-scratch/) about Rob's project, it turned out Rob isn't the only
person in the world that needs a non-commercial-grade thermal imaging camera.  
  
A project like this is impractical to implement in mass amounts using off-the-
shelf parts, so Rob figures out what it would take to mass-produce them and [
starts a Kickstarter project ](http://www.kickstarter.com/projects/andyrawson
/ir-blue-thermal-imaging-smartphone-accessory?ref=card) .  
  
If enough people need the product, the project gets funded and units get mass
produced.  If not, Rob still has the camera he needed, and can hand-make some
more for people who need one enough that the extra expense of a hand-made unit
is acceptable.  
  
_(in a perfect world Rob will have open-sourced the design, so if he isn't
interested in continuing work on the project it can be picked up by others who
have different resources at their disposal; I'm not aware if Rob went this way
or not)_  
  
There are numerous advantages to this approach but I think the best is that it
applies Darwinism to the survival of products at their earliest stages, and
the cost of "failure" is almost eliminated (or at least can be).  Since Rob
needed the camera, and built one to meet his needs, if the product development
were to stop right at that point, nothing is lost.  If it continues, then
others benefit from Rob's initial work, Rob benefits as well and the world is
a better place.  
  
This also reduces the role of Advertising as a means of convincing the world
they need a product because a company has sunk millions into creating it and
now needs to generate revenue or suffer a loss.  
  
The natural argument to this approach is to say that there is a ceiling on the
scale of product that this process can handle, and I agree that there are
things that simply are not reasonable for an individual to construct in their
garage, in their free time using their own financial and other resources.  I
have two counter-arguments to this:  
  
The first is no argument at all, and to accept the fact that products "too
big" for this approach are perhaps bigger than what the world needs.
Certainly you can cite exceptions to this argument, however consider that the
scale of product that can be executed this way is a function of time, and
products that seem too big today will not be too big tomorrow (if you need
evidence of this consider 3D printing, digital computer or refrigeration).  
  
The second argument is that it is our current mode of operation that limits
our ability to scale big products using small resources.  In reality, almost
every epic product comes from a small team, and it is only the antique
orientation toward mass production and other industrial revolution prejudices
that convince us that big things cannot come from small teams independently
backed and collaborating openly with others solving the same problems around
the world.  
  
A second reservation some will have is "what if I don't know how to build my
own?".  This is a trickier bit and something I've working on providing
solutions for.  This is an important role I see hacker/makerspaces filling,
and I'm also [ working on another project
](https://github.com/jjg/pullstarter) which is a sort of inverted-Kickstarter
designed to facilitate people with ideas but not necessarily the means to
connect with other individuals and help realize their products.

---
title: This might be my next personal computer
date: 2016-09-02
tags:
  - evernote
---
I have to admit, I'm fanboy-ing over [ this new Lenovo
](http://shop.lenovo.com/us/en/tablets/lenovo/yoga-book/yoga-book-android/) a
bit.  

  

Can we just look at that hinge for a moment...  

  

  

  

  

  

  

I'm torn because I'm really not excited about having to choose between Android
and Windows (Android, obviously).  Like most Lenovo's I imagine you could get
Linux to run on this thing but you'd be waiting a long time (and probably
writing drivers yourself) to make most of the interesting hardware work.  

  

But then again, would I need to run anything other than [ Evernote
](https://evernote.com/) on it?  

  

I'm pretty dubious about Android's security, but perhaps there are solutions
to that at this point (this isn't an iPhone vs. Android question, it's an
Android vs. Linux one).  If I can find ways to satisfactorily mitigate
Android's holes then I could probably live with this thing running Android
because it would amount to  little more than a kernel for Evernote.  

  

It looks like the street price is going to be around $500.00, but I wasn't
able to get a lock on release date.  I'd also like to lay hands on it before
making a decision as I'm really picky about keyboards.  That said, regardless
of how well the pen and keyboard work, it would be a significant upgrade from
anything I've used before, and a step in the right direction.  

  

Plus, I just like supporting people who have what it takes to bring something
cool to market (cough, _[ Microsoft
](https://en.wikipedia.org/wiki/Microsoft_Courier) _ cough)  

---
title: This week's reading
date: 2012-11-29
tags:
  - evernote
---
[ ![image](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/11/wpid-2012-11-29_06-29-47_5.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2012/11/wpid-2012-11-29_06-29-47_51.jpg)

---
title: ...time to make the zip line...
date: 2009-06-20
tags:
  - evernote
---
  
  

---
title: Tinkercad and Unhosted-style distributed processing
date: 2013-04-01
tags:
  - evernote
---
Troubleshooting the [ shutdown of Tinkercad
](http://blog.tinkercad.com/2013/03/26/announcing-airstone-and-the-closure-of-
tinkercad/) got me thinking about [ high-performance computing
](http://en.wikipedia.org/wiki/High-performance_computing) and distributed
architectures again (specifically in researching alternatives to the [ Gen6
kernel ](https://tinkercad.com/developer/gen6-intro.html) at the heart of
Tinkercad's objection to going open-source). While reading the [ Tinkercad
developer docs ](https://tinkercad.com/developer/) to gain an understanding of
the interface or API of the Gen6 kernel (with the intention of sourcing or
implementing a replacement for these server components of Tinkercad) it
occurred to me that with the introduction of [ Webworkers
](http://en.wikipedia.org/wiki/Web_worker) and [ WebRTC
](http://www.webrtc.org/) , as well as the commonplace nature of moderately
powerful GPU's in many computers, perhaps a truly distributed processing
architecture could take the place of the Gen6 kernel in an open version of
Tinkercad?  
  
But why stop there?  
  
Using something like the Javascript/JSON job control structures used by
Tinkercad, why not allow any processing job so defined be submitted to a
browser-based processing grid? Designed correctly, a compute-bound application
could emit its work as chunks of code+data as Javascript functions and JSON to
this grid and tap into extra clock cycles of underutilized systems (perhaps
simply interleaving requests within the single application's domain, or across
a more global subset).  
  
This seems like it could be a nice complement to the [ Unhosted-style
](https://unhosted.org/) decoupled identity and data concepts, adding high-
performance processing capabilities to Unhosted applications.  
  
Participation in such a distributed architecture needn't be limited to
general-purpose machines either. Specialized processing nodes based on GPU or
custom FPGA devices could be pooled in this fashion, or even spare cycles on
specialized devices such as network hardware or consumer electronics; but
starting at the browser seems like the lowest point-of-entry.

---
title: T Minus One
date: 2000-11-07
tags:
  - evernote
---
So here we are, just one day from another election. I was thinking about doing
a smart-ass poll or something, but I didn't think about it untill Saturday
night, which based on how fast my personal dev time is going, I wouldn't have
it done untill maybe next week, which would kinda defeat the point, but I have
another idea....  
  
What about a post-election poll? This sounds stupid at first, but wouldn't it
be interesting to see who we all actually voted for? It would seem obvious
that most of us voted for the winner, however there is this little thing
called the electoral college that if you know about it might make you
reconsider the above statement.  
  
yeah, that made allot of sense  
  
So, I'm going to try and code up a simple polling tool (yeah I know they're
out there, but it's so much fun re-inventing the wheel...).  
  
We'll see you tomorrow.

---
title: TouchFire (Part 1)
date: 2012-07-14
tags:
  - evernote
---
The [ TouchFire ](http://www.touchfire.com/) is an iPad accessory designed to
make the iPad's "soft" keyboard more like a real, physical keyboard.  
  
[ ![20120713-162647.jpg](http://www.gullicksonlaboratories.com/blog/wp-
content/uploads/2012/07/20120713-162647.jpg)
](http://www.gullicksonlaboratories.com/blog/wp-
content/uploads/2012/07/20120713-162647.jpg)  
  
  
I'm writing this review using the TouchFire as well, to get some idea of how
it works as a new user and compare this over time.  
  
Right off the bat it takes some getting used to. Part of this is because the
keyboard fees unlike anything else I've typed on before; it feels a bit like
bubble wrap. The other thing that is strange is, having using the iPad
onscreen keyboard for a long time, remembering to treat it like a regular
keyboard (resting your fingers on the home row, for example) takes some
getting used to.  
  
[ ![20120713-162930.jpg](http://www.gullicksonlaboratories.com/blog/wp-
content/uploads/2012/07/20120713-162930.jpg)
](http://www.gullicksonlaboratories.com/blog/wp-
content/uploads/2012/07/20120713-162930.jpg)  
  
That said I was able to see an improvement in typing speed almost immediately,
and as I type this review, it's becoming better and better.  
  
There is also a stickiness to the keyboard that is a little weird but I've
heard that goes away with use, so we'll see how that goes.  
  
My larger concern is with the storage of the keyboard. Neatly, it is very thin
and folds up nicely into the Apple Smart Cover on my iPad 2, but I worry about
it becoming deformed by the pressure of the cover or by the pressure of my
hand holding the device closed. I also worry that it may get creased by
accident if I'm not careful closing it an I accidentally fold-over part of the
keyboard when I'm closing the cover. Perhaps I'm just being paranoid, but
again, we will see.  
  
I'll be doing some more writing with the keyboard over the next week which
should give me enlighten time to get over any "learning curve" associated with
the device as well as break-in the plastic and see how it holds up over daily
use. After this I'll be posting a follow-up review to share my findings.

---
title: Turning Point
date: 2007-06-29
tags:
  - evernote
---
So last weekend I had a little time during a visit from the sister-in-law and
nephew and was able to get the remaining (important) parts left in the
"basket" that came with the bike and attach them where they belong.  
  
[
![](http://lh4.google.com/jason.gullickson/RokEpKTiBvI/AAAAAAAAAcA/oufsC4Ey8Ys/s400/IMG_0006_5.JPG)
](http://picasaweb.google.com/jason.gullickson/VQ1005Photos/photo#5082598759651739378)  
  
Lib and I hit the local AutoZone and picked up a fresh pair of plugs. When we
got back I showed her how they work by plugging one in and laying it on the
head while she turned over the motor. Not the fattest sparks I've seen but
then again we were working in direct sunlight.  
  
[ ![](http://lh6.google.com/jason.gullickson/RokErqTiBwI/AAAAAAAAAcI/yUhdmNGI-
6Y/s400/IMG_0007_6.JPG)
](http://picasaweb.google.com/jason.gullickson/VQ1005Photos/photo#5082598802601412354)  
  
Once the plugs were in place all that remained was to attach the left-side
carb and then rig up the Fuel IV. Attaching this carb was trickier than the
other due to the oh-so-cool-but-always-in-the-way high pipes the exit on the
left side (I'm not 100% sure but I think you have to remove the exhaust to get
the left side-cover off…rather un-Honda). Finally the carb was mounted and
even the crazy choke linkage was working properly.  
  
Now back when I started this project I debated whether I should properly
"rebuild" the bike or just put it back together and see if it works. Under
different circumstances I would have opted for the former but as the deadline
of the next Crud Run looms, I found the latter more prudent. I decided that I
would put the bike back together with minimal "tweaking" and if it runs, then
we'll just fix whatever problems come up. If not; well then it's time to take
it apart and restore it properly.  
  
Once the parts were in place I pulled the gas tank and the seat and prepared
to wire up the Fuel IV. Since the tank is old and rusty, not to mention the
unknown state of the petcocks I decided the best way to test run the motor
would be to build some MASH-style apparatus that would let me get a little
fuel to the carbs without using the regular fuel tank. What I came up with
consists of a length of fuel line, a "T" connector and a small plastic bottle
which once contained airsoft pellets.  
  
[
![](http://lh5.google.com/jason.gullickson/RokFtaTiCHI/AAAAAAAAAfA/cqA4G_X8v5I/s400/IMG_0033_2.JPG)
](http://picasaweb.google.com/jason.gullickson/VQ1005Photos/photo#5082599932177811570)  
  
The airsoft bottle was perfect because it already had a neat little spout that
would hold the fuel line snugly. There were some other potential design
problems but I just ignored them for this first test. JC Whitney sells a more
sophisticated version of this tool for something like $60.00 which was way
more than I needed to spend so I decided to go the DIY route.  
  
After cutting the lengths of hose and fitting them to the "T" it took a little
doing to get the hose connected to the carbs (especially the left one…stupid
pipes!) but finally it was done and we were ready to test-fire the motor.  
  
I filled the bottle about half-way with fuel; the entire apparatus was
untested so I wanted to use the smallest amount of gasoline possible should it
just spill all over the place. After filling the bottle it was screwed to it's
top and then inverted, and you could see the fuel flowing down the lines and
into the carburetors.  
  
Ignition on, hit the starter…nothing.  
  
Well almost nothing. Fuel begins to spill on the ground from the overflow on
the right-side (high) carburetor. I quickly attach the rotted overflow hose to
stop it from running all over the engine. Well we know fuel is getting to the
carb, let's try turning it over again and see what happens…  
  
Nothing.  
  
I flip the fuel bottle over to stop the flow and examine the carburetors. The
right-side carb (which is higher than the left as the bike is on the side-
stand) is clean but looks like it dumped all the fuel that went into it on the
ground. The left-side carb, which didn't eject anything out of it's overflow
is now starting to seep fuel from around the float bowl. I pull the plugs to
see what's getting into the chamber and they are both bone dry.  
  
I guess the carbs are not in as good of shape as I had previously observed.  
  
I turn the drain screws on the carb bowls and let the remaining fuel drain
down onto the engine and drip from there into the fuel bottle I was previously
using to feed the engine. There is a nice little "low-point" cooling fin on
the bottom of the motor that almost magically collects all the fuel running
over the crankcase neatly into one spot where it can easily be collected. Nice
to know…  
  
I spent that night considering the meaning of the day's events and deciding
what the next best plan of action would be. Based on my original criteria, it
was probably time for a rebuild, but fixing carburetors is a lot easier than
that. However what's to say once the carbs are "on-line" that there wont' be
another problem waiting in the wings? Finally I came up with three options:  
  
1\. Go down the rabbit hole, beginning with rebuilding the carbs  
2\. Tear the bike down to the frame and begin a proper restoration  
3\. Convert it to electric drive  
  
I gave these three options about 48 hours of contemplation and came up with a
hybrid solution. I realized that going down the path of troubleshooting each
discreet problem as it arose could be a long process with nothing but failure
until finally everything worked right (at least for awhile). I seriously
considered the electric option, but there is a significant up-front cost that
(at least for now) seems to exceed the cost of any other option.  
  
In the end I realized that even going electric would require (or at least,
desire) a rebuild of the rolling chassis so that is where I plan to begin.
Ignoring the engine and related things for now, I plan to pull the bike apart
and restore the rolling chassis to new (or better-than-new) condition. During
this time I'll continue to evaluate electric conversion options and hopefully
come to a solid conclusion by the time the chassis is restored and the time
comes to deal with the engine.  
  
I have a feeling I'll have plenty of time to think about this.  
  
[
![](http://lh5.google.com/jason.gullickson/RokEhaTiBsI/AAAAAAAAAbo/4nr1aHCkbF0/s400/IMG_0003_6.JPG)
](http://picasaweb.google.com/jason.gullickson/VQ1005Photos/photo#5082598626507753154)

---
title: Turning the corner (cleanly)
date: 1970-01-01
tags:
  - evernote
---
This post is about more Reprap tuning, so if you don't find stuff like this
interesting, feel free to tune in next week. The first test turned out like
this:  

  

  * Layer height: 0.254 
  

  * First layer height: 0.35 
  

  * Perimeter speed: 30 
  

  * First layer temp: 190 
  

  * Layer temp: 185 
  

  
Dimensionally the part is close, the sides are within .1mm but the height is
about 10.3, which has me worried I have something wrong in the firmware since
I swapped out the z-rods. The thing I was trying to address are "blobs" on the
corners of the Tantillus parts I printed last night. Unfortunately (?) when I
printed this part with the same settings I don't get those blobs. For now I'm
continue to test to get the dimensions closer, and to address one distortion
I'm seeing where the z axis changes layers. Second Test  

  

  * Layer height: 0.25 
  

  * First layer height: 0.25 
  

  
This didn't seem to improve the dimensional accuracy, and about 1/3 the way
through the print left the table, so we should re-test with an initial height
of .35 to see if the part sticks again. Third Test  

  

  * First layer height: 0.35 
  

  
This didn't appear to have any effect either, but at least the part stayed on
the table. Fourth Test  

  

  * All layer temps: 190 
  

  
I tried making the temp consistent across layers to see if I could get the
first layer to stop shrinking relative to the others.  Visually, the part
looked a little better but it didn't make a difference in what I'm testing so
I'm taking the temp back down to 185. Fifth Test  

  

  * First layer temp: 190 
  

  * Other layer temp: 185 
  

  * Retract: 0 
  

  
This did wonders for the distortion when the z axis is lifted, but I'm afraid
that it's going to cause stringing so adding back .3mm of retract (vs the
original 1mm). Sixth Test  

  

  * Retract: .3 
  

  
This looked alright but since the cube I'm testing with won't tell me much
about stringing at this point I re-printed one of the real parts I printed
last night for comparison. Seventh Test  

  

  * Extrusion multiplier: 
  

  
Lowering the extrusion multipler is a way to determine the correct e-steps you
need to make sure you're feeding the right amount of plastic in to get the
right amount out; too much and you get blobs, so i'm testing reducing this.

---
title: Turntable Reprap
date: 2013-01-18
tags:
  - evernote
---
_Lately I've been a bit obsessed with building a second Reprap printer, and in
particular interested in unconventional designs. One design that is
particularilly interesting to me is the idea of a Reprap with a rotating build
platform (like a turntable), and in my research I ran across references to the
information below._  
  
_The page linked to now returns a 404, but I was able to dredge up the content
using The Internet Archive's Wayback Machine. I have reprinted it here to
ensure that it remains avaliable, howerver if that is in violation of anyone's
rights I'd be happy to cooperate with moving it elsewhere, my primary concern
is that this information doesn't just dissapear into the vastness of the
Internet's Write-Only memory..._  
  
Turntable Proposal  
  
Hans Wargers  
  
It can be made with only a drilling machine and other simple not expensive
tools.  
It is driven by a computer in delphi but the dll can also be used by other
languages. It has x,y and z axes.  
It is also possible to work with one motor but this one is much easier to
make.  
You have a rotating surface something like a turntable.  
Here on this rotating surface the model is printed. This surface is driven  
by a electro motor  
The print head moves a right angle on the direction of the moving surface.  
These print head is driven by a threaded rod .  
It is the same principle as a lathe on which the two movable components  
(claw plate and support) are coupled by means of cog wheels and a threaded  
rod to each other.  
At each complete rotation of the rotating surface, the distance that the  
print head on the threaded rod has taken is equal to the size of the  
resolution of the print head  
  
For example the resolution is 1 mm and the pitch of the threaded rod is 1  
mm by each rotation(M6 or M7).  
At each rotation of the surface then also the threaded rod must make a 1  
revolution.  
When printed by the print head there will be a track of ink drops (similar  
to  
the spiral of the pit in a .L.P.) and this track covers the total surface  
without inter space  
As the print head has reached the end of the threaded rod .  
The electro motor stops and will start turning into the opposite direction.  
the print head succeeds the old track but only from the opposite direction  
and can therefore simply continue to print.  
As the print head reaches the start point, the electro motor stops and will  
start turning into the opposite direction again.  
This is repeated till the piece of work is printed.  
There are 2 sensors necessary: one to stipulate the beginning point of the  
threaded rod and one to stipulate the rotation of the rotating turntable  
each time. Everything can probably be made by means of rp except for the  
electro motor and the two sensors.  
Here some photo's and a movie made with two electromotors and a construction
possible with one electromotor. However not interesting on this moment.  
  
Parts wich are necesarry and photo's of it. Available at www.conrad.nl  
  
F-LAGER 8/16 (Bestnr.: 215295)  
  
C-KOGELLAGER 6/13 (Bestnr.: 214485)  
  
LAGERBLOK 13 (Bestnr.: 216003)  
  
DRAADSTANG MESSING M6 (Bestnr.: 221789)  
  
IGARASHI MOTOR 33G-125 (Bestnr.: 244031)  
  
TANDWIEL K 1:1 30T (Bestnr.: 219304)  
  
12V MOTOROMPOOLRELAIS (Bestnr.: 505013)  
  
MCA 2 NEIGINGSSCHAKELAAR (Bestnr.: 185442)  
  
USB EXPERIMENTEER INTERFACE BO (Bestnr.: 191137)  
  
  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/IMG_0287_2.JPG)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/IMG_0609.jpg)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/IMG_0612.jpg)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/IMG_0616.jpg)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/IMG_0631.JPG)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/koppe2rp.jpg)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/vooraanzichtrp.jpg)  
  
![](http://web.archive.org/web/20120205070716im_/http://staff.bath.ac.uk/ensab/replicator/Downloads/Wargers/Pix/zijaanzicht.jpg)

---
title: Twelve Days
date: 2008-09-24
tags:
  - evernote
---
The idle issue was solved today with some timing adjustments and cleaning of
the mechanical advance mechanism.  
  
The fuel leak has been located and can safely be ignored*.  
  
The remaining issues are a minor missing/backfiring issue at 1/4 throttle-ish
and the back brake has this "grabby" thing going on (who uses back brakes?). I
have almost two miles on the bike now, all of them up-and-down the street in
front of my house (much to the joy of my neighbors, I'm sure).  
  
The thing is loud, almost painfully so. I'm open to (cheap, fast and easy)
suggestions but chances are that I won't bother with it until after the run.  
  
At this point I think I'm done until the bike can be on the street legally. I
need to do a longer road test to figure out if the jetting is where it needs
to be but I'm chicken to do it until I have a plate on the back. It would be a
different story if I lived in the country...  
  
  
  
*for the time being, it only leaks when the petcock is open and the bike isn't running. 

---
title: Two Days
date: 2008-10-03
tags:
  - evernote
---
It's interesting, the motorcycle thing.  
  
In cars, most of the technology is driven by the needs of common, boring
street use. Motorcycle technology, on the other hand, seems to be driven
primarily by racing, secondarily by nostalgia or physical beauty.  
  
There was a point back in the 1970's or even late '60's where motorcycle
technology matched automotive technology in every measurable way. From that
point on, bikes took the lead and now there's no comparison.  
  
So with the practical transportation aspirations of automobiles behind them,
bikes went down this bizarre road where the only reason to get excited about a
new bike is either numerical gains on the dyno or aesthetic (specifically,
visual) beauty. Marketing has honed this to an extent that to the general
public, motorcycling **is** these two things and all of the subtle nuances
which make riding great are lost (or at best, ignored).  
  
To summarize, every motorcycle made today from the cheapest beginner bike to
the most expensive luxury liner is capable of performance exceeding anyone's
"needs", so if you're going to continue to sell new motorcycles you have to
focus on things that change over time even though they are irrelevant to what
motorcyclists know it's all about.  
  
The sad thing is that we (the people who buy motorcycles) are responsible for
this. The evidence is that, throughout the history of the motorcycle (in
particular, the last two decades) when manufacturers dared to come out with a
model that circumvents this one-upmanship approach to sales, these models go
unsold. Often these models become appreciated only long after the manufacturer
has dropped them from production due to poor sales (the Hawk GT being an
excellent example, but there are many more).  
  
I see electric bikes as a potential cure for this problem. This has nothing to
do with efficiency or performance but with the fact that they are different
enough from people's expectations that we have a chance to reset the image of
what makes bikes sell (out of necessity really, because in the near term
electric bikes can't compete with gasoline models on all measureable
performance fronts simultaneously). Honda did something like this back in the
1950's when they first brought their bikes to the states ("You meet the nicest
people on a Honda"), the bikes were so different as to be perceived as a
different class of vehicle from the large hooligan-machines driven only by the
violent and the unwashed.  
  
It would be an interesting intellectual experiment, to design a motorcycle
company to produce a product like this and tweak the variables until it
becomes sustainable (i.e., profitable). Similar things have been done before,
Saturn did something like this in automobiles (until their short-sighted
parent company pulled the plug on the philosophy and "burned the brand").  
  
...now I have something nice to think about this morning...  
  
_As you may have noticed, I don't have any good news to share about the
project. With no title in hand the odds are not in favor of getting the bike
to the fall run. That's all I'm going to say for now as the whole situation is
distressing and I'd rather change the topic._

---
title: UFO
date: 2016-10-23
tags:
  - evernote
---
On October 22, at approximately 7:40PM Central standard time, we saw an
unidentified flying object (UFO) over our home in Beaver Dam, Wisconsin.  

  

We were having a campfire in our backyard and I was looking up at the clear,
starry sky as we often do.  I stood up to warm myself by the fire and
continued to look up at the sky.  About a minute after I stood up I noticed
something strange coming from the west over the trees in our neighbors yard.
When it went over the pine tree in our yard I said something like "what is
that?", and my wife Jamie and daughter Liberty began to look for it.  Jamie
suggested it might be a meteoroid as there was a shower expected, but then she
saw it as well and knew it wasn't a meteoroid.  Shortly thereafter, Liberty
saw it as well, and they both saw it long enough to identify it in a way
similar to what I was seeing.  

  

The best way I've come up with to describe it is like this:  Imagine the night
sky reflected in a still, dark pond.  Now imagine dropping a single pebble
into this pond and how the waves created alter the reflection of the sky.  The
UFO looked like a single one of these waves, but cut into an arc of about 1/4
of a circle, slightly pointed at the center of the leading edge.  As it moved
across the sky, it obscured the stars causing a small amount of refraction,
with only the leading edge clearly visible.  I'm not sure if the leading edges
visibility was due to it emitting or reflecting light, or if it was just a
more intense distortion of the night sky.  

  

The objects speed and direction was constant, and it moved silently.  

  

It's hard to judge the altitude and therefore the size of the object, but it
was at least 200 feet above the ground because it was above the trees.  If it
was only that high up, then it was approximately 16 feet across from wingtip
to wingtip.  The total time the object was visible to me was about 8-10
seconds, in which time it traveled from behind my neighbors trees to behind
our house as it moved west to east.  This is a distance of about 50 yards.
Jamie and Libby picked up the object as it was about half-way through this
path.  

  

At first I thought it was an optical illusion, a problem with my vision or
perhaps due to me standing up too fast.  I also considered that it could be
related to smoke or heat from the campfire.  However Jamie and Liberty's
observations, and in particular their descriptions matching what I saw (before
I described it myself) ruled-out a problem with my vision as the explanation.
The other explanations (smoke, heat) didn't make sense as we continued to see
the object long after it was beyond the range of these local effects.  

  

Other possibilities we considered were animals.  The general shape was not
unlike the V formation birds fly in, but you don't usually see flocks of birds
flying at night, and the shape was too smooth to be made up of group of birds.
Also the effect it had on the starlight isn't something that would be created
by flying animals that I'm familiar with.  

  

I'm familiar with most civil and military aircraft, and the shape, speed and
translucency of the object was unfamiliar to me.  The fact that it was silent
would indicate that it was either unpowered, or so far away as to be unheard
(which would indicate that the object was of gigantic proportions).
Experimental aircraft may be an explanation as we frequently spot military
aircraft flying nearby, and we also live less than an hour from the Oshkosh
airport, home of the Experimental Aircraft Association.  

  

At this point a clear identification or explanation isn't available.  The most
rational explanation I can come up with is that it was some kind of
cloaked/stealth glider, using one of the recently described optical cloaking
mechanisms.  This would explain the shape, motion and visible signature of the
object as well as its silent flight.  That said I've never heard of such a
vehicle, and if it were a glider it would have had to have been dropped from
an aircraft as there are no natural high elevation points nearby.  It would
have also presumably landed within a few square miles, and so far I haven't
heard any other reports of the object being sighted in the city of Beaver Dam.  

  

There may be other environmental explanations as well.  Being a clear, cold
night I'm not aware of any such explanation but I'm hoping that by sharing
this story others with more knowledge of the subject can contribute additional
possible explanations.  

---
title: Updating to the release version of iPhone OS 3.0
date: 2009-06-20
tags:
  - evernote
---
  
...it's a bit more work for developers with the betas installed...  

---
title: User Base
date: 2016-08-22
tags:
  - evernote
---
The [ Crypton ](http://jjg.2soc.net/post/crypton) project is the first
software project I've had in a long time where I wanted to _use_ the software
even more than I wanted to _write_ it.  That's not to say I'm not enjoying
writing it, it's kind of fascinating, but I'm _really_ looking forward to
using it.  

  

I think that's the key to any great product, that it's created by people who
are actually going to use it.  I don't think that this guarantees that the
product will be great, but I think the opposite (products created by people
who will _never_ use them) are almost always less than great at best.  

  

Capitalists are very keen to point out the value of identifying the audience
for a product before investing too much in its development, but I think this
misses the mark.  Their concern isn't with pleasing an audience, but simply
taking their money.  What I'm talking about meets that requirement, but goes
further by providing the audience with something they actually want as well as
something that meets their needs, because they participated in creating it.  

  

In the case of Crypton it works out conveniently that someone who wants to use
it happens to be someone who can build it as well, and I think this is often
the case for many great products.  However this limits the range of products
to ones used by people who might be able to make them, and by definition that
leaves a lot of people unserved.  

  

What's needed is a methodology which cultivates products via their end-users,
somehow "mixing-in" the people necessary to bring the product into the world
while maintaining continuity with the users wants and needs.  Attempts have
been made at this but all the ones I'm familiar with prioritize something
other than the user in the process (cost, profit, marketability, etc.).

  

What is needed is _a process which is capable of creating a product with the
same level of quality it would have if created by one of its users._  

---
title: Using Reprap to bring Minecraft creations to (real) Life
date: 2013-03-10
tags:
  - evernote
---
[ ![Daisy1](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03
/Screen-Shot-2013-03-09-at-9.09.11-AM-150x150.png) ![Daisy
Printout1](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_20130309_131632_303-150x150.jpg)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.09.11-AM.png)  
  
  
  
  
  
  
  
  
  
[ Minecraft ](http://minecraft.net) is a great game, but it's also an amazing
teaching tool as well.  This post describes step-by-step how to take your
Minecraft creations and turn them into real-world physical objects.  
  
**Why Minecraft?**  
  
There's a lot of ways to create models that can be printed on a 3d printer
ranging from [ expensive, high-end professional tools
](http://usa.autodesk.com) to [ free, web-based systems designed for beginners
](https://tinkercad.com) but few of these tools offer much terms of support
for collaboration.  When they do, the traditional "windows and desktop" user
interface (while appropriate for one person working on one document at a time)
is far from an ideal experience for realtime collaboration, and their
2-dimensional interface is not necessarily well-suited for working with
3-dimensional objects.  
  
Minecraft on the other hand provides a "virtual Legos" interface for
constructing 3-dimensional objects that is immediately familiar to most kids
(and adults as well).  This, coupled with a naturally-collaborative
environment (the multiplayer game) makes building things together as natural
as doing it in real life, with an incredibly low learning curve.  
  
  
  
**Step 1: Create something awesome**  
  
[ ![Daisy: She's a big dog](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/Screen-Shot-2013-03-09-at-9.09.11-AM-300x190.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.09.11-AM.png) Daisy: She's a big dog  
  
This is Daisy, a multi-story monument designed by my daughter Liberty to
commemorate the loss of one of her Minecraft pets (when I say loss it's
unclear if the dog has passed or we simply can't find it).  The range of
things that can be created in Minecraft is extreme, but I'll leave it to [
other, more skilled Mincrafters ](http://www.minecraftforum.net/topic/1046616
-what-to-build-the-list/) to explain that process.  
  
  
  
**Step 2: Mark it**  
  
[ ![Stack Diamond, Gold, Iron to mark the area to export \(buried here so we
get some ground in the model\)](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/Screen-Shot-2013-03-09-at-9.16.12-AM-300x190.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.16.12-AM.png) Stack Diamond, Gold, Iron to mark the area
to export (buried here so we get some ground in the model)  
  
Once you've created your masterpiece you have to mark what portion of the
Minecraft world you want to print (lest you print the entire world, which
would take awhile).  This is done in-game, buy placing a special series of
blocks at the corners of the area you want to print.  Start by placing a stack
of blocks in this order: diamond, gold, iron at one corner of the area you
want to print, at the lowest point you want printed (the diamond block will
mark this point).  
  
[ ![Marker Tower](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/Screen-Shot-2013-03-09-at-9.18.33-AM-150x150.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.18.33-AM.png) [ ![Screen Shot 2013-03-09 at 9.18.57
AM](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.18.57-AM-150x150.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.18.57-AM.png)  
  
  
  
  
  
  
  
  
  
Next place another stack like this kitty-corner to the first, creating a
rectangle around the object you want to print.  The base of this stack (the
diamond block) needs to be elevated above the highest point you want to print
(remember we're marking a 3d space) so it may be necessary to construct a
tower to place this marker on.  
  
  
  
**Step 3: Export**  
  
Now that we've marked what we want to export from the Minecraft world, we need
something to do the exporting.  I use a [ Python
](http://en.wikipedia.org/wiki/Python_\(programming_language\)) script called
[ minecraft.print() ](https://github.com/codys/minecraft.print) to accomplish
this.  This script searches the entire world for the two markers placed in
step 2 above.  Once it finds them, it takes all the blocks in-between them and
exports them as an STL file, which is a format commonly used for 3d printing.  
  
If you're not familiar with Github and Python you might need some help getting
this to run.  I have created a separate page describing how to do this in
detail if you need a hand with this step.  
  
Once you have the script setup you can pause Minecraft and run the script.
You'll need to specify the name of the world you created your model in (just
the name as its displayed when you start the game, the script will find the
actual files).  You may need to put the name in quotes if it has spaces, etc.  
  
[ ![minecraft.print\(\) running in the OSX
console](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03
/Screen-Shot-2013-03-09-at-9.20.46-AM-300x220.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.20.46-AM.png) minecraft.print() running in the OSX
console  
  
If the script is sucessfull at finding your model you'll see this message,
then it will begin creating the STL file that you'll use in the next step.  
  
[ ![Markers found! If something is wrong \(no markers, too many markers\)
you'll see an error here](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/Screen-Shot-2013-03-09-at-9.20.55-AM-300x220.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.20.55-AM.png) Markers found! If something is wrong (no
markers, too many markers) you'll see an error here  
  
Now if you look at the contents of this directory you'll see a file with the
name you supplied and an extension of .stl; in our case this file is named
daisy1.stl  
  
[ ![minecraft.print\(\) output](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/Screen-Shot-2013-03-09-at-9.21.01-AM-300x220.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.21.01-AM.png)  
  
  
  
**Step 4: Verification (make sure you got what you want)**  
  
Now that we have an STL file let's make sure we got what we expected.  For
this step you'll need a way to view the STL file; on OSX I use [ Pleasant 3D
](http://www.pleasantsoftware.com/developer/pleasant3d/index.shtml) for this
purpose.  There are many other programs you can use for this, but I'll leave [
Google to help you ](http://www.google.com/#hl=en&sugexp=les%3B&gs_rn=5&gs_ri
=psy-ab&qe=c3RsIHZp&qesig=TY5JMjLImA8fcbb-
57sUEg&pkc=AFgZ2tn4c_hH9uQWgQGNiPvqw0vx8kaZWpDcNaqvA0bAPdbLN4IMEZ6VfBUAh2qTy3sNTK3Hsnusx2j0QqGk7hnGJDwkCdxPug&cp=6&gs_id=z&xhr=t&q=stl+viewer&es_nrs=true&pf=p&safe=off&sclient
=psy-
ab&oq=stl+vi&gs_l=&pbx=1&bav=on.2,or.r_cp.r_qf.&bvm=bv.43287494,d.aWM&fp=2309a8fc28997e73&biw=1152&bih=960)
with that process.  
  
[ ![Note the missing blocks in the base, this isn't what I want so time to try
again!](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03
/Screen-Shot-2013-03-09-at-9.23.40-AM-300x258.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.23.40-AM.png) Note the missing blocks in the base, this
isn't what I want so time to try again!  
  
Once you have a way to view the file, take a look at what you have and make
sure that it's what you're expecting.  In our case there are some gaps at the
base that were captured because our first marking stack was too low, so we
moved that stack and re-exported the model.  You may find that corners are
getting cut, or perhaps the top of your model is missing (or maybe some
unexpected animal has wandered into the scene), etc. so this is a good step to
make sure you're only taking what you want into the next step.  
  
  
  
**Step 5: Slice**  
  
_If you've used a 3d printer before the next two steps will seem familiar.  If
you plan to have someone else print your model you can skip these steps but I
include them for completeness and for readers who don't have experience with
3d printing but want to know more._  
  
Once you know you've got what you want in an STL file the next step to
printing it is called "Slicing", where a slicer program is used to turn the
STL file into code (specifically, Gcode) that the printer can understand.
There are a lot of options at this step, and there are several pieces of
software avaliable to slice your model but the key thing to be aware of
regardless of what software you use is the scale of the model.  
  
[ ![I prefer Slic3r, but use what you like
best](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.32.14-AM-300x200.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.32.14-AM.png) I prefer Slic3r, but use what you like best  
  
The STL file that comes out of Minecraft is probably going to be very small,
much smaller than the build capacity of even the smallest printers
(100x100x100mm).  When you open the STL in your slicing software you'll notice
this.  
  
You can print the model this small if your printer is up to the task, but I
recommend increasing the size at least 200%, and in our case we went up to
400% and achieved good results.  At 400%, the smallest surface you can create
in Minecraft (a single block) is still big enough for the printer to print
safely and sturdy enough to stand on its own and hold weight.  
  
[ ![400% seems about right, but
experiment](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03
/Screen-Shot-2013-03-09-at-9.32.51-AM-300x141.png)
](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/03/Screen-
Shot-2013-03-09-at-9.32.51-AM.png) 400% seems about right, but experiment  
  
Once the model is scaled adequitly export the gcode and you're ready to print!  
  
  
  
**Step 6: Printing**  
  
This step (like slicing) will vary greatly depending on your printer and the
software you use to operate it.  Lately I've been favoring a piece of software
called [ Octoprint ](https://github.com/foosel/OctoPrint) , which makes this
process very simple and also makes it easy to monitor the job (and record it
as you'll see shortly).  
  
[ ![Monitoring the print job using
Octoprint](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_0029-300x225.png)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_0029.png) Monitoring the print job using Octoprint  
  
[ ![Four legs, good start so far!](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_0035-300x224.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_0035.jpg) Four legs, good start so far!  
  
Regardless of your software, this step consists of preparing the printer,
uploading the Gcode file created in the previous step and starting the print
job.  Depending on the size and complexity of the model and the speed of your
printer, the printing step can take from 15 minutes to several hours.  In the
case of our example it took about 1.5 hours to complete the print job, but
this video makes it look much faster:  
  
http://www.youtube.com/watch?v=HbNt9hUaqv4  
  
  
  
**Step 7: Clean-up**  
  
After carefully removing the print from the printer you may need to clean up
around the edges.  As those of you familiar with 3d printing may have noticed
early on, there are elements to this example model that are particularilly
challenging to extrusion-type printers.  I expected more problems due to this
but in the end the model turned out better than expected.  
  
[ ![clean-up 1](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_20130309_131414_797-300x225.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_20130309_131414_797.jpg)  
  
It's possible to try to take the printers limitations into consideration while
designing the model, and there are "validation" tools that will examine the
STL file and try to identify parts of the model that will be hard to print,
but my opinion in this case is that trial-and-error is a valid way of
discovering these limitations.  In an educational environment, lessons learned
this way tend to stick with the learner longer, and require less repetition
that trying to ingrain this knowledge before or without experience.
Additionally, trying to keep all these rules in mind tends to squelch the
creative process, and often times the greatest creations come out of bypassing
rules or accidentially creating something new when the results are not what
was expected.  
  
[ ![clean-up 2](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_20130309_131639_971-300x225.jpg)
](http://www.gullicksonlaboratories.com/wp-
content/uploads/2013/03/IMG_20130309_131639_971.jpg) The final output, before
clean-up  
  
This is one of the greatest strengths of software development, and with the
advent of inexpensive 3d printing it can now be used for the creation of
physical objects as well; the capability to rapidly "iterate" over a design
from that initial creative spark through realization, testing, refinement and
then re-engineering until a desired (or greater) outcome is observed.  
  
What's Next?  
  
This is neat and all, but I really want to make it possible to do the entire
process from inside of the game.  I need to work on my Minecraft hacking
skills first, but the goal is to allow a player to create an object, mark it
and the press a button inside the game to kick-off a completely automated
export-slice-print process.  
  
I'll keep you updated :)

---
title: Warning - this post contains graphic content
date: 2009-07-09
tags:
  - evernote
---
  
I heard about GraphViz ( [ http://www.graphviz.org ](http://www.graphviz.org)
) via an interview with the author of InstaViz ( [ http://instaviz.com
](http://instaviz.com) ) and spent a little time this morning learning about
how GraphViz works and how to use it.

If you haven't already, check it out (and if you have a Mac, check out the
award-winning OSX version at [ http://www.pixelglow.com
](http://www.pixelglow.com) ). When you're freed from the need to think about
layout, alot of potential applications for charts like this open up, and in my
case this tool could play a significant role in a Google Wave (http://
wave.google.com) project I have been noodling on.  

---
title: Web Development Task Status Types
date: 2011-07-02
tags:
  - evernote
---
I'm working up a list of status types for the tasks of a web app project.
I've seen so many variations on this but they seem to be either too vague or
too numerous.  
  
  

I decided to write out what I felt mattered when I was on the receiving end of
such a list and here's what I came up with:

  
  
  
  

  

  

  

  1. There’s nothing there 
  

  2. There’s something there 
  

  3. It works 
  

  4. It looks good 
  

  5. It’s ready for testing 
  

  

  

  
  

I'm happy with the meaning here, but it's a bit unprofessional.  If you can
suggest a replacement for one of these that I like, I'll send you a coupon
code for a free copy of one of my iOS applications (just look under "Products"
on [ http://www.gullicksonlaboratories.com/
](http://www.gullicksonlaboratories.com/) ).

---
title: Week Without Records - End
date: 2011-07-12
tags:
  - evernote
---
The first record I listened to on Sunday was the Beastie Boy's " [ Paul's
Boutique ](http://en.wikipedia.org/wiki/Paul's_Boutique) ".  

  

![](http://upload.wikimedia.org/wikipedia/en/0/07/BeastieBoysPaul%27sBoutique.jpg)

  
  
  
  

I spent a lot of time during [ WWR ](http://jasongullickson.posterous.com/a
-week-without-records) thinking about what I'd listen to first when the week
was over.  On one hand I thought of the greatest recordings of all time, but
many of these are recordings of performances which pale in comparison to the
real thing.  On the other hand I thought of listening to music that cannot be
performed live, as this is something that you could never hear in a world
without records.

  
  

In the end I selected Paul's Boutique unconsciously.  I was sitting on the
floor playing with my daughter when I became aware that the song " [ 3-Minute
Rule ](http://en.wikipedia.org/wiki/3-Minute_Rule) " was playing in my head.
I sat with this for a minute and thought about how amazing that album was,
both in it's basic musical content and the place it holds in my mind, and I
really wanted to hear the rest of the record.  I had gotten so used to not
being able to turn on music that it wasn't immediately apparent to me that I
could listen to it now that the week was over.  Once I realized this I queued
up the album and drank it in.  It was almost like listening to it for the
first time.

  
  

Over the course of the week I had a lot of realizations about music,
recordings and the industry that has sprung-up around them in the last century
or so.  I'd like to distill these ideas into something I could share with you
that would communicate the insight I gained through the experience.  However,
like any other situation that you have lived through it is hard to isolate the
intellectual ideas from the visceral experience and I'm finding it very
difficult to communicate these ideas without the context of the emotional
state-of-mind that comes along with abstaining from recordings for a week.  If
I'm ever successful at this, I'll let you know, but until then I recommend
experiencing a [ Week Without Records ](http://jasongullickson.posterous.com/a
-week-without-records) for yourself and then perhaps we can discuss these
topics over the backdrop of shared experience.

  
  

See you at WWR 2012!

  

---
title: We have a winnah!
date: 2000-12-14
tags:
  - evernote
---
The results are in, and we have a new mascot, let's hear a round of applause
for SUPER DAVE!  
  
I'll be creating new merchandise using our new mascot in the next week or so,
and I'll let you all know when it's ready.  
  
Thanks for voting, and no, there will be no need for a recount.

---
title: Welcome to www.jasongullickson.com!
date: 2000-10-04
tags:
  - evernote
---
Here at jasongullickson.com our goal is to provide you with a weekly
dissertation on some interesting topic. Failing that, we hope to provide you
with at least an insightfull (or maybe mispelled) rant that you can use as
your own to impress your friends.  
  
In addition to the usual banter, we offer our exclusive line of do-dads and
such in the merchandise section for your shopping pleasure, and an ever-
increasing list of interesting spots on the web listed on your left.  
  
Each week you'll get a dose of mayhem or madness from yours truely, or
sometimes from a guest speaker, or if you're really motivated, email me Jason
J. Gullickson and I'll tell you about our associate program and you might get
to go-off yourself.  
  
This week I'm planning on putting something down about the presidential
debates, however right about now I'm a little too incoherent to even get
started. In the meantime, go to www.votenader.com and see what other options
the debate commitee didn't bother to tell you about.

---
title: What's that smell?
date: 2008-11-10
tags:
  - evernote
---
Last weekend when I opened the garage door I was hit with the beautiful smell
of gasoline. After the initial pleasure of the aroma subsided, a more serious
thought entered my head:  
  
"Why does the garage smell like gas?"  
  
Of course my first suspect is the CL. I take a look under the tarp and sure
enough, there is a spot under the center stand that smells of fuel. Only a
stain remains, but there was definitely some sort of leaking going on.  
  
I'm surprised by this because I tested the tank repeatedly after [ lining it
](http://jasonscrudrundiary.blogspot.com/2008/08/sunshine-of-my-hate-
part-1.html) and found no leaks. I even replaced the petcock with a NOS one,
and it has been turned off ever since I [ parked
](http://jasonscrudrundiary.blogspot.com/2008/10/208-days.html) the bike for
the winter.  
  
I took a close look at the lines, the carbs and anywhere else that fuel flows
and found nothing. Of course the liquid fuel evaporated before I became aware
of the leak, but I was hoping to find some tell-tale residue or something
along those lines. Nothing.  
  
So I moved the bike to a more level spot and kept an eye on it. Everything
seemed fine for a few days, then one morning, another spot.  
  
Now when this happens, it's not a lot of fuel, the spot is only about a foot
around and there is plenty of fuel left in the tank. My only guess is that it
has something to do with the +-30 degree temperature swings we're experiencing
around here lately but I still can't find the source and I'd like to address
this before the snow flies and I go a week or more without opening the door.

---
title: When N.A.S.A. closes a door...
date: 2012-02-22
tags:
  - evernote
---
... DIY opens a window.  
  
  
  
[ http://shop.oreilly.com/product/0636920021605.do
](http://shop.oreilly.com/product/0636920021605.do)  
  
  
  
I started reading this a few weeks ago and it's already inspired a new
project. I even have selected a primary and secondary mission. Stay tuned...

---
title: Where does he get those wonderful toys?
date: 2001-02-20
tags:
  - evernote
---
Here at jasongullickson.com we strive to provide you with sometimes useless,
but usually entertaining toys to kill some time with. In our relentless
attempt to give you something new each week, we present this weeks toy:  
  
Hate Mail!  
  
I'm not going to claim that the hate mail tool is anything I invented, and in
this version it resembles some other apps out on the net that do simular
things, however future versions will provide much neater (nastier?)
functionality. For now, drop in a message and take it for a spin, I will
hopefully adding to it's "vocabulary" often, so come back and try it again for
even more hyjinx.  
  
In addition to the new toy, I'd like to take this chance to annouce my
candidancy for President in 2012 under the Eisenhower party (don't worry,
we'll get the spelling right). You'll see more on this later, but for now just
an annoucement to begin the longest presidential campaign in history (ok, well
the longest next to this guy anyway...).  
  
Enjoy the toy, and we'll see you next week for something completely
different...

---
title: Whoa, V-Twin Enfield?
date: 2009-05-07
tags:
  - evernote
---
  
Royal Enfield fans, check this out: [
http://thekneeslider.com/archives/2009/04/09/royal-enfield-v-twin-the-musket/
](http://thekneeslider.com/archives/2009/04/09/royal-enfield-v-twin-the-
musket/)  

---
title: Why not
date: 2013-09-29
tags:
  - evernote
---
Ran across [ Postach.io ](http://postach.io) on the [ Evernote Tech Blog
](http://blog.evernote.com/tech/) and thought I'd give it a try.  I'm a
reluctantly lover of [ Evernote ](http://evernote.com/) and I have an affinity
for simple blogging platforms (still lamenting the death of [ ﻿  Posterous
](http://www.posterous.com) ).

  

We'll see what happens.

---
title: Writing copy for the new iPhone app
date: 2009-06-08
tags:
  - evernote
---
  
Usually this is my least favorite part of the process but for the first time I
have something that doesn't require specialized knowledge or boutique
interests to understand.

I shipped the first draft of my iTunes store copy off to my beta testers for
feedback so we'll see how that goes.

I'm getting excited about submitting this one and hopefully I'm not
overlooking some "hey there's no crashed car in the intersection"-sort of
problem. I'm totally anticipating approval problems only because I've been
hearing about more of them lately and I've even had a small taste of that
process myself with a previous app.

But for the moment all is well, it's been almost 24 hours since I posted RC1
and I've yet to receive a new bug report. I noticed one item that I'd like to
address but it's by no means a showstopper. If we make it through the next 24
w/o incident, I plan on damning the torpedos and submitting the app...

...unless I get this one last thing in...  

---
title: WTF?
date: 2000-12-30
tags:
  - evernote
---
Yeah, I'm taking my time getting the new mascot, merchandise, etc, but then
again I'm not so sure anyone is overly-anxious for this stuff anyway.  
  
I watched _Flatliners_ again the other night (which if you haven't seen it,
it's about some med students experimenting with coming back from the dead),
and it got me thinking about some work I had done when I was young analysing
bio-electrical signals, such as EKG, EEG, etc, so I decided to surf around a
bit and see what info I could find on this stuff, EEG in particular.  
  
I had always wanted to build an EEG of some sort; the idea of capturing
brainwave data and manipulating it in software was always intrguing to me.
After some searching, I came across [ www.brainmaster.com
](http://www.brainmaster.com) , where I found exactly what I was looking for.  
  
The brainmaster is a box that you hook up to your serial port on one end, and
your head on the other, and it amplifies and digitizes brainwaves and sends
them to your computer. Some of the applications for this are biofeedback,
treating ADD, etc. In addition to more typical applications, something like
this could be used to send brain activity data to an application that maybe
interprets them and controls software, or hardware, or a coffee machine.  
  
So, when I get back (I'm leaving for awhile), I'm going to start down the road
of building one of these boxes, and I'll report my findings here, along with
detailed info on the projects page.  
  
Oh yeah, merry xmas and happy new year, etc.

---
title: WWR Day 2
date: 2011-07-05
tags:
  - evernote
---
Today was tougher, my daughter insisted on playing a song for me and I tried
to convince her to sing it instead( unsuccessfully).  
  
  
  
Also Professor Elemental released a new video for his song "Splendid" today as
well, which I had to abstain from.  
  
  
  
I expect tomorrow to be even more difficult as I return to work and will have
to spend some time coding with only the soothing sounds of the computer fans.  
  
  
  
Talk to you tomorrow.

---
title: WWR day 3 & 4
date: 2011-07-07
tags:
  - evernote
---
This is turning out to be harder than I thought.  
  
  
  
The "bad song stuck in your head" problem has reached titanic proportions, I
had "Fireworks" (what I know of the song, which isn't much) on loop for almost
six hours today.  
  
  
  
My only relief was the concert at Swan Park tonight, which was good but over
too quickly.  
  
  
  
I have found myself feeling more musically creative than usual, coming up with
little beats and melodies in my head; but I haven't had any time to put them
down so it is all transient.  
  
  
  
I'm really looking forward to Sunday, I'm making a list of what I'm going to
listen to already.

---
title: WWR Day One
date: 2011-07-04
tags:
  - evernote
---
Well the Week Without Records (WWR) is starting out easier than expected. Of
course avoiding the deliberate listing to recorded music was fairly easy but I
was more concerned about how difficult it would be to avoid recorded music
embedded in other media (movies, etc.). I did catch myself once watching a
film with my wife and daughter which brings up one of the difficulties when
"fasting" from any shared activity.  
  
  
  
I haven't sought out any live music thus far but I'm hoping to encounter
something tomorrow. I've also already started brewing some ideas of creating
music of my own to at least flush the unwanted music from my head.  
  
  
  
Yes, the worst part of the experience so far has been getting something
annoying stuck in my head and having no easy way to replace it with something
desirable.  
  
  
  
I'll check-in again tomorrow.

---
title: Eternity
date: Fri, 16 May 2014 18:44:52 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Something I think about often is how to make information available forever.

  

A good example of my struggles with this is my blog.  I know there's stuff
I've posted over the last decade or two that might be of use to others, but
even keeping that published over that relatively short amount of time has been
challenging (as you can see here).

  

  

\- Jason

---
title: exception test 1
date: Sun, 15 Dec 2013 15:54:12 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
this is test 1 of exception system  

  

\--

---
title: External Config test 1
date: Sat, 14 Dec 2013 12:45:49 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Just testing externalization of config parameters (if nothing else makes doing
upgrades easier).

---
title: "Failure"
date: 2018-11-14T15:00:00-06:00 
draft: false
tags:
  - philosophy
---

There is no failure: you either give-up, or you prevail.

A discrete *task* can fail, but this is no more relevant to the overall mission than breaking a shoelace is to a shoe, or misplaced keys are to a car.  It's a temporary setback; it's still a shoe, it's still a car.

The exception of course is death.

However, this only applies if you go it alone, and underscores one of the most overlooked factors in continuous success: cooperation.

I grew up with a lot of role models I perceived as individualists: Thomas Edison, Nicola Tesla, Seymour Cray, Steve Jobs, etc.   I've always been happiest working alone than with others (with rare exceptions).  This has put a ceiling on the scope of my work, which I've learned to work within, and I'm sure I'm not alone in working this way.

But what I've overlooked is that it also makes my work *vulnerable*.

Without involving others, failure is imminent and unavoidable for any project that isn't completed before my time is up.  This means I have an ever decreasing amount of time to finish all of my work, and it also makes it fairly simple for someone to instantly stop my ideas from coming to fruition.

*Hmm...  this started out as something less personal but seems to have turned into being about me.  I'm going to guess that I'm not the only person working on this problem though.*

So I've come to terms with the fact that I'm going to need to resolve these two things: the perceived efficiency of working alone vs. the durability of working with others.  I don't know how I'm going to resolve this, but I'm going to start by looking for clues in the few enjoyable collaborations I've had in my life.  

I've made several attempts at this, but I've never found way to reliably recreate the experience of working together that feels natural.  The exception to this has been the projects I work on with Jamie.

...and maybe that's where the answer lies?  

The key may not be in the collaboration itself, but in working with people for whom I have a level of **trust**, **respect** and **love** that allows all fear and ego to be set-aside, and the work to flow naturally without and social or psychological constraints.

*  **Trust**, in the fact that no one on the team will deliberately harm one another, and that everyone is doing the best work they can.
*  **Respect**, the knowledge that everyone's work is valued equally and there will be no contempt for role in the project.
*  **Love**, for the joy of doing the work, and valuing one-another more than the the result of it.   

This may be a good place to start.  I don't always have control over who I work with, but when I do I'll make an effort to choose collaborators who fit the description above and see what happens.
---
title: Fire good 2
date: Wed, 25 Jun 2014 15:34:45 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/250-image.jpeg) ](assets/250-image.jpeg)

---
title: Fire good 3
date: Wed, 25 Jun 2014 15:36:11 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This fire is good!

[ ![](/preposterous/assets/251-image.jpeg) ](assets/251-image.jpeg)

---
title: Fire good 4
date: Wed, 25 Jun 2014 15:37:46 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
![image.jpeg](/preposterous/assets/252-image.jpeg)

  

This **fire** is _good_ !!

---
title: Fire good
date: Wed, 25 Jun 2014 15:33:56 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/249-photo.jpg) ](assets/249-photo.jpg)

---
title: First OffGRiD part rolls off the printer
date: Mon, 17 Aug 2015 19:33:01 -0500
author: jjg
draft: false
tags:
  - preposterous
---
![image1.JPG](/preposterous/assets/53-image1.jpg)  
  
Still working through the design of the parts but wanted to run some off just
to get a feel for size and fit.

  

More info and detailed updates on the **wiki** : [
http://wiki.2soc.net/doku.php?id=offgrid
](http://wiki.2soc.net/doku.php?id=offgrid)

  

  
//  jjg

---
title: Fix notifications test 1
date: Fri, 13 Dec 2013 23:20:09 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Seems like notification suppression might have broken notifications.  This is
the first test toward fixing them.

---
title: Fix notifications test 2
date: Fri, 13 Dec 2013 23:22:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Test to see if it worked in production.

---
title: Fix title production test 1
date: Sat, 14 Dec 2013 13:16:56 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Production test of fix for missing document title bug.

  

Also, better html.

---
title: Fix title test 1
date: Sat, 14 Dec 2013 13:14:18 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This should set the html title as the title of the post.

  

Also, a little closer to producing valid html documents.

  

  

\- Jason

---
title: Flixstarter
date: Sat, 1 Mar 2014 07:26:43 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
1\.  Watch Trailers

2\.  Buy tickets (or DVD's, downloads, etc.)

3\.  When films make their budget, they get made and you watch them

  

Kickstarter, Indiegogo and other crowd funding sources have been used to raise
money for many filmmaking projects, but none of them are designed to be a
great way to bring films and audiences together.  Flixstarter is designed to
connect movies and moviegoers in the the most natural way possible, through
the experience of watching trailers together in a darkened theater.

  

The Flixstarter experience for moviegoers is as minimalistic as possible, with
an extreme focus on presenting film trailers.  No ancillary information is
visible during the presentation, and only the minimum necessary film
"metadata" is displayed elsewhere on the site.  This allows audiences and
filmmakers alike to focus on the one thing that matters to them both, the film
itself.

  

Modern film promotion involves enormous amounts of marketing materials, press
kits, advertising, product placement, merchandise, etc.  Producing this
material is an enormous amount of work and expense for the filmmaker, and
consuming it provides the moviegoer with less of a chance of seeing a film
they will love than simply being aware of the films who have spent the most
time and money on promotion.

  

By experiencing each films trailer in a focused, distraction-free way,
potential audience members get a chance to dedicate their attention to the
film, notice details and connect directly with the work itself.

  

By pouring all their resources into producing the trailer, filmmakers can
focus exclusively on the one thing their audience needs from them: a great
film.

  

The same skill, talent and creativity are needed to produce a great trailer as
are needed to produce a great feature film, so if a filmmaker can engage an
audience in the short form, doing so in the long form is just a matter of
scaling-up their resources.

  

So how does it _actually work?_

  

**For Audiences**

1\.  Watch trailers

2\.  When each trailer is over, you can choose to buy tickets, downloads, etc.
or move on to the next trailer

3\.  When the completed film is released, you receive your purchase

  

**For Filmmakers**

1\.  Produce a trailer (or trailers) that honestly engage people who will love
your film

2\.  Determine a production budget

3\.  Submit your trailers and budget to Flixstarter

4\.  When you've sold enough tickets, etc. to meet your budget, begin
production

5,  When production is complete, distribute your film to your audience

  

As a system, Flixstarter's primary design goal is to require as little as
possible from all users, without the distraction of rewards, perks, updates
and biographical data.  Filmmakers can focus on making great films and
audiences can relax in the dark and preview glimpses of their next favorite
movie.

  

A preliminary beta version of the Flixstarter system should take between 2-4
weeks for a motivated development team consisting of 2-3 developers with a mix
of skills including API design, graphic design, UX/UI design, media data
manipulation and processing, web server programming, web client/browser
programming and consumer electronics device (set-top boxes, etc.) integration
experience.  Initial beta release would provide responsive desktop and mobile
web interfaces along with audience/viewer support for Roku streaming devices.

  

Pre-launch a selection of at least 100 film projects will need to be collected
which include at least one trailer for each film.  The selection should
include a broad range of genres, styles and subject matter and a production
budget range that is proportionate to the pre-launch audience number (in other
words, 50% of the first 100 films should be possible to fund if 100% of the
beta launch audience buys tickets).

  

Pre-launch beta invites should gather 500 beta users to experience the initial
film selection via web and Roku interfaces.  Consider involving a commitment
to purchase tickets for a fixed number of films as a condition of the
invitation (using personal or provided funds).

  

  

  

  

  

  

---
title: Formatting issues and iOS mail
date: Sun, 2 Aug 2015 06:16:01 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I _thought_ I had this problem licked, but based on the way my last post
looks, I had not.

  

There is an issue with the way the built-in iOS email client sends messages
that causes [ Preposter.us ](http://Preposter.us) to treat them like HTML
formatted messages even when there's no HTML formatting.  The result is a
giant block of text.

  

I've created a Github issue ( [ https://github.com/jjg/preposter.us/issues/56
](https://github.com/jjg/preposter.us/issues/56) ) to look into it again, but
in the meantime I'm experimenting with ways to work around it (I believe
including deliberate formatting in the message will do the trick).

  

  

\- Jason

---
title: Four is better than one
date: Fri, 13 Dec 2013 00:09:47 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
The band **foreigner** released it's most popular album _"Four"_ in 1972 which
included the hit single "Jukebox Hero".

  

Maybe someday Preposterous will be able to handle multimedia and I can embed a
copy of the song?  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: "From Preposterous to Hugo"
date: 2018-12-27T08:21:28-06:00
draft: false 
tags: 
  - hugo
  - preposterous 
---

Migrating content from [Preposterous](https://gitlab.com/jgullickson/preposter.us) to Hugo was easier than I expected.  You might think that since I created Preposterous I'd expect it to be easy, but it's been years since I looked at the code and since I never intended to make the content exportable, I thought it would be more complicated.

As it turns out I did myself a huge favor and used JSON files to store the content and rendered the HTML client-side when each post was loaded!

I also made things easier by creating a JSON index for each blog, which was good because I didn't capture things like the post date in the individual post files (duh!).

After poking-around a bit I was able to write a little Python script that did most of the migrating automatically:

```python

import json
from bs4 import BeautifulSoup
import html2text
import codecs

# open the posts file
with open('./input/posts.json') as posts_file:
    
    # convert post index json to python dictionary
    posts_data = json.load(posts_file)
    
    # loop through all posts
    for post in posts_data['posts']:
        selected_post = post['post']
        
        # DEBUG: limit to one post for image testing
        #if selected_post['slug'] == 'scaled-image-test':

        # translate to Hugo metadata
        # preposterous post properties:
        # * date
        # * url
        # * title
        # * slug
        # * author

        output = '---\n'
        output = output + 'title: ' + selected_post['title'] + '\n'
        output = output + 'date: ' + selected_post['date'] + '\n'
        output = output + 'author: ' + selected_post['author'] + '\n'
        output = output + 'draft: false\n'
        output = output + 'tags:\n'
        output = output + '  - preposterous\n'
        output = output + '---\n'

        # load refereneced HTML file
        post_html_filename = './input/' + selected_post['url'].split('/')[-1]

        # parse HTML into python object
        with open(post_html_filename) as post_html_data:
            post_html = BeautifulSoup(post_html_data,"lxml")

            # identify and copy images 
            image_tags = post_html.findAll('img')
            for image_tag in image_tags:
                image_tag['src'] = '/preposterous/' + image_tag['src']

            # TODO: handle other assets (video, audio, etc.)

            # reformat as markdown
            html = post_html.find('div', class_='content').prettify()
            markdown = html2text.html2text(html)

            output = output + markdown

            print('---output-----------------------------------------')
            print(output)
            print('--------------------------------------------------')

            # write the result to a properly-named file
            file = codecs.open('./output/' + selected_post['slug'] + '.md', 'w', encoding='utf8')
            file.write(output)
            file.close()

```

There were a few posts whose titles didn't format properly, but with a little manual massaging I was able to get Hugo to accept all of the content the script generated.  One problem I haven't solved is that the date on some posts are mis-interpretted by Hugo as year `0001`.  I'm not sure why this is, and I'd like to fix it, but after looking at the output for an hour I gave up.  Maybe fresh eyes in the future will figure it out?
---
title: "From Wordpress to Hugo"
date: 2018-12-27T07:42:48-06:00
draft: false 
tags: 
  - hugo
  - wordpress 
---

I couldn't find a one-stop tool for migrating my existing WordPress.com blog to Hugo, so here's what I did instead:

* Export the posts (XML) and media from WordPress (My Sites -> Gullickson Laboratories -> Settings -> Export -> Export your content/Export media library)
* Use [wp2md](https://github.com/dreikanter/wp2md) to convert XML to Markdown files
* Use [sed](https://linux.die.net/man/1/sed) to format metadata, re-work URL's, etc. 
* Extract media from `.tar` archive and copy it into a subdirectory under `static`

Here's the specific comamnds I used to transform the content.  If you use these you'll need to modify some of the parameters to match your source blog info, destination paths, etc.

1. `wp2md -ef "%Y-%m-%d %H:%M:%S" -ps gl-{title}.md -d ./md-output/ your.wordpress.export.xml` (exports all content to the `md-output` directory, prepends each file with `gl-` and uses a date format that Hugo likes)
2. `sed -i "1i ---" gl*` (marks the opening of the Hugo metadata section)
3. `sed -i "13i ---" gl*` (marks the closing of the metadata section)
4. `sed -i "2i tags:" gl*` (adds a tags section)
5. `sed -i "3i\  - gullicksonlaboratories" gl*` (tags the content)
6. `rpl "created:" "date:" gl*` (renames the created field to something Hugo understands)
7. `rpl "https://jjg2soc.files.wordpress.com/" "/wp/" gl*` (updates media URL's to use local files instead of the originals)
8. `cp gl* /path/to/hugo/content/posts` (copies all exported content into the Hugo site)

This doesn't fix everything.  The most notable problem is that links to pages within the blog will still point to the original server.  Another issue (at least in my case) is that there is a header with the title of the post inserted into the Markdown (which results in the title being displayed twice on the rendered page).  I think both of these issues can be addressed with a little more tweaking but neither was bad enough to stop me from publishing the site as-is.

One last thing I'll mention is that the WordPress XML file can contain some things that `wp2md` considers invalid (they may in fact contain invalid XML).  In my case the command fails and displays the offending line number, and it was just a matter of deleting the bad input from the XML file (mine were all messages related to failed attempts to share the post on Facebook).  Once these are cleaned-out the command should complete sucessfully.

---
title: Garage VR
date: Sun, 20 Sep 2015 21:55:16 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Here's what I think is next:

  

Commodity head-mounted-displays built from desktop-fabbed parts and powered by
burner handsets.  Completely self-contained, portable, cheap and networked.

  

Standard input devices are the sensors embedded in the handsets, but of course
these can be augmented with inexpensive Bluetooth keyboards, game controllers
or any number of homebrew/DIY devices.

  

These headsets can communicate with each other via the Internet, but they can
communicate directly as well, building local mesh networks that are functional
and useful even without constant connectivity to the public Internet.

  

The underlying network protocol will be TCP/IP (preferring IPv6 over IPv4),
but the application-layer protocol will be something new.  The Protocol will
be designed around conveying presence and position in 3-dimensional space in a
way that is compatible not only with operating an avatar in a virtual world,
but any other form of telepresence, including physical forms such as rovers,
drones and anthropomorphic androids.

  

The software side of this is a self-hosting virtual operating system, built
from a collection of existing tech or perhaps something new from scratch.
Either way, the key feature of it is that everything inside of it is created
from _inside_ the virtual world; not some "Dev kit" or IDE that runs on a
"normal" computer and requires people to be game programmers to create things
for inside the virtual world.

  

Rendering is deliberately kept simple and low-fi to ensure an immersive
experience, and de-emphasize the need for faster and more expensive hardware.

  

This virtual environment can also interface with existing Internet technology
like the Web, by rendering these things literally (as they would appear on a
screen) or in more abstract ways that are more appropriate for an immersive
environment.

  

Everything described here can be build with existing technology, and with the
proper implementation, it can be made inexpensive enough to be accessible to
almost anybody.  The critical path is development of The Protocol, and of the
Authoring Tools that allow this virtual world to be build from the inside out,
instead of from the outside in as it has been done in the past.

  

I'll be revisiting several of my own projects from the past which are
precursors to this vision.  If you're interested in participating in or
supporting this work, get in touch.

  

  
  
// jjg ( aka @jasonbot2000 )

---
title: GENA Progress
date: Fri, 21 Aug 2015 16:00:43 -0500
author: jjg
draft: false
tags:
  - preposterous
---
A little good news about the GENA device.

I was able to figure out why it would connect and then disconnect from my
phone.  Turns out that the problem was having the bluetooth devices from my
Pebble associated with the phone was somehow interfering, and when I told the
phone to "forget" the various Pebble devices (one watch has more than one), I
was able to pair the GENA with my phone!

This was encouraging enough that I spent some more time working on a case.
It's moving along, it's ugly, but I want to get it working right before I
worry about how it looks.

![IMG_2239.JPG](/preposterous/assets/57-img_2239.jpg)

The buttons work although refinement is needed (as you can see one broke off
during testing). Once I've got a button mechanism that is satisfactory the
next step will be a bezel (perhaps simply inverting the design so the back is
the open part?) and then attaching a strap.  There's some cool printable
straps out there, so I'll probably start there.

Once it's wearable I'll start to work on making it more attractive and
comfortable, but at that point I think it could replace my Pebble on a daily
basis.  I'm very excited about the forthcoming developer tools &
documentation, be even as-is it's a very cool and useful piece of hardware.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: GENA watch getting less ugly
date: Sat, 22 Aug 2015 21:34:41 -0500
author: jjg
draft: false
tags:
  - preposterous
---
  
  
![](/preposterous/assets/58-img_2254.jpg)

  
One more revision or two and all it will _need_ is a strap!

  

  
//  jjg

---
title: Get it together
date: Thu, 20 Aug 2015 22:53:57 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Since it may be awhile before I have the non-printed components for OffGRiD
took a run at modeling the basic dimensions of the parts and fitting them
together with the stuff I've designed so far.

  

![image1.PNG](/preposterous/assets/56-image1.png)  
  
Obviously there's a **lot** more to do, but once I have placeholders for the
rest of the parts I should be able to run off some more accurate printable
parts.

  

  
//  jjg

---
title: GID with 3D Printing - Hallway Lamp
date: Fri, 26 Dec 2014 06:27:06 -0600
author: jjg
draft: false
tags:
  - preposterous
---
After replacing the tiny, strange and _very inefficient_ bulbs in our hallway
lights upstairs once again we decided to find a replacement that could work
with common CF or LED bulbs.

  

One of the problems with old houses is that they don't always have modern
junction boxes (in our case, never), and most new fixtures, etc. expect this.
In this case it means that the tiny little base of the new lamp can't possibly
hold all the wiring involved.

  

There is probably an existing, off-the-shelf solution to this but I haven't
seen one that I love, and even if there was, they often require on-site
customization that ends up looking less than awesome (at least when I do it).

  

Of course once you've built a 3D printer, every problem looks like a modeling
problem, so instead of wasting time looking for another thing to buy to solve
the problem I took a few measurements and fired up **OpenSCAD** .

  

![image1.JPG](/preposterous/assets/9-image1.jpg)  

  

It's always tempting to do something fancy when you're designing parts to be
3D printed, as "complexity is cheap" but all I really needed was a tube of the
right diameter and height so other than adding a little lip to keep things
aligned I left the scroll-work and gargoyles out of this version.  Also with a
limited amount of Black PLA on-hand and with a 3+ hour print time I didn't
want to take unnecessary chances.

  

At 125mm diameter this is one of the widest prints I've attempted in a long
time.  Printing anything larger than 100mm has been a challenge with this
printer so this job would be a good test of the recent upgrades and
improvements I've made.

  

![image2.JPG](/preposterous/assets/9-image2.jpg)  

  

Fortunately, other than some "wandering spool" problems the print completed
without a hitch and the part came out fine.  I'm not sure if that's due to
better calibration on my part or perhaps the new gluestick I'm using to
prepare the print bed, but in either case I'm planning to try some more
ambitious prints to see just how far I can push the printer now dimension-
wise.

  

![image3.JPG](/preposterous/assets/9-image3.jpg)  

  

While the base was printing I realized that I'd also need one of those little
"hook" things to feed the cord through so the lamp could hang from a specific
part of the ceiling (not directly under where the previous lamp had been).
Here again this is probably something you can find off-the-shelf, but with 10
minutes of CAD time and about 15 minutes of print time (and a few pennies of
plastic) I was able to have one in-hand and ready to use without wasting time
and fuel driving around town.

  

![image6.JPG](/preposterous/assets/9-image6.jpg)  

  

With these parts in hand I was able to finish the job that afternoon without
having to make any more "parts runs", and minimal additional cost besides my
time.  You could argue that the time cost a lot more than the money it would
have cost to purchase these parts, but I don't think it would have taken a lot
less time to find them and purchase them, and the time I spent modeling and
printing them not only got the job done, but it made me a better modeler and
designer, and taught me new things about the printer as well.

  

It's this last part that is really the redeeming value of DIY in my eyes.
When you do something yourself it may not be less expensive in strict
financial terms, but the time and money spent is spent making you smarter, and
less dependent.  I think this is good for both you and the world you
participate in, especially if you then share your experience so others can
learn from it as well.

  

![image5.JPG](/preposterous/assets/9-image5.jpg)  

  

All of the design files for this project are open-source and available for
download here:

  

[ http://github.com/jjg/lamp-base ](http://github.com/jjg/lamp-base)

  

  

\- Jason

---
tags:
  - gullicksonlaboratories
  - rain
title: 2018
link: http://jjg.2soc.net/2018/01/01/2018/
author: jgullickson
description: 
post_id: 5055
date: 2018-01-01 23:40:18
created_gmt: 2018-01-02 05:40:18
comment_status: open
post_name: 2018
status: publish
post_type: post
---

# 2018

A few weeks ago I posted on Mastodon that I've never built anything I felt was the best work I could have done (it may have been more specific to programming, but you get the idea). I've done work that is good and that I've been proud of, but nothing I would consider "perfect". Each time there have been perfectly rational reasons to do less than I was capable of. Meeting a deadline, coming in under-budget or other competing needs often justify stopping short of giving your all. But it made me wonder what it would be like to make something with a singular focus of doing the best job I could possibly do. This might sound silly but this is a remarkably hard thing for me to do. At any given time I have several projects in-progress, and for the most part what I would consider to be "average" work is enough to meet or exceed expectations. From this perspective, it's almost wasteful to put everything I have into a project. Another challenge is finding a project that can keep me engaged enough to demand perfection. Many projects (even the more ambitious ones) become less interesting once I have a viable solution for the hard problems (even if I haven't implemented the solution yet). For these reasons and others I've never had the experience of completing a project that wasn't a compromise. That's not to say I haven't been happy with the results (or that the results haven't pleaded others), but uncompromising perfection is not something I feel I've accomplished yet. Having realized this, I've decided the time has come to change that. In 2018 I'm going to complete a project in such a way that all of the work I've done is of the highest quality I am capable of. I will refuse to compromise any aspect of the project for sake of time, budget or other justification. When the project is complete, it will represent the best of my abilities and the tools, technology and resources available to me. When I survey the projects I've been working on one stands out as a candidate for this endeavor: Raiden. Raiden Mark II is both narrow enough to demand the most from my strongest skill sets while complex enough to sustain my interest. It also represents a field (High Performance Computing) which has been central to both my personal and professional work for most my life. I also think that this project is one of the most valuable in my catalog in terms of solving some of the greatest problems I see in my field. Building Raiden Mark II to these standards is ambitious, but also within my capabilities. Unlike other projects, there are no external dependencies likely to force compromise. If I fail to complete this project to the best of my abilities, I will have only myself to blame. This means the project may take longer than I would like it to, and it means delaying some of the things I am looking forward to in future iterations of the system. But I believe this is necessary in order to test what I am capable of and to prove whether or not my best is good enough to capture the interest of others whose contributions will be essential to successfully complete future stages of the project. Here's to 2018!
---
tags:
  - gullicksonlaboratories
title: Amazon Basics Keyboard & Mouse
link: http://jjg.2soc.net/2017/07/23/amazon-basics-keyboard-mouse/
author: jgullickson
description: 
post_id: 4140
date: 2017-07-23 12:51:35
created_gmt: 2017-07-23 17:51:35
comment_status: open
post_name: amazon-basics-keyboard-mouse
status: publish
post_type: post
---

# Amazon Basics Keyboard & Mouse

So let’s see how much [keyboard](http://amzn.to/2gVuS6p) you get for $15. ![](/wp/2017/07/img_1217.jpg) It’s better than expected. It works when plugged-in, none of the keys are broken and it actually has a little physical and auditory feedback. So far so good. The Enter key is in the right place and is the right shape. The Backspace key is correct as well. I can type with reasonable speed and accuracy (within the limits of my fingers). There’s a numeric keypad, proper function keys an none of the real keys have been replaced with “multimedia” functions and the like. Overall (after spending two minutes with the keyboard) I would say that it’s as good as any sub-$100 device I’ve used before, which gives it a pretty high quality-per-dollar ratio. Now it’s the mouse’s turn. Not much to say about the mouse other than it works as well as any inexpensive corded mouse I’ve used recently. The finish is kind of nice (has some sort of rubberized coating that feels good). There’s not much weight to it but that’s to be expected. Otherwise it seems adequate, clicks seem to happen as they should and while the buttons are a bit flimsy, they click and rebound as you’d expect. There’s nothing notable about the third button/scrollwheel which is a good thing (it could be a lot worse). Overall it’s fine, nothing to get excited about but also no major problems that were noticeable after about a minute of use. Whether or not the pair holds up to heavy use is another matter. If there's anything new to report, I'll follow-up with another review in a few months. A few additional notes: As I mentioned earlier the keyboard doesn’t have a bunch of “extra” keys (which I prefer), but it does have volume up, down and mute which work with no additional effort in Ubuntu Linux (completely unexpected). There is also a key on the numeric keypad with a little calculator icon on it. ![](/wp/2017/07/img_1218-1.jpg) When pressed (again, at least on Linux), the calculator application is launched and you can immediately begin keying in numbers and performing arithmetic. Maybe this is something all keyboards do now but as someone who often turns to the calculator I found this amazingly useful and I was very surprised that it worked out-of-the box with Linux. Until you're ready to spend [serious money on a keyboard](http://amzn.to/2eGlF1h),  I strongly recommend you give this combo a try.  Or if like me you need something inexpensive (and quiet) for a secondary computer, this combo is orders of magnitude better than similarly priced units from Logitech, Microsoft, etc. [Amazon Basics Wired Keyboard and Mouse](http://amzn.to/2gVuS6p), $14.99 at [Amazon.com](http://amzn.to/2gVuS6p).---
tags:
  - gullicksonlaboratories
title: An Identity Co-op
link: http://jjg.2soc.net/2018/01/23/an-identity-co-op/
author: jgullickson
description: 
post_id: 5064
date: 2018-01-23 08:07:16
created_gmt: 2018-01-23 14:07:16
comment_status: open
post_name: an-identity-co-op
status: publish
post_type: post
---

# An Identity Co-op

A [co-op](https://en.wikipedia.org/wiki/Cooperative) that operates an [identity provider](https://en.wikipedia.org/wiki/Identity_provider) and advocates for the use of this provider by the services used by the co-op's members. 

## Consumer Perspective

  * One way to [sign-on](https://en.wikipedia.org/wiki/Login) to all the sites you use (sometimes referred to as [single sign-on](https://en.wikipedia.org/wiki/Single_sign-on))
  * Member control over what personal information is collected by the co-op and what is shared with other services
  * The co-op works collectively with services used by it's members to lobby for and support adoption of co-op sign-on
  * Members pay a membership fee which is used to cover operational and marketing costs (software development, hosting, training and promotion of the service to others)

## Technical Perspective

  * The co-op is an [OpenID Connect](https://en.wikipedia.org/wiki/OpenID_Connect) Identity Provider ([IdP](https://en.wikipedia.org/wiki/Identity_provider)) (potentially supporting other protocols as well)
  * All software is [open-source](https://en.wikipedia.org/wiki/Open-source_model)
  * Given the value of compromising the service, [multi-factor authentication](https://en.wikipedia.org/wiki/Multi-factor_authentication) is required (other advanced authentication mechanisms will be considered)
  * Regular [penetration testing](https://en.wikipedia.org/wiki/Penetration_test) is performed by a third-party

## Why do this?

There are a number of good reasons to consolidate identity, but first let's define how we're using the word identity in this article: 

  * A name or handle you can be referred to with
  * A set of credentials that you can provide to prove you are this identity
Potentially, there are additional things about you which could be valuable to associate with an identity: 
  * A visual representation of yourself
  * Your reputation
  * Links to other resources
  * etc.
There are numerous existing systems which provide some of the features described above. There are also existing standards for interfacing between these identity providers and external systems. However, in almost all cases existing identity providers do so as a _secondary_ service. This creates a number of problems: 

  * If a user discards the primary service, their identity is discarded as well
  * Additional service features create additional security vulnerabilities
  * Identity is _derived_ from the primary service and may include information that is irrelevant/unnecessary for external systems (in extreme cases this may constitute a security or privacy violation)
  * These services typically provide identity authentication as means of [lock-in](https://en.wikipedia.org/wiki/Vendor_lock-in) to _reduce_ user freedom, not to enhance it
  * These services typically do not actively advocate for adoption of their identity service by external or third-party systems
  * In many cases these services are proprietary, closed-source and often privately-owned, limiting the ability for their security and scalabilty to be thoroughly assessed and in most cases eliminating any control the user has over how their personal information is used
In contrast to this, the Identity Co-op provides a single service: identity authentication. As such all efforts are focused on providing the highest-quality identity authentication service possible. Control over the system is held by members of the Co-Op. All operational details are available to members and all source code is published openly. The information the co-op will collect from its members is decided by the members themselves and this information is used exclusively to provide value as an identity service. Additionally, this information is only shared with external systems and third-party organizations when necessary to authenticate an identity. As a co-op exists to serve it's members and their community, the Identity Co-op not only provides the software and infrastructure to authenticate the identities of its members, but it also must advocate for the adoption of this service by the systems its members use. Additionally, co-ops have a responsibility to help other co-ops. The Identity Co-op is no different and will help create additional identity co-ops serving other communities. In this way the service can be distributed in the form of separate co-ops serving members with different needs while sharing knowledge, experience and tools. The utility of this identity may go beyond the applications discussed, but no assumptions are being made that this is the case nor is it necessary for the Identity co-op to be valuable. 

## References

  * https://en.wikipedia.org/wiki/OpenAthens
  * https://oauth.net/articles/authentication/
  * https://en.wikipedia.org/wiki/OpenID_Connect
  * https://coreos.com/blog/announcing-dex.html
  * http://wiki.openid.net/w/page/12995226/Run%20your%20own%20identity%20server
  * https://tools.ietf.org/html/rfc6749#section-4.1
  * http://openid.net/connect/faq/---
title: "Gullickson Laboratories Archive"
date: 2018-12-25T15:00:00-06:00
draft: false
tags:
  - archives
  - gullicksonlaboratories 
---

My first pass at importing posts from [Gullickson Laboratories](/tags/gullicksonlaboratories) is complete!

There's still some clean-up to do (in particular, self-referential links still point to the old site) but I didn't want to delay making the content live any longer as the WordPress site will go offline in about a month.

I'm still working on a good way to redirect external links to this content.  Right now I'm planning to simply point the old hostname [jjg.2soc.net](http://jjg.2soc.net) at this site, but if I can find a way to make the old links work more transparently (without a lot of plumbing) I'll try to get that in place before shutting down the existing site.
---
tags:
  - gullicksonlaboratories
title: Arms & Legs
link: http://jjg.2soc.net/2018/03/15/arms-legs/
author: jgullickson
description: 
post_id: 5124
date: 2018-03-15 20:23:36
created_gmt: 2018-03-16 01:23:36
comment_status: open
post_name: arms-legs
status: publish
post_type: post
---

# Arms & Legs

Technically, the [Clusterboard](https://jjg.2soc.net/2018/02/23/pine64-cluster-board/) fits inside the case I've been designing around, but it doesn't fit inside the "endcaps" so it can't be mounted directly to the steel of the case using the mounting holes on the board. ![IMG_0064](/wp/2018/03/img_0064.jpg) To address this, I sketched-up some adapters to "relocate" the mount points somewhere more appropriate. Since I'm still not 100% sure where everything will belong in the final configuration of the chassis, I came up with a more flexible way to mount the board: magnets! ![IMG_0050](/wp/2018/03/img_0050.jpg) ![IMG_0051](/wp/2018/03/img_0051.jpg) I also need to mount a single PINE A64 board to serve as the "front-end node" so I whipped-up a couple of magnetic mounts for this board as well. I wasn't able to find appropriate magnets locally so I had to wait for some to arrive from The Internet. In the meantime I switched-gears and worked on [writing a little software](https://jjg.2soc.net/2018/03/14/front-panel-software/) to drive the panel's display. [caption id="attachment_5127" align="aligncenter" width="2448"]![IMG_0062](/wp/2018/03/img_0062.jpg) This has never happened before...[/caption] When the magnets arrived I was stunned to see they fit perfectly on the first try. However I didn't have any glue on-hand that was right for the job. Since I was tired of waiting I thought about how I might modify the mounts to eliminate the need for glue. This turned-out to be easier than expected and after two iterations I had working, glueless mounting brackets. ![IMG_0063](/wp/2018/03/img_0063.jpg) ![IMG_0072](/wp/2018/03/img_0072.jpg) All-in-all they work pretty well.  There is some alignment problem keeping all four feet on the Clusterboard from engaging the inside of the case completely, but I think this will be strong enough to safely move on to the next step: stuffing everything inside the box. ![IMG_0066](/wp/2018/03/img_0066.jpg) ![IMG_0071](/wp/2018/03/img_0071.jpg)   ![IMG_0073](/wp/2018/03/img_0073.jpg) The idea of modifying a part just because you don't want to run out and buy some glue would seem ridiculous before I had a 3d printer but now it's easier and faster to just "run-off a new part". The result is not only faster, but it's also a better part. This is one of the things I love about 3d printing, the ability to iterate at a pace similar to writing software and letting the robots do the work. ![IMG_0070](/wp/2018/03/img_0070.jpg)---
title: Glass + Gluestick + PLA FTW!
date: Sat, 4 Jan 2014 14:01:04 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Normally I print on 3M Blue painters tape but I got a tip from the 3D Printing
Google+ group that instead of using tape, a little gluestick applied directly
to the print bed surface (in my case glass clamped to aluminum) works to get
PLA plastic to adhere to the surface.

  

![Inline image 1](/preposterous/assets/176-glueglass.jpg)  

  

So far so good.  Usually the first layer is a good indication of how a print
is going to go, but there's still a possibility of the print curling-up away
from the surface (especially if the nozzle and bed are not perfectly "tram",
and mine is not perfect), but at this point it's looking good.

  

The benefit of using this technique vs. Blue tape is that the tape needs to be
replaced frequently (it wears out, and gets damaged sometimes when removing a
print).  Additionally it's easier & faster to apply the glue than it is to get
the tape down perfectly flat.  Supposedly the glue also makes it easier to
remove the print from the bed (if worse comes to worst, the glue is water-
soluble).

  

This one's estimated to wrap-up in about 3 hours, so we'll see what happens
and I'll update the post when that time comes...

  

  

\- Jason

  

---
tags:
  - gullicksonlaboratories
title: ATtiny Microcontrollers (and how bigger isn't always better)
link: http://jjg.2soc.net/2017/03/12/attiny-microcontrollers-and-how-bigger-isnt-always-better/
author: jgullickson
description: 
post_id: 781
date: 2017-03-12 15:58:13
created_gmt: 2017-03-12 20:58:13
comment_status: open
post_name: attiny-microcontrollers-and-how-bigger-isnt-always-better
status: publish
post_type: post
---

# ATtiny Microcontrollers (and how bigger isn't always better)

I spent a little time today learning how to program the ATtiny13 microcontroller. I'm not sure how I ended up with a few of these chips (the ATtiny85 seems more popular), but they've been in my parts bin for awhile and I've never gotten around to playing with them until today. The first thing you notice about the Attiny is that it is, tiny. It's an entire computer in an 8-pin package. [caption id="attachment_795" align="aligncenter" width="475"]![onquarter](/wp/2017/03/onquarter.png) Is that not the cutest little computer you ever saw?[/caption] If you're not familiar with the AVR microcontrollers you might know them better as the heart and brain of the [Arduino](http://astore.amazon.com/jjg00-20/detail/B006H06TVG) (or at least, the original Arduinos). Feature-wise they are very similar to the larger chips used to power the Arduino, but with fewer I/O pins, less memory, etc. That said they are still amazingly capable devices which require very little (if any) additional circuitry to be useful. They are also very inexpensive, available for [$3 a piece in a pack of 5](http://astore.amazon.com/jjg00-20/detail/B015GP29AC) (or cheaper if you look hard enough). That's a lot of computer (as the [176-page datasheet](http://www.atmel.com/Images/doc2535.pdf) can attest) for the cost of a cup of coffee. 

## Hardware

You can use the Arduino IDE to write software these chips, but you'll need some additional hardware to connect them to your computer for programming. I'm using a [USBtinyISP](https://learn.adafruit.com/usbtinyisp/overview) I purchased from Adafrit many years ago, but you can use an [Arduino board](http://highlowtech.org/?p=1706) as well, if that's easier to get ahold of. There are also [dedicated ATtiny programmers](http://astore.amazon.com/jjg00-20/detail/B00B6KNJRY) for these chips if you don't already have one of the other devices handy, which might be the way to go if you have to buy something anyway (they look cool but I haven't tried one myself). ![isp_and_breadboard](/wp/2017/03/isp_and_breadboard.jpg) It was unusually difficult (for an Adafruit product) to find documentation of the pin-out of the USBtinyISP programmer's cables (this is probably because they are standard and most people who use them regularly have memorized it). After a little searching I was able to find a diagram in the [Adafruit forums](https://forums.adafruit.com/viewtopic.php?f=20&t=12019) and with some experimentation, came up with this pinout for the six-pin connector: Holding the connector with the cable's red-wire up, looking into the pin holes: [code lang=text] |-------------| | VTG | MISO | | MOSI | SCK | | GND | RST | |-------------| [/code] These are then connected to the ATtiny13 as such: 

USBtinyISP ATtiny13 (physical pin) ATtiny13 (logical)

VTG
8
VCC

MOSI
5
MOSI (PB0)

GND
4
GND

MISO
6
MISO (PB1)

SCK
7
SCK (PB2)

RST
1
RESET (PB5)
You'll need a way to see if the code is actually working, so attach an LED between physical pin 5 (`PB0`) and 4 (`GND`). You should include a resistor in series as well the exact value will depend on your LED (I used a 1k ohm resistor which seemed to do the trick). 

## Software

As mentioned above you can use the Arduino IDE to write code for these chips but you'll need to add support for the specific model you're working with. If you chose the ATtiny85 linked above you'll need to add the following Boards Manager URL: https://raw.githubusercontent.com/damellis/attiny/ide-1.6.x-boards-manager/package_damellis_attiny_index.json In my case I had some ATtiny13's in my parts bin so I used this URL: https://mcudude.github.io/MicroCore/package_MCUdude_MicroCore_index.json In either case once you add the URL you can go into the Arduino IDE Boards Manager, search for "attiny" and you should see an option pop-up in the search results that matches the chip you selected. Select this result and click "install". Now that the IDE knows your chip exists, you need to select it. Under the Tools -> Board menu select a board that matches the chip you're using. Depending on the "board" selected there may be additional options added to the Tools menu that you need to use to refine the selection to the specific chip you're using. For example, if you choose the ATtiny24/44/84 option, a menu will be displayed where you can select the specific CPU you're targeting. There may be other options as well, but if you don't know what they are I suggest leaving them at the defaults (when in doubt, consult the documentation for your chip). Now you're ready to compile some code. I recommend starting with the basics and loading the "Fade" sketch from the **File** > **Examples** > **Basics** menu. This will let you test-out the compile and upload process to make sure things are working before you start writing any complicated code. You'll need to modify the example sketch in order for it to work properly with the bare ATtiny. I changed the `led` variable to `0` (physical pin 5 on the ATtiny13) and I changed the `fadeAmount` to `50` to make the effect more obvious. I suggest running the compile step only until you get a clean result, no need to add complexity by trying to upload the code as well. Once you get the code to compile cleanly, you're ready to try uploading the sketch to the chip. Before clicking "compile", make sure you have the right programmer selected under the Tools menu. In my case this was simply a matter of selecting "USBtinyISP" from the menu (although I had to go through a couple of extra steps to get the Unix permissions right), but if you're using an Arduino to program the ATtiny you'll need to make sure the right [sketch](http://highlowtech.org/?p=1706) is loaded to the Arduino and then select `ArduinoISP` from the **Tools** > **Programmer** menu. With all this in place you're ready to cross your fingers and click the "Upload" button. With any luck happy messages will appear in the Arduino IDE console and in a few seconds the LED will begin to fade on and off! http://www.youtube.com/watch?v=qQJW_q7wYB8 

## What's Next?

Working with the ATtiny gets me thinking about how little attention we pay to the power of the technology we have at our disposal. It's easy to justify spending a little more money to get the same results with less work. You could skip buying a programmer and use an Arduino instead. Once you go that far, you could spend a few more (or less) dollars and get a full-blown Linux computer. So why bother with the ATtiny? I'm of the mindset that if you can make something cool, what you make it out of is less important. If you want to use a desktop computer to blink an LED, that's better than spending your time watching TV. What I'm getting at is that I think the accessibility of these technologies can result in something of a "lack of appreciation" by designers, developers, etc. of what these devices are capable of. As such, we tend to apply the consumer mentality of selecting the device that "owns the market" in terms of performance, cost, etc. when we chose the parts we want to work with.---
tags:
  - gullicksonlaboratories
title: A Universal Hierarchical Designer
link: http://jjg.2soc.net/2018/06/28/a-universal-hierarchical-designer/
author: jgullickson
description: 
post_id: 5213
date: 2018-06-28 14:09:34
created_gmt: 2018-06-28 19:09:34
comment_status: open
post_name: a-universal-hierarchical-designer
status: publish
post_type: post
---

# A Universal Hierarchical Designer

When I start working on a new project I often start by breaking the system into a few large parts and then incrementally defining the details inside those parts. I do this recursively, working on smaller and smaller levels of granularity until the grain-size matches whatever I'm implementing the system in (code, electronic components, Lego, etc.). I'm sure this is a common process used by many people. However, I'm not aware of any software tools which automate this process as a whole. I use a combination of various generalized tools, various specialized tools and good-old-fashioned pen & paper. This works, but it's awkward because the tools overlap in some areas, have gaps in other areas and in general don't exchange information easily.  This means time is wasted deciding where one tool should end and another beings, filling-in gaps and translating data between packages. The other problem with discreet, non-compatible tools is that they don't lend themselves to producing a cohesive history or documentation.  If nothing else, they don't make documenting the work as streamlined as it could be if the tools were designed to work together. What I have in mind is probably based on an ancient flowcharting software I used back in school called "MacFlow". This was what you'd expect, a tool for drawing flowcharts on a Macintosh but it had one powerful feature that I haven't seen since. [caption id="attachment_5214" align="alignnone" width="716"]![s-l1600](/wp/2018/06/s-l1600-e1530211320894.jpg) I wasn't able to find a screen-shot of the version of MacFlow I used so you get Ebay photos instead.[/caption] In MacFlow, you could double-click on a flowchart symbol and it would "drill-down" into what the symbol represents. If it was a "Predefined Process" symbol, a new worksheet would open where that process was illustrated; if it was an "Off-Page Connector" symbol, the page it connected to would open, etc. In retrospect this seems like the most obvious feature a piece of flowcharting software could offer, but I haven't seen it in any other that does this (or if it exists, it's so non-obvious that I never found it). ![macflow1.jpg](/wp/2018/06/macflow1-e1530211289427.jpg) Now imagine applying this to a piece of software with the ability to model more abstract systems. For example, if you're designing a computer you might start with four boxes representing the CPU, RAM, ROM and I/O, connected by a box representing a bus. Double-clicking on the ROM might take you to a view where you could add more symbols to represent the physical arrangement of ROM chips; drilling further displays the underlying gates or a detailed schematic, etc. Drilling-down with an alternate command might take you into the source code for the ROM routines, etc. At some level things become less abstract and more concrete. For software this could go all the way down to editing and executing code. For hardware you might not be able to go quite that far, but schematic and PCB designs are certainly possible, along with software simulations of the components. The same could be true for physical objects which could be represented as 3D models suitable for fabrication. Essential to the usefulness of this tool (at least for me) is that it does this with as simple of an interface as possible. Again MacFlow is a great example of this because it had very few features beyond laying-out and connecting a handful of standard flowchart symbols. Within this seemingly simplistic interface lie a number of intuitive or quickly learned features that allowed the user to create (and modify) complex designs quickly. This is an area where almost all contemporary tools I've used fall flat. So I'm wondering if anyone has already built something like this? It seems so obvious to me that there _has_ to be something out there, but when I think about how software is created (and more generally how _products_ are created) I can see how something as generic and expressive as this might not fit into an existing "vertical" and therefore get overlooked. I can also see how had a system like this been created, it could be quickly absorbed by one particular vertical market who then "improved" it with a bunch of market-specific features which made it no longer useful for other things.---
tags:
  - gullicksonlaboratories
  - rain
title: Back in Bl..ue
link: http://jjg.2soc.net/2018/06/22/back-in-bl-ue/
author: jgullickson
description: 
post_id: 5199
date: 2018-06-22 14:21:38
created_gmt: 2018-06-22 19:21:38
comment_status: open
post_name: back-in-bl-ue
status: publish
post_type: post
---

# Back in Bl..ue

Thanks to a gracious gift of gadgetry for Fathers Day, I was able to spend last weekend increasing Mark II's computing power to _maximum capacity!_ ![IMG_0844](/wp/2018/06/img_0844.jpg) Not only did I have the parts, but I also had the time (again thanks to Jamie & Berty) and had very ambitious plans for pushing Mark II across the finish line. Things didn't go quite as planned (I spent one of the two days fighting O/S problems of all things) but I was able to get all 8 nodes online by the end of the weekend. ![IMG_0848](/wp/2018/06/img_0848.jpg) Since then I've been working on running [high-performance Linpack](http://www.netlib.org/benchmark/hpl/) (hpl) to see how Mark II's performance stacks-up against Mark I. After a few runs, I was not only able _match_ the best results I achieved with Mark I, but slightly _exceed_ them. The best hpl performance I could get out of Mark I was 9.403 Gflops. Using a similar hpl configuration (adjusted for reduced memory size per node), I was able to get 9.525 out of Mark II. On both machines this result was achieved with a 4x4 configuration (4 nodes, 16 cores) so even though it wasn't the maximum capacity of either machine, it's a pretty good apples-to-apples comparison between the two architectures. That said, there's more performance to be had. I've learned a lot about running hpl since I recored Mark I's results and I'm confident that I can figure out what was stopping me from improving the numbers by adding nodes back then. In addition to better hpl tuning skills, I'm able to run all nodes of Mark II without throwing a circuit breaker, which was not the case with Mark I. This makes iterating on hpl configs a lot easier. The theoretical maximum hpl performance (Rpeak) for Mark II is 76 Gflops. Achieving this is impossible due to memory constraints, transport overhead, etc. but I think 75% of Rpeak is not unreasonable, which would yield a measured peak performance (Rmax) of 57 Gflops. This would rank Mark II right around the middle of the [Top 500 Supercomputer Sites](https://www.top500.org/)... [in 1999](https://www.top500.org/list/1999/06/?page=2). [caption id="attachment_5206" align="aligncenter" width="1005"]![crayt3e_005](/wp/2018/06/crayt3e_005.jpg) RAIN Mark II's hpl performance should be similar to the room-sized, 300KW Cray T3E (example pictured above)[/caption] Not too shabby for a $500.00 machine you can hold in your hand. ![ganglia_for_blog](/wp/2018/06/ganglia_for_blog.png) Still, there's a long ways to go from my current results of not-quite 10 Gflops to 57. I think theres a lot to improve in how I'm configuring hpl and I also think there is work to do at the O/S and hardware level. Minimally I'm going to need to increase the machine's cooling capacity (right now I don't even have heatsinks on the SOC's so I _know_ the CPU throttle is kicking in). So if I'm able to find an hpl config that doesn't _loose_ performance as I add nodes to the test, and I can keep the system cool enough to make sure cpu throttling doesn't kick-in, I should be able to get much closer to that Rmax value. But even with no improvement this test confirms the validity of the Mark II design. The original goal for this phase of the project was to establish the difference in performance between the Mark I machine and a similar cluster built from ARM single-board computers. The difference, surprisingly is that the ARM machine has a _slight_ node-for-node edge over the "traditional" Mark I. Perhaps just as important, Mark II achieves this with significantly less cost, physical size, thermal output and electricity consumption. Once I complete the power supply electronics for Mark II I'll be able to get more precise measurements of power consumption but even if Mark II operates at it's maximum power consumption that's still 1/10 of the power consumed by Mark I. This is a significant milestone in the RAIN project and marks the "official" end of the Mark II phase. I plan to finish Mark II's implementation (wiring front-panel controls for all nodes, designing & fabricating the front-end interface board, etc.) and generate a "reference design" for others who would like to recreate my results (perhaps even offer a kit?), but with these results in hand I can confidently enter into the Mark III phase of the project as well.
---
tags:
  - gullicksonlaboratories
  - rain
title: Beaten to the punch?
link: http://jjg.2soc.net/2017/11/15/beaten-to-the-punch/
author: jgullickson
description: 
post_id: 4948
date: 2017-11-15 09:36:23
created_gmt: 2017-11-15 15:36:23
comment_status: open
post_name: beaten-to-the-punch
status: publish
post_type: post
---

# Beaten to the punch?

If you've been following my [Raiden project](https://jjg.2soc.net/category/raiden/) you'll understand my mixed-emotions about this [announcement](https://www.anandtech.com/show/12037/cheap-supercomputers-lanl-has-750node-raspberry-pi-development-clusters) at the [Supercomputing 17](http://sc17.supercomputing.org/) conference. ![bitscope-cluster-module-zwlj6pz3](/wp/2017/11/bitscope-cluster-module-zwlj6pz3.jpg) It would be easy to take this as a defeat, since [many of my ambitions and design choices](http://jjg.2soc.net/2017/10/28/raiden-roadmap-102017/) for the Raiden project overlap with this product in many ways, and had I started the project when I first felt a personal need for it, I would have easily reached a similar stage years ago. On the other hand, it's encouraging to know that there is  demand for a computer like this,  and especially confirmation that developers desire personal supercomputers to aid in the development process. This need is the primary application called-out in the product announcement (it's what drove their project to happen apparently) and it's squarely in the sights of what Raiden Mark II is designed to provide. That's exciting for two reasons: 

## Reason #1

It's not overly-ambitious for me to expect to have a working [Mark II](http://jjg.2soc.net/2017/10/28/raiden-roadmap-102017/) prototype completed by spring. While it won't be built at the scale of the BitScope system, I don't believe that scale is necessary to fulfill its role as a developer's machine. Additionally, even hand-built production models of Mark II will be _exponentially_ more accessible to developers in terms of cost, space, power and manageability. 

## Reason #2

This announcement means that my plan for [Mark III](http://jjg.2soc.net/2017/10/28/raiden-roadmap-102017/) (a scalable system with enough power to be useful for production work) is _significantly more advanced_ than the BitScope system. If their machine is powerful enough to be useful for scientific work (and valuable enough to demand that price), Raiden Mark III should _easily_ be competitive in terms of utility and cost. Something else that isn't mentioned in the available information is evidence of any focus on the software tooling around setting up, maintaining and programming the machine.  Developing software for supercomputers is a perennial problem in high-performance computing.  There have been great strides recently in hardware for these machines but there seems to be less focus or progress on making it easier to write software that takes advantage of these hardware features. As a programmer who has studied & built machines like this for decades I understand that making supercomputer hardware accessible to developers has limited value if the development tools are not equally accessible to most programmers.  This is why combining the two is a key aspect to the Raiden project, and something that seems to differentiate it from many modern supercomputers. Clearly, it's time to get back to work...
---
tags:
  - gullicksonlaboratories
  - rain
title: Beginner's Luck (KiCad Part 2)
link: http://jjg.2soc.net/2018/04/23/beginners-luck-kicad-part-2/
author: jgullickson
description: 
post_id: 5152
date: 2018-04-23 13:09:58
created_gmt: 2018-04-23 18:09:58
comment_status: open
post_name: beginners-luck-kicad-part-2
status: publish
post_type: post
---

# Beginner's Luck (KiCad Part 2)

[Last time](https://jjg.2soc.net/2018/04/09/its-time-for-me-to-kicad/) I left-off with a 2-D printed version of a printed circuit board I designed in KiCad which was enough to confirm that the board I was designing would fit into the board I'm designing it for. This felt like a lot of progress! ![2018-04-08 10.10.57](/wp/2018/04/2018-04-08-10-10-57.jpg) However there was still a lot to do, primarily consisting of making the actual electrical connections between the components on the board. For some dumb reason I thought this would be easy, but I ran into problems getting the traces to connect, or prevent them from overlapping, etc. Part of the problem is that I have no _real_ experience in the more abstract (i.e., not related to learning the software) aspects of PCB design. Luckily [Bob](https://hackaday.com/author/bobbaddeley/) was willing to help me out and gave me a list of steps to use as a starting point when laying out a board: 

  1. Lay out the connectors where they have to go
  2. Logically place groups of components where they need to go (power section should be in one area, microcontroller + decoupling caps in another area, etc.)
  3. Refine component placement to have as few overlapping airwires as possible to ease routing
  4. Route length sensitive traces first
  5. Route other traces
  6. Route power traces, but do a ground pour to make it easier
  7. Tidy up
  8. Lay out silkscreen and be as descriptive as possible
In addition to this, Bob said traces could be 6 mil wide minimum, but to aim for 10 mil in general and 12 mil for power. This advice helped _a lot_ and between this an finding a "[mode](http://docs.kicad-pcb.org/stable/en/pcbnew.html#_interactive_router)" for laying traces that worked for me, I was able to connect all the parts and get the board to pass the [Design Rules Check](http://docs.kicad-pcb.org/stable/en/pcbnew.html#_final_drc_test) (DRC). Bob eyeballed my layout, gave me some tips on improving it and said it looked like it would work. ![panel_driver_pcbnew](/wp/2018/04/panel_driver_pcbnew.png) It was around this time that I noticed an error in the pin assignment where the panel driver board connects to the headers on the Clusterboard. Since the driver board will connect using a right-angle connector, but I designed it with a straight connector in mind, pin 1 would end up in the wrong place. This was easily fixed by rotating the connector on the board, but since this connector has traces leading to almost every other component, the layout became a complete mess. Instead of fighting with this, I took the opportunity to redo the board from scratch and apply everything I've learned so far. The second time around went much faster and I think it looks nicer as well. ![panel_driver_v1.4.0](/wp/2018/04/panel_driver_v1-4-0.png) After reviewing the design (for what felt like the millionth time), I was reasonably sure I didn't make another mistake like the connector and decided it was ready to upload to [OSH Park](https://oshpark.com/) for production. ![panel_driver_osh_upload1.png](/wp/2018/04/panel_driver_osh_upload1-e1524500820364.png) This process went _very_ smooth. I don't have any personal experience to compare it to (being the first board I've designed to be produced this way) but the website was easy to use, the visualizations and "checklist" guides were very helpful and I felt like I had a  clear idea of what the final product would look like when they were done. Now it's just a matter of waiting. ![2018-04-21 13.41.03](/wp/2018/04/2018-04-21-13-41-03.jpg) The boards showed up a few days early and I was lucky enough to have time to try one out (the minimum order was 3 boards).  I had only enough parts on-hand to complete one, but this was intentional because I assumed that I would have made some mistakes and that I could order more parts after I fixed the design and re-ordered a second batch of boards. [gallery ids="5157,5158,5159" type="rectangular"] After assembly, I tried to quell my excitement and properly check-out the board in steps (inspired by [Bob's article](https://hackaday.com/2016/06/29/tools-of-the-trade-inspection/)) to reduce the chances of burning-up the new panel driver board or worse, the precious Clusterboard. [archiveorg 2018042115.38.41 width=640 height=480 frameborder=0 webkitallowfullscreen=true mozallowfullscreen=true] First, I tested the driver board alone to make sure power was flowing to the right places.  Then, I attached it to the Clusterboard and checked the i2c bus to see if it was showing-up correctly. Finally. I wired-up the LED's and toggle switch and ran the python script. [gallery ids="5160,5161,5162,5163" type="square"] Much to my surprise, the LED's lit up and the switch correctly toggled between display modes! Something was not _quite_ right in that the LED's don't illuminate in exactly the right order.  It's not clear yet if that's a problem with the LED wiring or a design error on the board. The good news is that I _might_ be able to correct this with a software change, so at least the boards themselves may be salvageable. Even if I have to make changes to the board to fix this, there's a number of improvements I'd like to implement anyway so this won't be the final version of the panel driver board regardless. It's hard for me to express how exciting it was for me to complete this process. Learning to design circuits using an EDA is something I've put-off for a long time, but now that I'm getting the hang of it I can see so many ways in which learning to use these tools will expand and accelerate my ability to make things. There are many ideas I've had in the past that I didn't pursue because I knew they would involve working with components that I couldn't use with the old construction techniques, or that wouldn't be practical to produce in any volume using those techniques. ![2018-04-21 16.22.37](/wp/2018/04/2018-04-21-16-22-37.jpg) I have a feeling that learning these skills will be like building my 3d printer. Building the printer taught me a lot, but also opened my mind creatively by making it faster and easier to make both things I was already making but also things I never considered because I didn't have the tools/skills/materials necessary.

## Comments

**[pfeerick](#154 "2018-04-28 04:37:26"):** I'm guessing the ports on the I/O expander going in opposite directions on each side was the cause of the problem? The chip manufactures don't want to make our lives TOO easy! ;) So what are your thoughts as to how to actually make the cluster network boot... or are you not quite there yet? Is it going to be clusterboard wired to the pine64, and the pine64 does the DHCP & PXE boot serving, and then maybe wireless access to the pine64 for 'hands-free' access? As I tried to run ... forget what it's called now... dns-masq... service on one machine on my network to spoof the PXE boot for a rock64 (as I had it before I got the clusterboard) and whilst I could manually make it network boot, I couldn't get it to automagically boot... It didn't seem to be able to bet getting the magic packet that told it where the PXE server actually was, although dns-masq was logging the request and claiming it was responding... Just curious on your thoughts to this (or if it is a non-issue in how you're designing the system).

**[Jason J. Gullickson](#155 "2018-04-28 09:28:37"):** Long-term I'd like to have the nodes on the Clusterboard network boot using their on-board SPI flash, but I haven't figured out exactly how to make that all work. For now I have the stand-alone PINE64 board acting as a "front-end" node providing DHCP and acting as a router between the Clusterboard's network and my wireless network, similar to how the Mark I machine worked except it was all Ethernet (those servers had two wired Ethernet controllers in them). Do you know of a good guide to using the SOPINE's on-board flash to boot over the network? I've seen a few things discussing the various components but I haven't seen a concise guide that pertains specifically to this setup. After I wrap-up the hardware work I'll be able to spend more time learning all the intricacies of this but it would be very convenient if I had a simple guide to follow for now :) Until then I'll probably stick with this setup and add NFS to the mix so I don't have to keep manually replicating the application binaries between hosts...

**[pfeerick](#156 "2018-04-28 22:58:43"):** No, I'm afraid not. As the SPI is only 16MB (128Mbit), I'm thinking it's not going to store much of an operating system, so I am thinking of doing exactly the same as you... PXE off the front-end/master node, and have NFS off the front end for both the OS and file storage. For the rock64, it was a process of running one of the /usr/local/sbin scripts to copy the uboot bootloader from the SD card (it may accept eMMC, not sure) to the SPI Flash, and then the rock64 was able to boot with no eMMC or SD card installed, opening up USB boot and PXE boot in its case. So I'll try the same again for the sopine module, and hopefully get the pine64 to co-operate providing PXE boot info. Fingers crossed! :D

**[pfeerick](#157 "2018-04-28 23:07:18"):** Hm, looks like the SPI UBOOT stuff is here (https://github.com/ayufan-pine64/bootloader-build/releases)... you write an image to the SD card or eMMC and boot an image that simply write uboot to the SPI. So the script to do it from the installed OS like on the rock64 isn't there... this is quicker for the sopine... just boot each in turn, load uboot, and you're done... but it would have been nice to have the option... wonder how transferable the script is. And just for reference, the SPI Flash chip is the Winbond W25Q128JV

**[Jason J. Gullickson](#161 "2018-05-18 09:33:18"):** Awesome, thanks for all this info @pfeerick. I got pulled into some springtime projects and haven't had a chance to try this out, but it's very helpful to talk to someone else who's fiddling with this hardware!

**[pfeerick](#162 "2018-05-18 17:43:19"):** No problem... and same here! I'd suggest you put it aside for a while if you want to do PXE boot as I've hit a roadblock with that - the uboot bootloader isn't loading the networking support properly so can't actually network boot. Someone else on the pine64 forum has gotten the pxe boot stuff working by compiling a version of the mainline uboot, but I'm not sure what else was needed or how to get that onto the sopine modules easily, or of there are other problems that result.. so just leaving that for a little while and working on other stuff for now.

---
tags:
  - gullicksonlaboratories
  - rain
title: BOINC, Gridcoin and scaling dynamically beyond local nodes
link: http://jjg.2soc.net/2018/08/03/boinc-gridcoin-and-scaling-dynamically-beyond-local-nodes/
author: jgullickson
description: 
post_id: 5269
date: 2018-08-03 09:46:11
created_gmt: 2018-08-03 14:46:11
comment_status: open
post_name: boinc-gridcoin-and-scaling-dynamically-beyond-local-nodes
status: publish
post_type: post
---

# BOINC, Gridcoin and scaling dynamically beyond local nodes

Providing a way to run workloads too large to run on a single RAIN computer has been part of the roadmap since the beginning. As work on [Mark II](https://hackaday.io/project/85392-rain-mark-ii-supercomputer-trainer) draws to a close I'm starting to think about the work that will go into Mark III and one portion of that work is support for dynamic scalability. As much as I enjoy designing things from scratch, I do make an effort to make sure there isn't an existing solution that fits the problem I'm trying to solve (discarding my work on ganging-together entire PINE64 A64 SBC's when the [Clusterboard became available](https://jjg.2soc.net/2018/02/23/pine64-cluster-board/) is an example of this). So when I started looking into ways to create a dynamic, distributed network of RAIN machines I looked at existing [grid computing](https://en.wikipedia.org/wiki/Grid_computing) systems to see if I could avoid starting from scratch. As it turns out, there is one clear choice in off-the-shelf, open-source grid computing systems and that system is [BOINC](https://boinc.berkeley.edu/). I've known about BOINC for a long time but I didn't know it by that name. I knew it as [SETI](https://seti.org/), which is the specific project from which the general-purpose BOINC system emerged. Today BOINC is used for a [wide-range of scientific computing projects](https://boinc.berkeley.edu/wiki/Project_list) and I was very pleased to see that it's an active, well-used project. I spent a little time learning about [how BOINC works](http://boinc.berkeley.edu/trac/wiki/AppIntro) and it looks like it could be used as-is with RAIN the same way it can be used with almost any computer. The challenge isn't in making RAIN support BOINC so much as it is in making the user's application compatible with BOINC. Once that's achieved, the application could be run on a single RAIN machine across the internal cluster, a distributed grid of RAIN machines and of course any other computer participating in the larger BOINC network. This is pretty cool, but there's not much for me to improve on here so to me this is akin to other established parallel computing tooling like [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface); supported by RAIN but not really anything I need to contribute to. Something else I discovered while researching BOINC is [Gridcoin](https://gridcoin.us/). The idea behind Gridcoin is similar to any other [cryptocurrency](https://en.wikipedia.org/wiki/Cryptocurrency) (such as [Bitcoin](https://en.wikipedia.org/wiki/Bitcoin)), but instead of securing the network by executing [pointless busywork](https://en.wikipedia.org/wiki/Bitcoin#Mining), the network is secured by executing distributed applications in the BOINC network. At first this seemed a lot like the "global network" I describe in "[Why Personal Supercomputers](https://jjg.2soc.net/2017/12/13/why-personal-supercomputers/)" but as I looked closer it doesn't provide all of the features I'm looking for. What's missing is the ability to use Gridcoin to secure future processing resources on the grid. Instead, Gridcoin is intended to be a general-purpose currency just other cryptocurrencies. What I'm looking for is a way to encourage RAIN operators to lend their unused compute cycles to others, and encourage them to do this by providing a way to tap-into other's unused cycles when they are needed in the future. This allows users who are acquiring a RAIN machine to immediately run an application access to computing resources beyond their own machine, while incentivizing others who are interested in the technology but don't have a specific application in mind to get value out of the system until they are ready to utilize it. In my mind this took the form of a "currency", but it doesn't have to be, and I don't have any strong reason to create a currency vs. an accounting system that ensures users will have access to an amount of processing power equivalent to the power they have contributed to the grid. Based on this it looks like I can learn some things from BOINC and Gridcoin, but unfortunately neither is a perfect fit for my design. Regardless I've learned a lot from studying both, and I'm very excited about the fact that I can contribute currently unused capacity of the Mark II prototype to ongoing scientific work with little effort.
---
tags:
  - gullicksonlaboratories
title: Build vs Buy
link: http://jjg.2soc.net/2017/07/18/build-vs-buy/
author: jgullickson
description: 
post_id: 3917
date: 2017-07-18 06:19:59
created_gmt: 2017-07-18 11:19:59
comment_status: open
post_name: build-vs-buy
status: publish
post_type: post
---

# Build vs Buy

This dilemma is one of the hardest problems in software.  The problem exists in many other spheres, but I think it's worst in software because it's so hard to understand the cost of either option at the time the choice has to be made. This is further complicated by the fact that those who are most qualified to make the the choice are almost exclusively biased toward one option or the other, making it very hard to have an objective opinion. As a programmer, I'm biased toward build.  For me building something always includes the added value of learning, something very important for the curious and creative sort of people who make good programmers. On the other side are technicians, operators and software vendors who typically prefer the "buy" option, either due to simplicity, deligated responsibility, perceived cost advantages or personal enrichment. Of course fear is another factor which affects both sides.  Programmers fear off-the-shelf solutions will limit their ability to work with the software or customize it to suit the application.  Others fear building software because the cost appears less clear, and an industrial mindset makes "reinventing the wheel" seem inefficient. In my experience it rarely makes a difference in terms of the metrics in which a software project success is typically measured.  I've seen no consistent difference in terms of overall cost, ability to deliver on-schedule or finished-product quality between the two choices.  These factors are more influenced by other aspects of the project. These aspects include the experience of the team, the methods of management, the balance of time/cost/quality and the overall satisfaction the people doing the work have with the direction chosen. There are of course many other factors, but the point is that the build vs buy decision is probably the least important component determining the success of the project.  That being the case, I promote the build option because the results are less likely to result in entanglements which result in hidden costs after the project is complete.  It also affords the team greater autonomy than the "but" option, along with the aforementioned benefit of making the people involved smarter.---
tags:
  - gullicksonlaboratories
title: Charity Arcade - All Together Now
link: http://jjg.2soc.net/2017/04/24/charity-arcade-all-together-now/
author: jgullickson
description: 
post_id: 1996
date: 2017-04-24 11:09:33
created_gmt: 2017-04-24 16:09:33
comment_status: open
post_name: charity-arcade-all-together-now
status: publish
post_type: post
---

# Charity Arcade - All Together Now

Last weekend Ma & Pops brought the cabinet up to the lab, and it looks amazing. Now just screw the control panel to the rest of the cabinet and call it a day, right? ![2017-04-22 08-55-05 0360](/wp/2017/04/2017-04-22-08-55-05-0360.jpg) Not exactly. This was the first time the control panel would be fit into the cabinet. There's a lot that could cause this to not go smoothly (small measurement/cutting errors, the dynamic nature of wood, etc.). Surprisingly, the panel slid right in! It was a little snug, but not enough that any changes were necessary. ![2017-04-22 13-37-01 0367](/wp/2017/04/2017-04-22-13-37-01-0367.jpg) Once that was confirmed, the next step was to mount the coin acceptor. I completely underestimated this part of the job. The first challenge was that I never gave dad the measurements for the coin acceptor (it hadn't arrived back when I made the original drawings) so it was kind of sheer luck that there was enough room for it between the donations window and the edge of the control panel.  Luckily there was enough room, but just barely. Additionally, the mechanism was designed to be mounted in thin sheet metal, not 1/2" plywood, so there we had to do a little bit of jigsaw work to get everything to fit and function properly once it was mounted. ![2017-04-22 10-55-48 0363](/wp/2017/04/2017-04-22-10-55-48-0363.jpg) After that things went pretty smoothly. We drilled a temporary hole in the back for the power cords (this will eventually be replaced with a receptacle), ran the power cords, connected the coin slot and screwed-down the control panel. Last but not least, the trap door on the donation bin needed something to hold itself shut. This isn't designed to be a high-security machine, so a small lock and hasp was enough to keep anyone from accidentally spilling the collections all over the place. I have to say it was really cool to see the whole thing together and to play it "at scale". The dimensions turned out to fit well and the machine is very comfortable (at least for me, maybe for taller people you'd want to set it on a pallet or something). You never know for sure when you start designing something like this how it's going to feel when it all comes together, and while there are things we'd likely change in the next iteration, overall it's turned out great, and in such a short amount of time. ![2017-04-22 14-12-55 0369](/wp/2017/04/2017-04-22-14-12-55-0369.jpg) Now it's time to tear it apart again (or at least remove the electronics) so the rest of the cabinet can be painted. There's a few other things I'd like to improve  (add power receptacle, bigger/better speakers, illuminate the donation bin, tidy-up wiring, etc.) but I think a few more hours of "beta testing" are in order before I dismantle it...---
tags:
  - gullicksonlaboratories
title: Charity Arcade - Cabinet
link: http://jjg.2soc.net/2017/03/31/charity-arcade-cabinet/
author: jgullickson
description: 
post_id: 1507
date: 2017-03-31 13:29:12
created_gmt: 2017-03-31 18:29:12
comment_status: open
post_name: charity-arcade-cabinet
status: publish
post_type: post
---

# Charity Arcade - Cabinet

Last weekend dad and I started working on the cabinet. I wanted to take more pictures, but it's tricky to document and get this kind of work done at the same time. The design is based on a combination of the [original Pacman cabinet](https://duckduckgo.com/?q=original+pacman+arcade&t=ffab&iax=1&ia=images&iai=http%3A%2F%2Fwww.agsarcadegamesales.com%2Fimages%2Fpacman390.jpg) crossed with a [Space Invaders cabinet](https://duckduckgo.com/?q=original+space+invaders+cabinet&t=ffab&iax=1&ia=images&iai=http%3A%2F%2Fdirectgamesroom.com%2Fprodimages%2Fvoyager2%2Fspaceinvaders.jpg). I liked the Pacman designs simplicity of mounting the controls and screen in a single panel but I also liked the more angular outline of the Space Invaders cabinet (felt more 80's to me than Pacman's curves). I sketched this up a few times and once I had something with a few measurements in it I passed the drawing along to dad who ran with it and turned it into [something](/wp/2017/03/img_0096.jpg?w=756) with enough information to actually build something from. [caption id="attachment_1539" align="aligncenter" width="2448"]![2017-03-25 10-45-03 0134](/wp/2017/03/2017-03-25-10-45-03-0134.jpg) Dad with the full-size mock-up (the perspective makes it look a bit smaller than in person)[/caption] The cabinet design has a few other considerations that might not matter if you're building one for personal use, but were important for the Charity Arcade machine. First is the transparent "coin case" that displays the donations and second is that it needs to be portable enough that Jamie & I can get it around. In our case this means light enough for two to carry it and small enough to fit into the back of our Honda Element. After a few phone calls and emails dad tweaked the design to meet these requirements and made a few improvements as well. A lot of the time was spent confirming the design and measuring the "fixed" components of the machine (the screen, controls, other electronics, etc.) as well as validating the basic measurements for usability and portability. Once that was done, we could start making some cuts. [caption id="attachment_1542" align="aligncenter" width="3264"]![2017-03-25 11-39-24 0138](/wp/2017/03/2017-03-25-11-39-24-0138.jpg) Partial cardboard mock-up of the control panel[/caption] The design is intended to make use of two 4x8' sheets of 1/2" plywood, so the first cuts were to get the sheets down to a more manageable size. The other priority was to get the "control panel" cut so I could take it back to the lab and install the electronics. Getting the angles just right was a bit of work due to the size (it wouldn't fit on the radial arm saw, etc.) so we ended up freehanding it with a circular saw, but it turned out just right. [caption id="attachment_1545" align="alignnone" width="2448"]![2017-03-25 13-02-33 0140](/wp/2017/03/2017-03-25-13-02-33-0140.jpg) The "uncarved block"...[/caption] It was really cool to start seeing the parts take shape. It's easy to get focused on the goal at this point but there's still a lot of work to do and I don't want to miss out on the joy of building the machine by focusing on having it done. Next step for me is to layout all the electronics on the control panel, cut some holes and see how it all fits together. In the meantime dad's going to keep working on the cabinet and we'll get together and compare progress again soon.---
tags:
  - gullicksonlaboratories
title: Charity Arcade - Control Panel
link: http://jjg.2soc.net/2017/04/12/charity-arcade-control-panel/
author: jgullickson
description: 
post_id: 1563
date: 2017-04-12 09:36:57
created_gmt: 2017-04-12 14:36:57
comment_status: open
post_name: charity-arcade-control-panel
status: publish
post_type: post
---

# Charity Arcade - Control Panel

In our [last installment](http://jjg.2soc.net/2017/03/31/charity-arcade-cabinet/), dad sent me home with a board that was destined to become a home for the screen, controls and numerous hidden bits of the arcade machine.  I call this the "control panel".  Last weekend we had a chance to dig into putting the "control" into the panel... We'd noodled on a number of layouts for the panel throughout the course of the project.  After looking at a lot of existing designs and doing a few "usability tests", we settled on a fairly basic layout for the controls loosely based on the mid-80's Nintendo cabinets.  This follows angular geometry of the rest of the design (no ergonomic curves here!), and gives us more horizontal room compared to designs that place the player select buttons in the same plane as the rest of the controls. ![2017-04-08 11-13-16 0227](/wp/2017/04/2017-04-08-11-13-16-0227.jpg) Once a layout was decided on, measurements were made and marks were placed.  Next comes the cutting, which is of course where things get ugly (because they are less reversible). I knew that the most challenging part would be cutting the hole for the screen.  This would involve a jigsaw, and cutting straight lines with a jigsaw is, in my experience, challenging.  Unfortunately, that wasn't the worst of it.  The worst was that I had marked the board on the "inside" of the panel (thinking this would leave the outside cleaner), but on the first cut the saw readily made a mess out of the front of the panel.  After this happened, I realized that I already knew that the nicer looking side of the cut is the side the saw is on, but for whatever reason I spaced this and the panel paid the price. So I knew I needed to make the cuts from the outside face of the panel, but I didn't want to re-measure everything to transfer all the marks to the other side.  I had already made a mistake marking the board earlier and I didn't want to risk repeating something like that.  So, after a little noodling it occurred to me that since everything was either a hole of a straight line, I could just drill small holes through the panel at the various points and then connect the dots! ![2017-04-08 12-37-08 0229](/wp/2017/04/2017-04-08-12-37-08-0229.jpg) This worked well and after a few minutes I had the hole for the screen cut without making too much more of a mess out of the panel. Now it was time for the easy part, the round holes for the controls. Only this wasn't so easy. I started with the buttons which are just slightly more than 1" around. For this I tried using a spade-style drill bit and basically got nowhere. I don't know if the bit is dull or if it's just a poor way to cut a hole in plywood but essentially all I got was a nice round indentation in the wood an a little bit of rustic-smelling woodsmoke. Instead of risking further damage to the panel, I headed to Fleet Farm to pick up a proper hole saw. I came back with a small [Milwaukee hole saw kit](http://astore.amazon.com/jjg00-20/detail/B0045CWX6S) which included a 1 1/8" saw which made holes that were almost a perfect fit for the buttons. The kit also included a 1 1/2" saw that was perfect for making a hole for the joystick. ![2017-04-08 13-21-05 0231](/wp/2017/04/2017-04-08-13-21-05-0231.jpg) The hole saws made short work of getting the controls mounted, and after a little clean-up and sanding, everything fit right into place. ![2017-04-08 13-24-02 0232](/wp/2017/04/2017-04-08-13-24-02-0232.jpg) ![2017-04-08 14-10-24 0236](/wp/2017/04/2017-04-08-14-10-24-0236.jpg) After a test fitting, the controls were removed and Jamie spent some time giving the front of the panel a few coats of black paint.  Once the paint was dry, the controls went back in and I moved on to the next challenge: attaching the screen. Beyond cutting a hole for it, I hadn't given much thought to exactly _how_ I was going to attach the screen to the panel. I had a number of ideas, but now that I could lay the screen on the panel, none of them seemed that great. The biggest concern was that the panel is only 1/2" thick, so it seemed like it would be hard to fasten anything to the back of the panel without having to pierce the face as well. Since it wouldn't be a GL project if I didn't find some way to drag 3D printing into it, I decided to experiment with modeling some parts to clamp-down the screen. I still didn't feel confident that they would be strong enough without going all the way through the panel, but it was worth a try. ![2017-04-08 17-53-21 0240](/wp/2017/04/2017-04-08-17-53-21-0240.jpg) ![2017-04-09 07-54-05 0241](/wp/2017/04/2017-04-09-07-54-05-0241.jpg) When I mounted the test part to a piece of scrap panel with three screws I was amazed to find out that not only was it strong enough to hold the screen, but I couldn't pull it apart if I tried. Those three little screws were able to resist all the pulling and yanking I could manage, so I felt comfortable that four of those clamps could hold the screen in place under reasonable conditions. ![2017-04-09 12-46-57 0250](/wp/2017/04/2017-04-09-12-46-57-0250.jpg) Next I needed to find a way to mount the [JAMMA board](http://astore.amazon.com/jjg00-20/detail/B01L1UT0R6) and its power supply. I knew I wanted to mount the board to the back of the screen, and I imagined creating an adapter that would mate the mounting holes on the JAMMA board to the VESA mounting holes on the screen. However for now I just cheated and lined one of the board's mounting holes up with one of the VESA mount holes, and then used a self-tapping screw in the opposite hole of the board to thread into a ventilation hole of the screen. This is cheesy, but good enough for testing until I fabricate a proper adapter. ![2017-04-09 12-55-57 0251](/wp/2017/04/2017-04-09-12-55-57-0251.jpg) ![2017-04-09 12-56-06 0252](/wp/2017/04/2017-04-09-12-56-06-0252.jpg) Pinning the power supply to the panel was simply a matter of securing the input and output cords using cable clamps. I might do something more sophisticated in the long run but this seems adequate. Now that things are tied-down I could start wiring things up to test out the system. Since I had done this once before I didn't expect any surprises, but I did notice one small problem when I booted the system up the first time. ![2017-04-09 13-02-46 0255](/wp/2017/04/2017-04-09-13-02-46-0255.jpg) Fortunately this was easily remedied by flipping a configuration DIP switch on the JAMMA board, as flipping the screen physically would have caused some alignment problems. Once this was resolved, I tied a temporary speaker to the wiring harness and the system was ready for testing.---
tags:
  - gullicksonlaboratories
title: Charity Arcade - Electronics
link: http://jjg.2soc.net/2017/03/24/charity-arcade-electronics/
author: jgullickson
description: 
post_id: 1260
date: 2017-03-24 15:11:00
created_gmt: 2017-03-24 20:11:00
comment_status: open
post_name: charity-arcade-electronics
status: publish
post_type: post
---

# Charity Arcade - Electronics

While the electronics are off-the-shelf, the documentation available is not exactly straightforward. Here's what I've been able to decipher so far. ![2017-03-22 19-49-04 0128](/wp/2017/03/2017-03-22-19-49-04-0128.jpg) The 60-in-1 board uses the standard JAMMA connector, however it also includes a few modern connectors as well: 

  * A VGA video connector
  * A Molex power connector
  * An 1/8" stereo audio jack
  * 2 Mystery connectors
I struggled a bit to get a handle on what the power supply requirements are for the [JAMMA board](http://astore.amazon.com/jjg00-20/detail/B01L1UT0R6). After a number of conversations (and a lot of web searches) I found out that the Molex connector is exactly what it looks like; a standard PC-style power supply input. Of course I had already bought the _wrong_ supply before finding this out. For some reason, I thought the JAMMA board only needed a 5VDC supply. This was a drag because I later found out that the coin slot I bought needs a 12VDC supply. Turns out the JAMMA board needs both 5VDC and 12VDC. The up side is that when I have the right supply for the JAMMA board I'll have everything I need for the coin slot... I talked to my friend Jeff about this (he's built one of these cabinets before) and he said that he just used an [external hard disk power supply](http://astore.amazon.com/jjg00-20/detail/B016BLJ142). I did some digging and found something like this in the parts bin and sure enough I had one already. I wired-up up the power supply via the JAMMA wiring harness because I knew I would need a 12VDC supply for the [coin slot](http://astore.amazon.com/jjg00-20/detail/B00C16P03I). Later (after talking to Jeff again) I found out that there are 12VDC supplies coming off the JAMMA connector to power things like the coin slot, so it makes more sense to connect the power supply to the Molex connector and then feed the rest of the system from the wiring harness. After noticing that the wires in the harness are bundled into groups that lead to the sections of the cabinet (the player controls, the coin box, etc.), it wasn't too hard to trace the wires from the board to to figure out where to hook-up the joystick, start buttons, speaker, etc. With the basics wired-up I applied power to the board and it didn't start on fire! ![2017-03-22 19-48-56 0127](/wp/2017/03/2017-03-22-19-48-56-01271.jpg) I spent some time figuring out how to access the settings and configure the board as well as navigate around the game selection interface. We played a couple games of Ms. Pacman just to make sure everything was working properly. ![2017-03-22 20-33-39 0130](/wp/2017/03/2017-03-22-20-33-39-0130.jpg) The goal of all this was to make sure the parts would work together and nothing was DOA with enough time before the deadline to order any replacements. Everything other than the coin slot tested out (I kind of forgot to try hooking it up). I labeled all the wires and tied things up with zip-ties so I won't need the entire dining room table to hook it up next time, and through the process learned a number of ways to improve how things should be connected. There's a few more things I want to test out with the electronics, and I have some more parts on-order that I'll need to incorporate if they show up in time. In particular, I want to have a display which shows the total amount of donations collected. This will use an LED matrix installed in the "marquee" part of the cabinet, and use an [Arduino](http://astore.amazon.com/jjg00-20/detail/B008GRTSV6) (or perhaps an [ATTiny](https://jjg.2soc.net/2017/03/12/attiny-microcontrollers-and-how-bigger-isnt-always-better/) ) to drive the display. I also want to add some illumination to the coin collection bin, perhaps controlled by the same brain as the LED matrix. We'll see. Next time I'll be diving into building the cabinet with my father. This is the part I'm most anxious about because I have a lot less experience with woodworking than I do with electronics. Fortunately my father has a lot _more_ experience with this than I do, so I'm looking forward to diving into that with him. 

## References

  * https://www.highscoresaves.com/60-in-1-icade-classic-arcade-multigame-jamma-pcb-board.html
  * http://www.instructables.com/id/install-a-JAMMA-harness-in-an-arcade-cabinet/step7/Connect-speakers-and-coin-bundles/
  * https://forums.arcade-museum.com/archive/index.php/t-120535.html
  * http://arcadecontrols.com/BBBB/jh.html
  * http://www.therealbobroberts.net/meter.html---
tags:
  - gullicksonlaboratories
title: Charity Arcade Machine
link: http://jjg.2soc.net/2017/03/16/charity-arcade-machine/
author: jgullickson
description: 
post_id: 958
date: 2017-03-16 06:58:29
created_gmt: 2017-03-16 11:58:29
comment_status: open
post_name: charity-arcade-machine
status: publish
post_type: post
---

# Charity Arcade Machine

A year or so ago Jamie shared a link about a project in Sweden where modified video game arcade machines were placed in airports and used to collect money for charity.  Being the philanthropic type, she wanted one to raise money for local charities.  We weren't able to find much more information about the project or how to obtain one of the machines, so of course we set out to build one ourselves. ![IMG_0096](/wp/2017/03/img_0096.jpg) In this series of posts I'll document the process of researching, designing, building, testing and finally, putting the machine to use at an actual charity event.  If you'd like to see the results in person we'll be debuting the machine (_assuming the project meets its deadline_) at the [Dodge County YMCA's](http://www.theydc.org/) "[Back to the 80's](https://www.facebook.com/events/260254934428525/)" fund-raiser on April 29th, 2017. The machines from the [original project](https://www.youtube.com/watch?v=9eIRP68PQvo) are very similar to "classic" video game machines from the 1980's. The only obvious difference is that the base of the machine is cut-away and replaced with transparent walls which make the funds collected visible. ![charity-arcade](/wp/2017/03/charity-arcade.jpeg) We thought this was a nice touch, so our design incorporates this as well as few additional design goals: 

  * Overall cabinet size is reduced to make it more portable (i.e., fits in our Honda Element) while preserving the classic "feel" of vintage arcade games
  * Cabinet design is optimized to be cut from two 4'x8' sheets of plywood
  * Incorporate an LED matrix display in the marquee area to display the amount of funds collected
Regarding the electronics, there are a lot of options to choose from but ultimately we settled on using mostly off-the-shelf parts. There are are numerous reasons for this choice, but ultimately we hope this increases the chances that things will work out-of-the-box, and continue to work reliably in the field. While it was tempting to consider a more DIY option like using a Raspberry Pi and emulation, for this particular application (an 80's party) we only needed a limited number of games, and the cost difference between the options was negligible. Also, we wanted to create a design that others could replicate without a lot of specialized knowledge. Here's a list of the electronics hardware purchased so far: 
  * JAMMA 60 in 1 Multicade Arcade PCB ([eBay](http://www.ebay.com/itm/182474508071?_trksid=p2057872.m2749.l2649&ssPageName=STRK%3AMEBIDX%3AIT), $55.99)
  * JAMMA 60 in 1 controller kit w/ JAMMA connector ([eBay](http://www.ebay.com/itm/251336973880?_trksid=p2057872.m2749.l2649&ssPageName=STRK%3AMEBIDX%3AIT), $39.00)
  * Coin acceptor ([eBay](http://www.ebay.com/itm/262760538183?_trksid=p2057872.m2749.l2649&ssPageName=STRK%3AMEBIDX%3AIT), $14.99)
  * MAX7219 Dot Matrix LED module (1x4?) ([eBay](http://www.ebay.com/itm/311704573803?_trksid=p2057872.m2749.l2649&var=610604236285&ssPageName=STRK%3AMEBIDX%3AIT), $5.43)
In addition to this we'll need a VGA monitor, a power supply and an audio amplifier + speakers (if we want sound). We'll also need an Arduino or something similar to drive the LED display (this is something I haven't worked with before so I'm not exactly sure what it will take). These are things that I have in the parts bin so I shouldn't need to purchase them for this build, but when the project is complete I'll post a full list of all the parts that went into it (including the cabinet) with sources in case you want to make one yourself. Like everything else the plans, design files, etc. will be open-source and can be found in their [Github repository](https://github.com/jjg/charityarcade).---
tags:
  - gullicksonlaboratories
title: Charity Arcade - The Final Chapter
link: http://jjg.2soc.net/2017/05/01/charity-arcade-the-final-chapter/
author: jgullickson
description: 
post_id: 2145
date: 2017-05-01 06:20:11
created_gmt: 2017-05-01 11:20:11
comment_status: open
post_name: charity-arcade-the-final-chapter
status: publish
post_type: post
---

# Charity Arcade - The Final Chapter

## Finishing

Painting begins T minus 3 days before the fundraiser. Electronics are removed and  front panels are pulled in preparation. Jamie begins working nonstop to get paint on pieces in a sequence that allows reassembly start before all the painting is complete. ![2017-04-26 17-44-47 0393](/wp/2017/04/2017-04-26-17-44-47-0393.jpg) I picked-up a nice power switch/fuse/socket to tidy-up the input side of the power supplies (and make tripping over the cord less disastrous). Also having a fuse is always a good idea. ![2017-04-24 17-54-43 0378](/wp/2017/04/2017-04-24-17-54-43-0378.jpg) About 24 hours later, control panel reassembly begins and touch-up paint is applied. The weather decides to be uncooperative so operations move from the garage to the livingroom. ![2017-04-27 19-34-49 0401](/wp/2017/04/2017-04-27-19-34-49-0401.jpg) Final assembly commences and breaths are held as power is finally applied. To everyone's relief, beeps are heard, backlights illuminate and the machine comes alive. ![2017-04-27 20-28-39 0404](/wp/2017/04/2017-04-27-20-28-39-0404.jpg) Now that everything is done, the question "_how do we get it there?_" makes its way to the front of the consciousness. Mike agrees to save the day. ![2017-04-28 11-20-54 0405](/wp/2017/04/2017-04-28-11-20-54-0405.jpg) The next day DONOR 1 makes its way successfully to the venue and to my surprise, weathers the ride with aplomb. A few quick test games with all systems nominal and we put the machine to rest until the following night. 

## Party Time

![2017-04-29 18-25-18 0411](/wp/2017/05/2017-04-29-18-25-18-0411.jpg) Overall, DONOR-1 faired well and the party was a success.  There were no technical problems and we received lots of complements about the machine from players and attendees. We also spoke to a number of people at the party who inquired about having DONOR-1 raise money for other events.  This has been something we've wanted to do all along, so it was exciting to hear that there was some interest in putting the machine to work elsewhere (I'll be sure to post here when it's going to make another appearance). 

## What's Next?

I've learned a lot since first putting pencil to paper designing this machine, some of which can be applied to improving DONOR-1 and some that will need to be used in the next iteration of the design. For now we're discussing how best to share DONOR-1 with others so it can continue to fulfill its mission as a fundraising machine. The long-term goal is to produce a "reference design" that we could use to create additional machines or could be used by anyone to build their own. Plans like that might take some time to put together (there are a lot of notes to sort through) but in the meantime if you are interested in building a machine like this (or would like to use DONOR-1 as a fundraising tool) [get in touch](http://jjg.2soc.net/contact/) and we'll share the information we have available even if it's not in "final draft" form. 

## Thanks

I want to take a moment to thank the team who made DONOR-1 possible. Given the array of skills needed to come up with the idea and turn my pencil sketch into a working machine on a very aggressive timeline, it would have been impossible to have done it alone (and a lot less fun!). Thank-you Jamie, Libby, Jim, Patti, Justin, Jameson and Alyssa for all your hard work and patience in making DONOR-1 a reality. Thank-you Jeff for technical support and input on the electronics, and thanks Mike for coming out on a moments notice to help us get the thing delivered. I also want to thank Kristin for providing a "totally rad" venue for DONOR-1's debut, and for enthusiasm and encouragement throughout the project.

## Comments

**[Justin Krosschell](#30 "2017-06-08 08:58:46"):** Sweet arcade machine. It's cool that you can share it with different charities in the area!

---
tags:
  - gullicksonlaboratories
title: Come Play Between the Lakes
link: http://jjg.2soc.net/2018/05/18/come-play-between-the-lakes/
author: jgullickson
description: 
post_id: 5177
date: 2018-05-18 10:31:39
created_gmt: 2018-05-18 15:31:39
comment_status: open
post_name: come-play-between-the-lakes
status: publish
post_type: post
---

# Come Play Between the Lakes

# Goodbye ReStore,

![2018-05-16 16.54.01](/wp/2018/05/2018-05-16-16-54-01.jpg) This week, DONOR-1's stay at the [Beaver Dam Habitat for Humanity ReStore](http://www.hfhrestore.org/) came to a close. We are very thankful of the ReStore for hosting DONOR-1 through the winter months, but now that the snow has finally melted it's time for a new adventure. 

# Hello Madison!

We've heard from a lot of people who want to check-out DONOR-1 but didn't find themselves near any the places who have hosted us so far. I'm very happy to announce that [Horizon Coworking](http://horizoncw.com/) has offered to host DONOR-1 in downtown Madison, Wisconsin! ![2018-05-16 18.45.17](/wp/2018/05/2018-05-16-18-45-17.jpg) Conveniently located right on the capital square, Horizon is accessible by foot, bike, bus and car and should make it easier for more people to get a chance to play and support some local organizations. Speaking of which, DONOR-1 will be raising funds for a number of Madison organizations during its time at Horizon, beginning with [Domestic Abuse Intervention Services](https://abuseintervention.org/) (DAiS). In addition to being a great place for DONOR-1 to live for awhile, we're noodling on new ways to collaborate with the folks at Horizon to explore new ideas we have for this machine as well as how we can find more ways to use technology to support work for social good. Horizon Coworking is located at **7 N Pinckney Street, Suite 300, Madison, WI**.  Take the front stairway (or central elevator) to the third floor and follow the hall toward the back of the building (or turn left when you exit the elevator).  You'll find DONOR-1 among the tables and couches lovingly referred to as the "Nerd Deck". Be sure to bring some quarters (50 cents per game)! Hope to see your initials on the scoreboard!---
tags:
  - gullicksonlaboratories
title: Commercial Innovation
link: http://jjg.2soc.net/2017/03/19/commercial-innovation/
author: jgullickson
description: 
post_id: 1084
date: 2017-03-19 07:39:27
created_gmt: 2017-03-19 12:39:27
comment_status: open
post_name: commercial-innovation
status: publish
post_type: post
---

# Commercial Innovation

Contrary to popular belief, the greatest thing Steve Jobs helped create was [NeXT Computer](https://en.wikipedia.org/wiki/NeXT), and it was a commercial failure. What does that tell you about commercialism? By chance I watched a video called "The Machine to Build the Machines". https://www.youtube.com/watch?v=dSj6kvv7_Sg _If the title sounds familiar it's because Elon Musk has been using a [similar phrase](https://www.youtube.com/watch?v=f9uveu-c5us) recently to refer to the Tesla Gigafactory._ This video (with it's perfectly-tuned 80's flair) was fun to watch but more interesting was this bit of a documentary about NeXT that showed up in the recommendations: https://www.youtube.com/watch?v=BNeXlJW70KQ If you've been involved in the "tech industry" and watch this video, you're going to hear a lot of problems that seem very familiar. Two things are What's most jarring about studying NeXT. The first is that 30 years ago they were experiencing the problems that most "innovative" technology companies are still struggling with today. The second is that their solution to these problems wasn't business or even technology related, but to return to the _meaning_ of what they are building. In the case of NeXT, Steve wanted to build a computer that could put the power of the most advanced university facilities (not just computing power but books, laboratories, etc.) in every student's dorm room. When things went sideways and compromises had to be made this objective was revisited and sacrifices were made only if they did not threaten this mission. It's heard in this video (and I've heard elsewhere as well) that if the company (NeXT) can't reach this goal it should go out of business. Ultimate that is what happened. It's easy to criticize NeXT from a business perspective, but I prefer to criticize business instead. Had NeXT "succeed" (in business terms) it would have benefited the stockholders and perhaps the employees, but at the cost of the vision Steve had for the company and its computer. The resulting system would have done far less to improve the lives of the people who bought those computers, and in turn the people who would benefit from the work of those who bought the computers. But what if _business_ would have been compromised instead? What if instead of judging innovation in terms of profit and loss it were judged in its value to improve the lives of people and the planet? Clearly the objectives that NeXT set out to achieve could have (and to some degree did) make a dramatic improvement to the lives of many people, both directly and indirectly. It's not hard to imagine that computers would have been more useful if every university student (and later, every student) had access to NeXT workstations (and the advanced programming tools that shipped with them) instead of Windows 3.11 or System 6 Macintoshes. After all, the [World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web#History) you're using to read this post was created on a NeXT by just such a person. If we are truly interested in progress, instead of recounting the business failures of technology companies, instead of searching for a way to make the development of technology compatible with business success, we should be finding a way to make business support technological breakthroughs.---
tags:
  - gullicksonlaboratories
title: Control 4 appliances for about $10 with Alexa
link: http://jjg.2soc.net/2017/06/12/control-4-appliances-for-about-10-with-alexa/
author: jgullickson
description: 
post_id: 1036
date: 2017-06-12 08:10:54
created_gmt: 2017-06-12 13:10:54
comment_status: open
post_name: control-4-appliances-for-about-10-with-alexa
status: publish
post_type: post
---

# Control 4 appliances for about $10 with Alexa

A while back I wrote about controlling an LED via Alexa by using an ESP8266 to emulate a switch that Alexa natively supports. This is interesting as a proof-of-concept, but not terribly useful. In this post I'll be applying the same concept to the [LinkNode R4](http://astore.amazon.com/jjg00-20/detail/B01FVJ8XSU) board which combines an ESP8266 with four relays capable of switching up to 10 amps at 125 volts AC. 

## Why buy? I thought this was DIY!

When I  originally started this post, the plan was to continue on from controlling an LED to adding a relay to the circuit and demonstrate controlling a more practical load (a lamp, fan, etc.), but while digging up links for the article, I stumbled-upon the LinkNode board. This board combines the ESP8266 with four relays and all necessary support circuitry into a neat, compact board which sells for less than $10 on Amazon. Based on component cost alone, I couldn't beat this price doing it myself. Also, there's something attractive about reducing my exposure to "mains current", especially in something I plan to use outside of the lab. Thus, I thought it was worthwhile to order one of these boards, see what it is like to work with and share my findings. Don't worry, since the LinkNode is based on the ESP8266 it has all the same open-source goodness of the [previous project](http://jjg.2soc.net/2017/02/21/open-source-home-automation-with-alexa/). 

## Hardware

The LinkNode board contains almost everything you need to start turning things on-and-off using Alexa hardware-wise (other than a power supply). However, what I have in mind requires a little bit more, including: 

  * A way to get AC power in
  * 4 ways to get AC power out (one for each relay)
  * A safe, reasonably attractive (or concealable) enclosure
  * A 5vdc power supply
Of these the power supply is the most frustrating part because I want to avoid using an external "wall wart"-style adapter. I'll already have AC power inside the case, so it would be cleaner to include the DC power supply inside and have only one "power in" connection outside the enclosure. On the other hand, including the power supply inside the case means the case has to be larger. Exactly how much larger is the question, because the power supply designs I'm familiar with are somewhat old-fashioned and involve large-ish transformers, lossy voltage regulators, etc. (perhaps this is a good opportunity for me to learn about contemporary power supply design). _Note: I had originally planned to include the enclosure and DC power supply design in this post, but since that is taking more time than expected. I'm going to leave that for a future article; for now let's move on to making the board do something._

## Prepare for Programming

To program the LinkNode board you'll need some sort of TTL to serial adapter. For this project I used a simple and cheap USB-to-TTL cable like [this one](http://astore.amazon.com/jjg00-20/detail/B00QT7LQ88). USB to TTL serial adapter pinout: Line sequence defined:Red +5V, Black GND, Green TXD, White RXD To connect this USB adapter to the LinkNode: 

USB TTY LinkNode

Black
G (ground)

White
TX

Green
RX
Set the board to "program via UART" mode: ![linknodejumper](/wp/2017/03/linknodejumper.png)

## Software

From the Tools menu of the Arduino IDE, select the following settings: 

  * Board: `Generic ESP8266 Module`
  * Flash Mode: `QIO`
Paste the code below into the editor: 
    
    
    #include 
    #include 
    #include "fauxmoESP.h"
    
    #define SWITCH_1        14   // top-left
    #define SWITCH_2        12   // top-right
    #define SWITCH_3        13   // bottom-left
    #define SWITCH_4        16   // bottom-right
    
    #define SERIAL_BAUDRATE 115200
    
    // Wifi credentials REMOVE BEFORE COMMITTING TO GITHUB!!!
    #define WIFI_SSID "change"
    #define WIFI_PASS "me"
    
    fauxmoESP fauxmo;
    
    // -----------------------------------------------------------------------------
    // Wifi
    // -----------------------------------------------------------------------------
    
    void wifiSetup() {
    
        // Set WIFI module to STA mode
        WiFi.mode(WIFI_STA);
    
        // Connect
        Serial.printf("[WIFI] Connecting to %s ", WIFI_SSID);
        WiFi.begin(WIFI_SSID, WIFI_PASS);
    
        // Wait
        while (WiFi.status() != WL_CONNECTED) {
            Serial.print(".");
            delay(100);
        }
        Serial.println();
    
        // Connected!
        Serial.printf("[WIFI] STATION Mode, SSID: %s, IP address: %s\n", WiFi.SSID().c_str(), WiFi.localIP().toString().c_str());
    
    }
    
    void setup() {
      
        // Init serial port and clean garbage
        Serial.begin(SERIAL_BAUDRATE);
        Serial.println();
        Serial.println();
    
        // Wifi
        wifiSetup();
    
        // Relays
        pinMode(SWITCH_1, OUTPUT);
        pinMode(SWITCH_2, OUTPUT);
        pinMode(SWITCH_3, OUTPUT);
        pinMode(SWITCH_4, OUTPUT);
    
        // Fauxmo
        fauxmo.addDevice("master bedroom lights");
        fauxmo.addDevice("master bedroom fan");
        fauxmo.addDevice("master bedroom humidifier");
        fauxmo.addDevice("master bedroom other");
        
        // fauxmoESP 2.0.0 has changed the callback signature to add the device_id, this WARRANTY
        // it's easier to match devices to action without having to compare strings.
        fauxmo.onMessage([](unsigned char device_id, const char * device_name, bool state) {
            Serial.printf("[MAIN] Device #%d (%s) state: %s\n", device_id, device_name, state ? "ON" : "OFF");
    
            switch(device_id){
              case 0: 
                digitalWrite(SWITCH_1, state);
                break;
              case 1:
                digitalWrite(SWITCH_2, state);
                break;
              case 2:
                digitalWrite(SWITCH_3, state);
                break;
              case 3:
                digitalWrite(SWITCH_4, state);
                break;
              default:
                Serial.printf("Unhandled device #%d\n", device_id);
                break;
            }
        });
    }
    
    void loop() {
    
        fauxmo.handle();
    
        static unsigned long last = millis();
        if (millis() - last > 5000) {
            last = millis();
            Serial.printf("[MAIN] Free heap: %d bytes\n", ESP.getFreeHeap());
        }
    }
    
    

Now customize the code to your liking: 

  * Replace `change` and `me` with the SSID and password for your WiFi network
  * Replace "master bedroom lights", "master bedroom fan", etc. with the name you will use when speaking to Alexa. i.e., if you want to say "Alexa, turn on bedroom lights" replace "switch one" with "bedroom lights".
Save the file and click the "Compile" button. If the code compiles without errors, click "Upload" to install the firmware on the LinkNode board. If you get errors when you compile the code, double-check the changes you made above and make sure you've installed the prerequisites. 

## Testing

Once the firmware is uploaded, unplug the power cable from the board and move the jumper to the "boot from flash" position as shown in the image above. In the Arduino IDE, select Tools -> Serial Monitor and make sure the baud rate (the drop-down lower left-hand corner of the monitor window) is set to `115200 baud`. Finally, plug the power cable back into the LinkNode board. You should see some strange characters and then a message that says `Connecting to yourssid......` (Where "yourssid" is the SSID of your Wifi network) After a few seconds the board should connect and display: `[WIFI] STATION Mode, SSID: yourssid, IP address: 111.111.111.111` (Where "111.111.111.111" is a valid IP address for your network) If the board doesn't connect to your WiFi network, make sure it is in range (it has a small antenna) and double-check the SSID and password you set in the code above.

## Comments

**[Alan (@Alan_667)](#57 "2017-09-04 11:11:21"):** Awesome...

---
tags:
  - gullicksonlaboratories
  - rain
title: Corrections
link: http://jjg.2soc.net/2018/06/27/corrections/
author: jgullickson
description: 
post_id: 5208
date: 2018-06-27 14:02:39
created_gmt: 2018-06-27 19:02:39
comment_status: open
post_name: corrections
status: publish
post_type: post
---

# Corrections

There were a number of errors in my last post. 

# Error #1

The most egregious was in my interpretation of the high-performance Linpack (hpl) results. I was mystified by the fact that as I added nodes to the cluster performance would improve to a point and then suddenly drop-off. I attributed this to some misconfiguration of the test or perhaps a bottleneck in the interconnect, etc. In the back of my mind I had this nagging idea that there was some sort of "order-of-magnitude" error but I couldn't see it. Then when I was reading a paper by computer scientist who was writing about running hpl it jumped out at me. The author said they measured 41.77 Tflops, but when I looked at their hpl output it looked like 4.1 Tflops... Then I realized my mistake. [caption id="attachment_5209" align="aligncenter" width="807"]![tflops](/wp/2018/06/tflops.png) Taken from "[HowTo - High Performance Linpack (HPL)](http://www.crc.nd.edu/~rich/CRC_Summer_Scholars_2014/HPL-HowTo.pdf)" by Mohamad Sindi[/caption] There's a few extra characters tacked-on to the Gflops number at the end of hpl's output: 

## 1.826**e+01**

When I first started using hpl, that last bit was always **e+00** so I didn't pay any attention to it. However at some point that number went from **e+00** to **e+01** without me noticing, and it turns out that change was significant. Since I didn't notice this, I was mis-reading my results and doing a _lot_ of troubleshooting to try and figure out why my performance dropped through the floor after I added a fifth node to the cluster. Once I realized the mistake I went back and reviewed the Mark I logs and sure enough, that mistake sent me on a wild goose chase and resulted in a seriously erroneous assessment of Mark I's performance. The good news is that this means Mark II is considerably faster than I thought it was when I wrote about it earlier. The bad news is that this means Mark I is _also_ faster, and my conclusion that Mark II was exceeding Mark I's performance was incorrect. Knowing what I know now, Mark I was roughly 4x faster than Mark II in the 4x4 configuration. Performance parity between the two systems seemed too good to be true, so in a way this is a relief. It's a little disappointing, but this doesn't undermine the value of Mark II because in terms of physical size, cost and power consumption Mark II easily out-paces Mark I. I don't have detailed power consumption measurements for either system yet, but if we compare their theoretical maximum power consumption, it's pretty clear that in terms of flop-per-watt, Mark II is an improvement: 

  * Mark I: 40 Gflops, 4,000 watts (800 watts * 5 chassis) = 100 watts per Gflop
  * Mark II: 18 Gflops, 75 watts (15 amps @ 5 volts * 1 chassis) = 4 watts per Gflop

# Error #2

The second mistake I made related to the clock speed of Mark II's compute nodes. The SOPINE module's maximum clock speed is 1.2Ghz and this is what I used when determining the theoretical peak (Rpeak) and efficiency values for the machine. I knew that this speed might be reduced during the test due to inadequate cooling (cpu scaling) but I thought it was a reasonable starting point. Based on this I was disappointed to find that no matter how hard I tweaked the hpl configuration, I couldn't break 25% efficiency. I even added an external fan to see if added cooling could move the needle but it seemed to have no effect. I took a closer look at one of the compute nodes during a test run and this is when the real problem began to become apparent. I wasn't able to find the CPU temps in the usual places, and I couldn't find the CPU clock speed either. I had read something about issues like this being related to kernel versions so I looked into what kernel the compute notes were running. As it turns out they are running the "mainline" kernel, and the kernel notes on the [Armbian website](https://www.armbian.com/pine64/) state that cpu scaling isn't supported in this kernel. This means the OS can't slow-down the CPU if it gets too hot, so the clock is locked at a very low speed for safety's sake. From what I can tell that speed is 408Mhz, around 1/3 of the speed I expected the CPU's to be spinning at. Using this clock speed to calculate the efficiency, my hpl results look a _lot_ better: ![hpl-mainline-kernel-chart](/wp/2018/06/hpl-mainline-kernel-chart.png)

# What's next?

With these errors corrected I was able to execute a series of tests and confirm that Mark II's system efficiency is well within acceptable values. This means that if I address the clock speed issue, hitting 50 Gflops should not be unreasonable (that's only about 75% efficient, and the worst efficiency in the last round of tests was 80% or better). ![mark_ii_ganglia_full_power](/wp/2018/06/mark_ii_ganglia_full_power.png) Getting the clock speed up to max (and keeping it there) will require cooling, so I've picked-up some heatsinks for the compute module SOC's and I'll be turning my attention to completing the front-end interface hardware (which includes dynamic active cooling). Once I have that in place I'll turn my attention back to performance tuning and see how far we can go. One other side-node, the results of the last batch of tests indicate that there's room to improve performance reasonably by adding nodes beyond the 8 that one Mark II chassis can contain. It might be interesting to assemble a second chassis to see how linear performance scales across more nodes. Since I've heard some interest in developing an "attached processor"-style chassis for the Clusterboard this might be something worth exploring.
---
tags:
  - gullicksonlaboratories
title: Current status 
link: http://jjg.2soc.net/2017/05/13/current-status/
author: jgullickson
description: 
post_id: 2470
date: 2017-05-13 20:58:10
created_gmt: 2017-05-14 01:58:10
comment_status: open
post_name: current-status
status: publish
post_type: post
---

# Current status 

![](/wp/2017/05/img_0564.jpg)---
tags:
  - gullicksonlaboratories
title: DONOR-1 - Beaver Dam ReStore
link: http://jjg.2soc.net/2017/12/13/donor-1-beaver-dam-restore/
author: jgullickson
description: 
post_id: 5041
date: 2017-12-13 14:31:41
created_gmt: 2017-12-13 20:31:41
comment_status: open
post_name: donor-1-beaver-dam-restore
status: publish
post_type: post
---

# DONOR-1 - Beaver Dam ReStore

DONOR-1 will be spending the snowy months cozily entertaining customers (and hopefully not annoying employees too much) at the [Beaver Dam Habitat for Humanity ReStore](http://hfhrestore.org/). ![25152296_814543168728040_6782820194895214702_n](/wp/2017/12/25152296_814543168728040_6782820194895214702_n.jpg) DONOR-1 arrived at the ReStore last week (during the first significant snowfall) but unfortunately decided to act-up.  After bringing the control panel back to the laboratory, it turned out the monitor had reset itself to factory settings and was going into power-save mode after 15 minutes or so. ![](/wp/2017/12/img_1900.jpg) After a little debugging and a lot of testing, the panel was reassembled and returned to the rest of the cabinet.  After a quick game of Time Pilot the field testing was complete and DONOR-1 was ready for business. If you are in the area be sure to stop in, drop a few quarters and check out the store! Funds collected by DONOR-1 will be donated to Habitat for Humanity to support their mission of providing shelter and self-reliance (as well as the side-effect of operating an excellent place to get parts for projects!).---
tags:
  - gullicksonlaboratories
title: DONOR-1 DOWN!
link: http://jjg.2soc.net/2017/06/08/donor-1-down/
author: jgullickson
description: 
post_id: 2593
date: 2017-06-08 08:24:21
created_gmt: 2017-06-08 13:24:21
comment_status: open
post_name: donor-1-down
status: publish
post_type: post
---

# DONOR-1 DOWN!

DONOR-1 is on the road again! This time it's headed to [The Idea Studio](http://www.fdlpl.org/ideastudio) at the [Fond du Lac Public Library](http://www.fdlpl.org/). We're very excited to have The Idea Studio hosting DONOR-1, however after it arrived at the Studio, we received a troubled message from Josh. ![2017-06-06 20-52-11 0791](/wp/2017/06/2017-06-06-20-52-11-0791.jpg) Josh said the machine fired-up and for the most part appeared to work, but there were some intermittent problems with the controls. This wasn't a complete surprise, based on how quickly we had to wrap the project up I was more surprised that it hadn't run into trouble earlier. Also, this was the first time that DONOR-1 had traveled at "highway speeds" and experienced the associated vibration, shock and other forces which tend to liberate mechanical connections. In any case this gave us an excuse to take a short drive to beautiful [Fond du Lac](https://www.fdl.wi.gov/), Wisconsin and see what could be done about it. ![2017-06-07 16-50-40 0795](/wp/2017/06/2017-06-07-16-50-40-0795.jpg) I was a little worried that something more serious than a loose connection might be going on, and I was really hoping it wouldn't involve ordering parts. It would be a real bummer to have the machine at the Studio with an "out of order" sign on it for a few weeks while we waited for a component shipment. ![2017-06-07 16-44-17 0792](/wp/2017/06/2017-06-07-16-44-17-0792.jpg) Fortunately Occam's Razor was right and after snugging-up a few connectors the hardware diagnostics passed with flying colors and DONOR-1 was once again back in service. After a couple games of Mr. & Mrs Pac-Man (for testing of course) the cabinet was screwed back together and we were on our way. ![2017-06-07 19-23-42 0799](/wp/2017/06/2017-06-07-19-23-42-0799.jpg) Minimally, DONOR-1 will be at The Idea Studio through July (perhaps longer depending on what the next stop of the tour turns out to be).  If you'd like to see it we highly recommend visiting and checking out all of the [cool stuff](http://www.fdlpl.org/pdfs/IdeaStudioEQUIPMENT_6.1.2017.pdf) The Idea Studio has to offer. The Studio is located in the Public Library (which itself is excellent) and is situated downtown surrounded by lots of great restaurants and interesting shops. There's also a lot of other great stuff to do in Fond Du Lac, so if you want to try out DONOR-1, doing so while it's at The Idea Studio would make a great afternoon or day trip, so check it out while you can.---
tags:
  - gullicksonlaboratories
title: DONOR-1 Hits The Road
link: http://jjg.2soc.net/2017/05/03/donor-1-hits-the-road/
author: jgullickson
description: 
post_id: 2281
date: 2017-05-03 09:57:04
created_gmt: 2017-05-03 14:57:04
comment_status: open
post_name: donor-1-hits-the-road
status: publish
post_type: post
---

# DONOR-1 Hits The Road

First stop is the [Dodge County YMCA](http://www.theydc.org/). DONOR-1 will be appearing in the lobby through the month of May, 2017. Fifty-cents a game, come on down and take it for a spin! [caption id="attachment_2290" align="aligncenter" width="2448"]![2017-05-03 09-05-48 0490](/wp/2017/05/2017-05-03-09-05-48-0490.jpg) Lines are forming already...[/caption] If you can't make it to the YMCA we're lining up additional events now. I'll post times and locations as the information becomes available.---
tags:
  - gullicksonlaboratories
title: DONOR-2 Springs to Life
link: http://jjg.2soc.net/2017/09/18/donor-2-springs-to-life/
author: jgullickson
description: 
post_id: 4689
date: 2017-09-18 16:48:41
created_gmt: 2017-09-18 21:48:41
comment_status: open
post_name: donor-2-springs-to-life
status: publish
post_type: post
---

# DONOR-2 Springs to Life

A lot has happened in the last two weeks. After six iterations of corner designs, the display is securely attached. ![IMAG0056](/wp/2017/09/imag0056.jpg) The top of and the bottom of the cabinet are painted and overall it's starting to take its final form. ![IMG_1414](/wp/2017/09/img_1414.jpg) Starting with a table saved us a lot of cabinet-building time, but we need a place to put the player contrls so there was still some carpentry to do.   [gallery ids="4708,4709,4710,4711,4712,4713,4714,4715,4716,4721"] With the control boxes completed it was time for _my_ favorite part, the electronics. ![IMG_6130](/wp/2017/09/img_6130.jpg) I made some assumptions about the electronics hardware based on the fact that were are using a JAMMA-based module as we did with DONOR-1. However, the new module has a few significant differences (other than a **lot** more games). The first difference is that it doesn't have a Molex-style connector for the power supply. Another difference is that there's no DIP switches to configure things like the display orientation. Luckily neither of these turned out to be a problem. I was able to use he power lines on the JAMMA connector with the power supply I had picked from the parts bin, and a few minor modifications to the corner clamps allowed me to flip the screen to work with the fixed orientation. One _helpful_ difference is the fact that the board not only has a built-in audio amplifier but also a physical volume control (instead of software-controlled volume like DONOR-1). I didn't think the new board had an amp built-in so I thought that would be an additonal piece required for this build. One less thing to find a home (and power) for! Wiring the JAMMA harness to the controls can be tedious because the documentation isn't great (just getting the orientation of the connector right can be difficult), and there's not enough colors to make every wire a unique one. I thought this would be doubly-hard this time around because we have two sets of controls. However, it turned out that the harness was wired cleverly in that the same color wires are used for both sets of controls. This doesn't help much with the first controller but it lets you use the first controller as a reference when wiring the second. This makes wiring the second controller a very quick and error-free job. ...assuming you wire the first controller correctly... Once all the necissary connections were made it was time to apply power and pray to the blue smoke gods to stay home today. The fates were on our side and the display lit up with no indication of fatal-levels of wiring mistakes. ![IMG_6132](/wp/2017/09/img_6132.jpg) There was one small error which reversed the up and down direction of both joysticks, but a quick wire swap and all was in order. ![IMG_6135](/wp/2017/09/img_6135.jpg) At this point the system is playable, but there is more work to do. The most obvious missing piece is a place to mount the 1 & 2 player start buttons. There wasn't enough room to cram them into the control boxes, so we'll need to find a different place to mount them. We also need to replace the temporary speaker with something more permanent (and more securely mounted), clean-up the internal wiring and mount the AC power switch/input/fuse plate. There's also additional design work to do and we still haven't decided exactly how we want it to accept donations yet... ![21731092_1844048105611517_5545361067476921529_n](/wp/2017/09/21731092_1844048105611517_5545361067476921529_n.jpg) Still, there's been a lot of progress in only a couple weeks (and really only a few working days in that time). We're still learning, but the experience of building DONOR-1 has definately had an impact on how well this build is going, even if it's very different from the first.---
tags:
  - gullicksonlaboratories
title: DONOR-2 - This time it's portable
link: http://jjg.2soc.net/2017/09/04/donor-2-this-time-its-portable/
author: jgullickson
description: 
post_id: 4593
date: 2017-09-04 10:53:03
created_gmt: 2017-09-04 15:53:03
comment_status: open
post_name: donor-2-this-time-its-portable
status: publish
post_type: post
---

# DONOR-2 - This time it's portable

Over the last few months we've slowly been accumulating parts for the next charity arcade machine DONOR-2. Applying what we've learned from [DONOR-1](https://jjg.2soc.net/category/charity-arcade/page/2/), we came up with a few design criteria: 

### Improve portability

Something that doesn't require a full-sized pick-up to transport. Ideally something that can be transported by one person. 

### Cabinet simplicity

Most of the time spent on DONOR-1 was fabricating the cabinet. Designing something simpler (or using an existing cabinet) would make building these a lot faster & easier. 

### Smaller "vault"

The amount of DONOR-1's cabinet that is dedicated to holding quarters is so large that it takes a lot of games before the donations are even _visible_. If it were to be filled to capacity, it would weigh so much that I don't think it would be movable without a forklift. This makes sense for long-term (year or years?) installations but for the venues we've been working with something with 1/10th the capacity would be more than enough. 

### Multi-player

It's more fun to be able to play with/against friends, and there's a lot of games which kind of require more than one player. 

## Components

So far we've tracked down a few of these things and recently started putting them together. 

### Cabinet

[caption id="attachment_4653" align="aligncenter" width="1351"]![IMAG0031](/wp/2017/09/imag0031-e1504539446977.jpg) Jamie painting the cabinet[/caption] With the criteria above in mind, Jamie went on the hunt for a cabinet and found a great candidate. Not only was it inexpensive ($2.00!) but it's a recycled end-table which means one less piece of furniture that was likely to become trash. [caption id="attachment_4674" align="aligncenter" width="1520"]![IMAG0033](/wp/2017/09/imag0033.jpg) amazing what a difference some paint (in the right hands) makes[/caption] 

### Display

[caption id="attachment_4657" align="aligncenter" width="2688"]![IMAG0029](/wp/2017/09/imag0029.jpg) sizing things up for the first cut[/caption] DONOR-1 used a display from my parts bin but since that was my last VGA monitor I needed to go shopping to find a screen for DONOR-2. Based on the measurements of the cabinet Jamie found I thought we could use a 19" monitor this time (DONOR-1's display is 17"). I figured this might be the most expensive component of the build but after a trip to [UW SWAP](https://swap.wisc.edu/) I had a very nice 19" LCD monitor for a mere $20.00. [caption id="attachment_4678" align="aligncenter" width="2688"]![IMAG0038](/wp/2017/09/imag0038.jpg) not perfect, but sooo much better than last time[/caption] 

### Brains

DONOR-1 was originally designed for an 80's-themed fund-raiser and as such the 60 super retro games were a perfect fit. However this didn't include [Jamie's favorite arcade game](http://retro-heart.blogspot.com/2012/06/x-men-six-player-arcade-cabinet-model.html) so this time around she made sure we wouldn't make the same mistake with DONOR-2. The result is that DONOR-2 will be able to play [645 different games](https://www.jammaparts.net/pandora-s-box-645-1/143-horizontal-pandora-s-box-4-645-in-1.html), (including X-Men). 

### The rest

There's still a few things undecided about DONOR-2. A common complaint about DONOR-1 is that it only accepts quarters, so we're looking into options to allow DONOR-2 to accept other forms of currency. Another issue with the design of DONOR-1 is that serviceability is a bit of a hassle and requires tools, so this is another area we're looking to improve with DONOR-2. One major design task that remains is the housing for game controls. The current plan is to build boxes containing the controls which will then be attached to the sides of the cabinet but the exact nature of the design, construction and materials for these boxes is still undecided. This will become more clear when the control hardware arrives and we can begin to experiment with layout, etc.---
tags:
  - gullicksonlaboratories
title: Experimenting with FORTH
link: http://jjg.2soc.net/2017/08/28/experimenting-with-forth/
author: jgullickson
description: 
post_id: 4476
date: 2017-08-28 19:43:24
created_gmt: 2017-08-29 00:43:24
comment_status: open
post_name: experimenting-with-forth
status: publish
post_type: post
---

# Experimenting with FORTH

I've come across the [FORTH programming language](https://en.wikipedia.org/wiki/Forth_\(programming_language\)) a few times in the past (notably as the foundation one of my [favorite personal computers](https://en.wikipedia.org/wiki/Canon_Cat)) and while it seemed interesting, I couldn't come up with an application for it that justified the time I'd need to invest learning it. More recently I came across a [series of articles on Hackaday](http://hackaday.com/?s=forth) centered around running FORTH on [STM32](https://en.wikipedia.org/wiki/STM32)-based [microcontrollers](https://en.wikipedia.org/wiki/Microcontroller). This intrigued me because I've been on the lookout for a more "interactive" programming environment for microcontroller work (vs. the typical code-build-flash-test-repeat cycle). The ability write code directly on the controller itself and tweak a system build around one _in situ_ really clicks with the way I like to work and would speed up my experiments considerably. It also makes the whole development system more portable since all you need is a serial terminal. I had an STM32 board on-hand from some some (apparently unfinished) project but in order to put FORTH on it I would need a [programmer](https://www.amazon.com/Logisaf-ST-Link-Emulator-Downloader-Programming/dp/B01N79YDJE/ref=sr_1_3?ie=UTF8&qid=1503967201&sr=8-3&keywords=stm32+programmer). They're not very expensive, but it's kind of a special-purpose thing and not something I had in the toolbox. Once that came I was able to get FORTH flashed to the STM32 in about 30 minutes (there's some tooling that needs to be downloaded, compiled, etc.). Then I realized I'd need to solder on some headers before I could talk to it via serial, so that was another delay. However, after I found a few minutes to solder some headers on I coupled the STM32 with a slick serial-to-usb interface I picked up on [Tindie](https://www.tindie.com/products/Earth_People_Technology/ftdi-usb-to-serial-breakout-board-the-visiport2/) and snapped it all together on a small breadboard. ![IMAG0010](/wp/2017/08/imag0010.jpg) After figuring out the right minicom settings (seems like I'm away from it just long enough to forget how to use it) I was having a conversation in FORTH with the chip. ![IMAG0012](/wp/2017/08/imag0012.jpg) I'm really excited to play with this setup. In a way it's a lot like the 8-bit micros I grew up with; you turn it on, you get a prompt and you start writing programs. I have an application in mind for this which has been on the back burner because it requires a microcontroller and a lot of fiddly experimentation. If I can get my head around the language, I think FORTH on the STM32 might be the perfect tool for that job.---
tags:
  - gullicksonlaboratories
title: FindDay #0
link: http://jjg.2soc.net/2017/02/24/findday-0/
author: jgullickson
description: 
post_id: 157
date: 2017-02-24 07:36:47
created_gmt: 2017-02-24 13:36:47
comment_status: open
post_name: findday-0
status: publish
post_type: post
---

# FindDay #0

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Right to Repair bills

<http://repair.org/fair-repair/> Several U.S. states are working to pass legislation that protects the right of customers to repair the products they own.  This is a cause that is near to my heart for many reasons. If you live in one of these states I hope you'll do what you can to make this happen and set a precedent for the rest of the union. 

## 2 - lowRISC

<http://www.lowrisc.org/> As it says on the website, "A fully open-sourced, Linux-capable System-on-a-Chip".  It's easy to run an open-source operating system on your personal computer and know (either by reading the source yourself or trusting someone who has) that it's not doing something nasty like sending your personal data somewhere you don't want it.  However it's not as easy to know what the hardware the system is running is up to, and in the case of [Intel processors](https://en.wikipedia.org/wiki/Intel_Active_Management_Technology) it could certainly be doing things you didn't ask it to do. The lowRISC project is working on solving part of this problem by implementing a System On A Chip (think [Raspberry Pi](http://astore.amazon.com/jjg00-20/detail/B01CD5VC92) ) using open-source code.  This code can generate a physical CPU using an [FPGA](https://en.wikipedia.org/wiki/Field-programmable_gate_array) chip, which means you have a processor capable of running Linux that is open-source all the way down to the chips. The project is still young and there are other peripherals (network, graphics, etc.) that will need an open-source implementation to build computers that are completely immune to black-box code, but this is a major step in the right direction. 

## 3 - Free NASA Sounds

<https://m.soundcloud.com/nasa> What more can I say, this is an amazing collection of audio from NASA that they have made available for free.  I'm disappointed that they used Soundcloud for this because Soundcloud makes you jump through hoops to get at the files, but it's better than nothing. 

## 4 - Arduino Altair simulator

<https://create.arduino.cc/projecthub/david-hansel/arduino-altair-8800-simulator-3594a6> ![img_0189_1ivxnf92fd](/wp/2017/02/img_0189_1ivxnf92fd.jpg) The Altair is one of the most iconic machines of the personal computer revolution (is there a cooler user interface than the front panel of the Altair?  I think not).  However owning an original one today is an expensive proposition both in terms of the cost and the likely restoration. If you want the Altair experience but you have more sweat than cash, this simulator can get you there for a lot less than a restored classic (or even a [commercially-available reproduction](http://altairclone.com/) ).  It will also give your soldering skills a workout. 

## 5 - Swarm user interface

<http://shape.stanford.edu/research/swarm/> To me this is straight out of science fiction.  These little robots ("Zooids") swarm to build physical interfaces which can then be manipulated to software, devices, etc. https://youtu.be/8Ik7V_QH5wk The design is simple and elegant and also open-source.  This is the kind of technology that might not seem to have an immediate practical application, but once you think about the possibilities all sorts of interesting applications come to mind.  It's brain candy. Have something you'd like to be featured on FindDay?  [Drop me a line](http://jjg.2soc.net/contact/).---
tags:
  - gullicksonlaboratories
title: FindDay #10
link: http://jjg.2soc.net/2017/05/12/findday-10/
author: jgullickson
description: 
post_id: 2339
date: 2017-05-12 08:49:21
created_gmt: 2017-05-12 13:49:21
comment_status: open
post_name: findday-10
status: publish
post_type: post
---

# FindDay #10

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - A sub-$100 Open-Source Linux laptop

<https://www.pine64.org/?page_id=3707> I've been on the lookout for a great Linux laptop for decades. A few years back I resigned myself to believe that the only way I would be able to have an open-source laptop designed for Linux would be to [design & build one myself](https://github.com/jjg/offgrid). That may still be true, but the Pinebook is definitely a step in the right direction. I started following the Pinebook about a year ago and signed-up to be on the pre-order list with pretty low expectations (most of the devices like this that I sign up for never make it to market). Not only is the Pinebook now shipping, it appears to actually work as well. A writer at [Hackaday](http://hackaday.com/2017/04/28/hands-on-with-the-pinebook/) got their hands on one and shared some details a week or so ago. My number came up and I've placed an order for the 14" version. I still have pretty low expectations, but based on what I've seen so far, it might be just the ticket to give me something to work with until I finally get around to building my own machine... 

## 2 - 600W 3D Printed Brushless Motor

<https://www.instructables.com/id/600-Watt-3d-printed-Halbach-Array-Brushless-DC-Ele> ![f5a400rj1gow6tr-medium](/wp/2017/05/f5a400rj1gow6tr-medium.jpg) This is a very impressive project whose result is a very powerful and useful component. This is a good example of something that would be a lot more difficult to do with traditional tools, and proof that real, useful things can be made with consumer-grade 3D printing technology. 

## 3 - Owntracks

<http://owntracks.org> ![ipad-public-map](/wp/2017/05/ipad-public-map.png) I love being able to passively share my location with people I trust, but I hate sharing it with companies who want to turn me into a product (or worse). Owntracks provides location tracking capability without the baggage. Owntracks provides open-source applications for iOS and Android which monitor device location and publish location data to a server where it can be consumed by various pieces of software. The beautiful thing is that _you_ control the destination server, and the mobile apps provide a large amount of customization so you can get what you want (and nothing you don't want) out of them. Additionally the system uses standard protocols and transports so it's very easy to interface into other systems. This opens the door to a wide array of applications that are not constrained by what other proprietary systems offer. I've setup an experimental server and I'm just beginning to learn how to put it to use, but as the experiments continue I'll be posting more detailed information about the project in the future. (via [Linux Voice Podcast](https://www.linuxvoice.com/category/podcasts/) ) 

## 4 - Print your own housekeeper

<https://www.instructables.com/id/Build-Your-Own-Vacuum-Robot/> ![f4fhuwdj26ezkkh-medium](/wp/2017/05/f4fhuwdj26ezkkh-medium.jpg) I've owned a Roomba before and I really loved it, but it was destroyed through an interaction with pet waste that I'd rather not discuss further. Replacing it is too expensive to justify, but this project provides an alternative that is both less expensive and more fun than buying an off-the-shelf unit. As the design is open-source, it could even be improved to avoid the fate of my original Roomba.. 

## 5 - Parallel Python on Parallella

<https://github.com/parallella/parallella-examples/tree/master/epython> If you're not familiar with the [Parallella](https://www.parallella.org/board/) board, it's like a Raspberry Pi with a supercomputer crammed into it. Thanks to the Epiphany chip, you can get a 18-core (16 Epiphany cores + 1 ARM) in a pocket-sized board for about $100. The Parallella is an amazingly powerful device, but for a long time it was also amazingly hard to make the most of it; ePython changes that. Through a [series of blog posts](https://www.parallella.org/author/nbrown/) on the Parallella website, you can learn how to take advantage of the power of the Epiphany processor gradually through series of examples using familiar Python structures.---
tags:
  - gullicksonlaboratories
title: FindDay #11
link: http://jjg.2soc.net/2017/05/19/findday-11/
author: jgullickson
description: 
post_id: 2480
date: 2017-05-19 09:00:24
created_gmt: 2017-05-19 14:00:24
comment_status: open
post_name: findday-11
status: publish
post_type: post
---

# FindDay #11

No regular FindDay this week due to vacation.  Instead﻿, here's a few snapshots from the [American Computer and Robotics Museum](http://compustory.com) in Bozeman. ![](/wp/2017/05/img_0713.jpg)![](/wp/2017/05/img_0714.jpg)![](/wp/2017/05/img_0716.jpg)![](/wp/2017/05/img_0710.jpg)![](/wp/2017/05/img_0708.jpg)![](/wp/2017/05/img_0702.jpg)![](/wp/2017/05/img_0700.jpg)![](/wp/2017/05/img_0698.jpg)![](/wp/2017/05/img_0696.jpg)---
tags:
  - gullicksonlaboratories
title: FindDay 12
link: http://jjg.2soc.net/2017/06/02/findday-12/
author: jgullickson
description: 
post_id: 2493
date: 2017-06-02 09:20:26
created_gmt: 2017-06-02 14:20:26
comment_status: open
post_name: findday-12
status: publish
post_type: post
---

# FindDay 12

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Zerobot

<https://hackaday.io/project/25092-zerobot-raspberry-pi-zero-fpv-robot> https://www.youtube.com/watch?v=16rxNzQvfYY Zerobot is a cool little telepresense robot reminiscent of my own [Stepbot project](https://jjg.2soc.net/category/stepbot/); focused on creating a simple platform for experimentation which can be built from minimal electronics and 3D-printable parts. via [Hackaday](https://hackaday.com)   

## 2 - Musk gives up on Trump

<http://www.teslarati.com/musk-leaves-trumps-council-potus-exits-paris-agreement/> It's too bad that it took such a catastrophic mistake for Elon to realize getting Trump to do the right thing is hopeless, but now he can spend more of his time working with people who actually want to make things better. via [Teslarati](http://www.teslarati.com)   

## 3 - Claw Machine

<http://rasterweb.net/raster/2017/05/31/claw-machine-version-1/> https://www.youtube.com/watch?v=M_KzGlPLHn8 My friend [Pete](http://rasterweb.net) has created another curious art/engineering piece called Claw Machine. It's not what you think, it's a lot more interesting that what you think... via [RasterWeb](http://rasterweb.net)   

## 4 - Metalimbs

<https://blog.adafruit.com/2017/05/31/metalimbs-gives-you-an-extra-pair-of-robotic-hands-wearablewednesday/> https://www.youtube.com/watch?v=sKjAp0iZ7dc I often tell people one of the things I'd love to have are prosthetic tentacles (not unlike Doctor Octopus). There are so many times that I could use extra hands while working on a project, and if they were more durable (and didn't experience pain) than my human hands, all the better. Metalimbs is on the same page. via [Adafruit](https://blog.adafruit.com)   

## 5 - Endless OS and Endless Computers

<http://www.linux-magazine.com/Issues/2017/198/Endless-OS> ![computers_endlessmini2x](/wp/2017/06/computers_endlessmini2x.png) I'm very excited about this particular find. Endless is an attempt to answer the question "How do you make a modern computer useful without a connection to the Internet?". This might sound like a dumb question but if you were to buy a computer today, take it out of the box and try to use it without an Internet connection, you probably wouldn't make it past the setup screens. If you did, you wouldn't have much to work with in terms of tools and you would almost certainly have no access to documentation, reference information or other materials that you could use to learn how to make the most out of the machine you just bought. This is a problem I've spent a lot of time thinking about myself, and I've experimented with a number of solutions, but I wasn't sure if anyone else felt the same. That's part of the reason I was so excited to come across the kindred spirits at Endless. The [Linux Magazine article](http://www.linux-magazine.com/Issues/2017/198/Endless-OS) does a good job elaborating on this point so I won't belabor it here. The [software](https://endlessos.com/download/) that makes this possible is free, and can be downloaded and installed on almost any computer to get access to the tools and features described in the linked article above. But beyond the software, Endless is a [hardware company](https://endlessos.com/our-computers/) as well, and produces a number of inexpensive and attractive computers which ship with EndlessOS installed. ![computers_missionone2x](/wp/2017/06/computers_missionone2x.png) Unlike a Mac, PC or even most Linux machines, you can unbox an Endless computer and start doing things immediately, without an Internet connection, without installing updates, without downloading software, etc. Not only do you have the tools but you also have access to content and documentation to learn about almost anything. The target audience for Endless computers are parts of the world where Internet access is expensive or unobtainable, but I think it has application in many other places as well. We often conflate the roles of information device and communications device in modern computers with communications usually taking the dominant role. However, this wasn't always the case, and in the much-less-connected dawn of the personal computer, many of us spent countless hours learning how to use the machine, how to make the most out of the software and how to create new software of our own. The Endless machines can provide this same experience, with the convenience of having the documentation in lighter, searchable form. I believe that the use of these systems could lead to an increase in the creative application of computers in addition to making computing accessible to unserved communities. By decoupling communications from the computer experience, not only does this open up the possibility of working from anywhere, but it also eliminates much of the distractions that come with trying to work and learn using an "always connected" machine. Combined with a push-oriented data source like [Outernet](https://outernet.is/), it's possible to imagine a stand-alone computer with up-to-date information without the baggage associated with full-time connectivity. This seems like an ideal solution not only for "primary" education or communities unserved by technology but anyone who wants to use computers to learn, to create or to explore the world of knowledge without the advertising, exploitation and idle chatter that is now a fixed part of the online experience. I'm very excited about what the Endless team is working on, and I think there are applications beyond what even they have imagined. via [Linux Magazine](http://www.linux-magazine.com)---
tags:
  - gullicksonlaboratories
title: FindDay # 13
link: http://jjg.2soc.net/2017/06/09/findday-13/
author: jgullickson
description: 
post_id: 2663
date: 2017-06-09 09:30:35
created_gmt: 2017-06-09 14:30:35
comment_status: open
post_name: findday-13
status: publish
post_type: post
---

# FindDay # 13

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Silk

<http://developers.silk.io/> Silk provides an open-source framework for building "internet-of-things" devices with machine intelligence. What's different about their approach is that they leverage the computational power of the device to power the machine intelligence, as opposed to off-loading this processing to "the cloud" like most other similar platforms. This has a number of advantages (especially where privacy is concerned). They also leverage commodity hardware (basically smartphones) and use mainstream programming tools, both of which significantly lower the cost-of-entry to experimenting with the platform. I'm particularly impressed with the fact that it is open-source, because that has not been the case with many other "IoT" systems and devices. 

## 2 - Easily visualizing MongoDB geospacial data

<https://s3.amazonaws.com/geodndmap/index.html> I don't know who created this, but if you work with [geospacial data in MongoDB](https://docs.mongodb.com/manual/applications/geospatial-indexes/), it's very cool. Simply drop theJSON from MongoDB onto this page and you'll get a selected region, BOOM! via Mike 

## 3 - The Living Automaton

<https://www.youtube.com/watch?v=FK6zCvzG3oQ> https://www.youtube.com/watch?v=FK6zCvzG3oQ A great video showing the inner workings of the Maillardet Automaton (you may recognize this automaton from the movie [Hugo](https://en.wikipedia.org/wiki/Hugo_%28film%29)). Personally, I'm fascinated by machines like this. As a child it wasn't hard for me to understand how electronic machines could do math, play games or sequence control steps, but I thought the mechanical equivalents were pure witchcraft. via [Adafruit](https://blog.adafruit.com/2017/06/05/video-showing-the-inner-workings-of-the-maillardet-automaton/)

## 4 - Wheels of Fail

<http://rasterweb.net/raster/2017/06/03/wheels-of-fail/> My friend Pete did a good write-up on the pros and cons of the [very affordable little wheels](https://www.harborfreight.com/10-in-pneumatic-tire-with-white-hub-62409.html) you can get from Harbor Freight. The timing of this is great because I'm re-designing [Sux0rz](https://github.com/jjg/sux0rz) drivetrain and some bigger, grippier tires may be in order... via [Rasterweb](http://rasterweb.net)

## 5 - Solardozer

<http://hackaday.com/2017/06/07/solar-bulldozer-gets-dirty/> https://www.youtube.com/watch?v=OglAu8jeYAM A built-from-scratch solar-powered bulldozer. What's not to like? via [Hackaday](http://hackaday.com)---
tags:
  - gullicksonlaboratories
title: FindDay #14
link: http://jjg.2soc.net/2017/06/16/findday-14/
author: jgullickson
description: 
post_id: 2835
date: 2017-06-16 13:19:20
created_gmt: 2017-06-16 18:19:20
comment_status: open
post_name: findday-14
status: publish
post_type: post
---

# FindDay #14

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - No Tech Magazine

<http://www.notechmagazine.com/> ![brendan-howell_the-screenless-office](/wp/2017/06/brendan-howell_the-screenless-office.jpg) I don't know how I ended up here, but there's some fun stuff to read and some interesting perspectives that come from looking at non-high-tech ways to solve problems.  Obviously, this is very different from how I usually try to solve things.  

## 2 - Thoughts on Annie

<http://web.stanford.edu/dept/SUL/sites/mac/primary/docs/bom/anthrophilic.html> ![a20-1220](/wp/2017/06/a20-1220.jpg) Jef Raskin created the original Macintosh team at Apple, but the Macintosh that Apple eventually sold (and the one they still sell today) turned out to be almost nothing like what Jef had in mind. This essay is from a collection of documents the team collected in the early days of the project. This particular one, while from 1979 describes a computing device (a "work processor", as they like to say) which, save for a few technical details, sounds like a really awesome machine to work with. Jef passed away before the genius of his work was put to use (it's rarely understood even today). This is entirely a shame, because we could be having a much better relationship with our technology today if he could have seen these ideas to fruition.  

## 3 - NextCloud 12

<https://nextcloud.com/blog/welcome-to-nextcloud-12/> ![nextcloud-12-1024x576](/wp/2017/06/nextcloud-12-1024x576.png) The easiest way to describe NextCloud might be to say that it's an open-source alternative to Google Apps that you can run on your own servers. Why would you would want to do this is a much larger discussion (one that is covered in detail [on the NextCloud site](https://nextcloud.com/secure/) ). What I do want to talk about is that major performance improvements have been made and NextCloud 12 is quite snappy compared to previous versions. I've been running a NextCloud server in Germany for about a year, and while it's been usable, it's now much more enjoyable to work with. It has played a big role in my exodus from Google and now that it performs well (on the servers I can afford) I'm looking forward to leaning on it for more of my critical tasks.  

## 4 - tilde.club

<http://tilde.club/> ![Screenshot_20170616_130759](/wp/2017/06/screenshot_20170616_130759.png) Remember when the web wasn't owned by three companies? Tilde is something of a throwback to the "wild west" days of the web, but it's not just nostalgia. As I understand it, it started as a lark (alcohol may have been involved) but it turned into a community of people helping each-other learn the skills to create their own place on the web. I'm a fan of this approach. With as much focus as there is on teaching kids to "code", something like Tilde seems like a much more real way to get kids (and everyone) learning skills to express themselves on the web while creating a supportive community that they can own a piece of as well. This is a refreshing break from the complex platforms and proprietary systems which have become commonplace and in various ways dissolve creativity and innovation. I might even setup a server like this if I can find a few other people who would find it interesting and would like to learn how to hand-craft artisinal HTML pages themselves.  

## 5 - ELLO 2M

<https://www.crowdsupply.com/yellow-beak-computer/ello-2m> ![4260371455569424518](/wp/2017/06/4260371455569424518.jpg) ELLO 2M looks like a really cool little computer for learning about computers. I'm not exactly sure how you get your hands on one, but it seems to combine the essential hardware in a fun, handy package as well as a software orientation that is reminiscent of the 8-bit personal computers I cut my programming teeth on. There is also a [Hackaday.io project page](https://hackaday.io/project/9692-ello-2m) for ELLO 2M.---
tags:
  - gullicksonlaboratories
title: FindDay #15
link: http://jjg.2soc.net/2017/06/23/findday-15/
author: jgullickson
description: 
post_id: 2911
date: 2017-06-23 11:53:19
created_gmt: 2017-06-23 16:53:19
comment_status: open
post_name: findday-15
status: publish
post_type: post
---

# FindDay #15

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Resonate

<https://resonate.is> ![Screenshot_20170623_113456](/wp/2017/06/screenshot_20170623_113456.png) Resonate is, as the website says, a streaming music co-op. I won't try to [explain what a co-op is](https://en.wikipedia.org/wiki/Cooperative), but the basic idea is that it's a shop owned by the producers whose goods are sold and the consumers who buy those goods. When I was growing-up, co-ops selling groceries and other farm goods could be found in most small towns. It's considered by many to serve producers, consumers and communities better than the more common for-profit corporate entities. When I was at [Murfie](https://murfie.com/) we discussed applying the co-op model to a portion of the business and it seemed (to me) like a very good fit. Ultimately we didn't got that direction, so I was very excited to see someone else going for it. There's some very cool things about Resonate's co-op implementation and I think you should read the descriptions of all the [membership options](https://resonate.is/#join) they offer (I was most impressed that profit shares are distributed to volunteers first). Personally, I can't decide if I want to sign-up as a listener, artist or volunteer (I could see myself as all three and I'm not sure if you can switch/combine/etc.). Once I've had a chance to explore the platform I'll probably write a more detailed post about it. In any case I'm very excited about the prospect of tech/online products and services adopting the co-op model. I think it's a very natural fit and we could use a business model in tech that could bridge the gap between the truly free world of FOSS and the capitalist-driven "tech start-up" world. _via @samtoland@social.coop_

## 2 - Krita

<https://krita.org> ![krita-313-screenshot-750x386](/wp/2017/06/krita-313-screenshot-750x386.jpg) [Krita](https://krita.org) is that rarest of beasts; a beautiful, well-designed piece of open-source application software. That's not to say that there isn't loads of well-designed FOSS software, but when it comes to _application_ software (the kind someone might use directly), open-source software isn't known for having highly-polished user interfaces. This seems doubly-true for graphics software, ironically. I lack the skills to determine if Krita is a great tool for making art, but the [stuff I've seen come out of it ](https://krita.org/en/features/gallery/)looks amazing, and Libby was impressed. Krita is available for [Linux, Mac & Windows](https://krita.org/en/download/krita-desktop/) and if you use this type of thing I recommend giving it a try. If you like it, [support the developers](https://krita.org/en/support-us/donations/) however you can, we can use more well-designed open-source software like Krita and supporting this kind of work is likely to attract and retain developers and designers who understand the importance of these things. _via a friend on Mastodon, but I can't seem to locate their name atm :(_

## 3 - Mold-a-Rama on Waymarking.com

[http://www.waymarking.com/cat/details.aspx?f=1&guid=6de6e6c2-89b3-4ed2-8884-85003af637e9&st=2](http://www.waymarking.com/cat/details.aspx?f=1&guid=6de6e6c2-89b3-4ed2-8884-85003af637e9&st=2) ![mold-a-rama-01](/wp/2017/06/mold-a-rama-01.jpg) If you visited zoos or museums during the 1980's you've probably seen a [Mold-a-Rama](https://en.wikipedia.org/wiki/Mold-A-Rama). They are automated injection molding machines which will create a souvenir for you as you watch the machinery through it's Plexiglas bubble canopy. Of course as a child I was fascinated with these machines and begged for a couple dollars every time I saw one. As time rolled on, they seemed to disappear and I had all but forgotten about them until my friend Preston mentioned getting a plastic Gorilla from one when he was a kid. This rekindled my interest in these machines and, [thanks to the Internet](https://duckduckgo.com/?q=mold-a-rama), I was able to learn everything I could have wanted to know in no time at all. One of the most exciting pieces of information I ran across during this research was a [database of operational Mold-a-Rama's](http://www.waymarking.com/cat/details.aspx?f=1&guid=6de6e6c2-89b3-4ed2-8884-85003af637e9&st=2) on Waymarking.com. Up until now, finding these machines was a hit-or-miss operation, but thanks to this website you can now know where to find these machines and what models are available. Needless to say, this was enough to kick-start my desire to build a collection of these useless objects, and has absolutely nothing to do with the fact that each resides at an interesting location requiring some kind of road-trip... 

## 4 - WiFi232 Internet Modem

<http://www.bytecellar.com/2017/05/30/the-wonderful-wifi232-bbsing-has-literally-never-been-easier/> https://www.youtube.com/watch?v=92RIT_L-8jA I cut my telecomputing teeth on BBS's, and I've even dabbled in resurrecting them on the Internet, so this project struck a chord for me. The video tells it all, essentially this device is a way to get computers that only speak serial on to the Internet just enough to talk to BBS's which can be connected to via Internet protocols (instead of landlines). I've considered building a network interface for my TRS-80 Model 100 using an ESP8266, but I didn't consider this application, and it's quite an elegant hack. _via [Hackaday](http://hackaday.com/2017/06/17/bbsing-with-the-esp8266/)_

## 5 - cjdns

<https://github.com/cjdelisle/cjdns/> You have to be a fairly serious networking geek to appreciate the beauty of [cjdns](https://github.com/cjdelisle/cjdns/), but if you're there you'll know why it made this week's list. I haven't had a chance to play with it yet, but the potential is pretty exciting, and I'll be setting up an experimental network in the lab in the near future. I'll document the experiment and the results here, so if you're not a network engineer but you still want to know why this is cool, stay tuned.---
tags:
  - gullicksonlaboratories
title: FindDay #16
link: http://jjg.2soc.net/2017/06/30/findday-16/
author: jgullickson
description: 
post_id: 3084
date: 2017-06-30 15:13:22
created_gmt: 2017-06-30 20:13:22
comment_status: open
post_name: findday-16
status: publish
post_type: post
---

# FindDay #16

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - The most portable watercolors set. Ever.

<https://www.vivivacolors.com/> https://www.youtube.com/watch?v=4yrzjep2Dks _guest post by Jamie_ I came across a video of these on a social media feed and it took forever to actually find how/where to order them. I love the idea that I can have a lightweight watercolor kit in my purse, kayak or glove box for those moments of on the go inspiration. It is currently a reasonably priced Indiegogo campaign - we ordered two sets for $30. Other, much bulkier, portable watercolor kits run about $15-$20.  Delivery is anticipated in November 2017. In the mean time, I'll be experimenting with a DIY alternative using dissolving paper from [Amazon](https://www.amazon.com/SmartSolve-IT117138-Dissolving-Paper-Pack/dp/B01BGGC3KQ/). I'm going to try dripping ink pigments on it and letting it dry in the sun to see if I can achieve the same thing for less. Then I would cut out squares and keep them in an old Altoids tin - a trick I learned from a video on making travel soap kits with the same paper and soap. via Jamie  

## 2 - Ship your 3D print via UPS

<https://www.theupsstore.com/print/3d-printing> I had to return some hardware via the UPS Store and noticed they had a 3D printer. Turns out you can now email them a model and have it printed at more than 60 UPS Store locations. The location I was at has a [Stratasys UPrint SEplus](http://www.stratasys.com/3d-printers/idea-series/uprint-se-plus) and pricing starts at $20.00 a print. I'm not sure if all locations use the same printer, but the price isn't bad compared to other printing services, and the convenience of having the job done locally makes up for the difference. Also, it's kind of cool that you could ship a physical object in electronic form via UPS.  

## 3 - Let's Make Music with LMMS

<https://lmms.io/> https://www.youtube.com/watch?v=W6tEolVz3_4 LMMS is an open-source music authoring program for creating electronic music. It comes with a surprisingly large assortment of instruments and effects, and it's available for [Linux, Mac & Windows](https://lmms.io/download/#linux).  

## 4 - XFCE Desktop Environment

<https://xfce.org/> ![xfwm4-tiling-small](/wp/2017/06/xfwm4-tiling-small.png) I recently found myself stuck using Ubuntu MATE Linux instead of my favorite distro (Debian) and I got frustrated with the Gnome desktop so I looked for a lightweight alternative (which ruled out KDE) that I could switch to easily. XFCE was suggested to me on Mastodon so I gave it a try. After spending a few days with it I have to say I like it. It's easy to customize and performs well on limited hardware. If you're running Linux on something without a lot of memory or processing power, give it a try.  

## 5 - Parametric Pi holes

<https://www.youmagine.com/designs/pihole-openscad-library-for-raspberry-pi> ![carousel_jamrendering_11445_3935520160403-28752-ejom95](/wp/2017/06/carousel_jamrendering_11445_3935520160403-28752-ejom95.jpg) Not to be confused with the excellent [PiHole ad blocker](https://pi-hole.net/), this collection of OpenSCAD models make it easy to create mounting plates for all of the various Raspberry Pi form factors. If you've ever had the need to mount a Pi to a project you'll appreciate how much easier this is than trying to measure the board and create your own model from scratch. It would also make designing a universal mount (that supports any version of Raspberry Pi) a lot easier.---
tags:
  - gullicksonlaboratories
title: FindDay #17
link: http://jjg.2soc.net/2017/07/07/findday-17/
author: jgullickson
description: 
post_id: 3530
date: 2017-07-07 11:11:21
created_gmt: 2017-07-07 16:11:21
comment_status: open
post_name: findday-17
status: publish
post_type: post
---

# FindDay #17

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._  

## 1 - Create books directly from Wikipedia articles

<https://en.wikipedia.org/wiki/Help:Books> This has been around for awhile (apparently before 2014) but I had never heard of it before this week. Simply put, Wikipedia has a built-in feature that lets you bundle-up single or multiple articles into an e-book you can download, or even have printed. I love and rely on Wikipedia. I've taken a few stabs at creating a local replica that I can take with me in leiu of access to the website. While you wouldn't use this method to capture all of it, it's a pretty slick way to carve out some of the parts you find useful and keep them handy.  

## 2 - Cheap and easy platform bed

<https://www.instructables.com/id/Cheap-easy-low-waste-platform-bed> ![fcu5lg0fgf7reer-medium](/wp/2017/07/fcu5lg0fgf7reer-medium.jpg) Aside from being simple and attractive what I love about this design is that it makes a conscious effort to reduce waste without compromising the structural integrity of the results. Engineering FTW! I ran across this great project a few months too late, but the techniques can be applied to projects other than bed-building. [aeray](https://www.instructables.com/member/aeray/) also has a number of other projects designed with the same philosophy which I might take a stab at. _via Instructables newsletter_  

## 3 - Hacker Calculus

<https://hackaday.io/project/20621-hacker-calculus> ![1827791490637951983](/wp/2017/07/1827791490637951983.jpg) If you're like me you struggle with math not so much because "math is hard" but because there's something about the symbology, or the abstract nature of talking about math. This project addresses that communication problem by rendering calculus into physical objects. By interacting with math using more senses, the abstract constructs can be more accessible to those who have trouble relating to them using more traditional methods.  

## 4 - Primitive Technology videos

<https://www.patreon.com/user?u=2945881> https://www.youtube.com/watch?v=i9TdoO2OVaA I find it fascinating to watch these videos. I believe that a lot of good ideas get left behind not because they are worse than the ideas that continue on, but because some of them under-perform within the context of various cultures. There's a lot of cool techniques demonstrated in these videos and I respect the commitment the author makes to living up to idea.  

## 5 - Pietenpol Annual Reunion

<http://www.pietenpols.org/annual-pietenpol-reunion/> ![dsc_9480](/wp/2017/07/dsc_9480.jpg) My first flight experience was in a wide-open ultralight, so I have an affinity for bare-essentials flying. Few aircraft optimize this better than [the Pietenpol Air Camper)[https://en.wikipedia.org/wiki/Pietenpol_Air_Camper]. I was introduced to this aircraft by a friend-of-a-friend and found out that an annual gathering of these aircraft and their builders happens within my neck of the woods. I'm planning to attend at least one day and bring some pictures (and perhaps video) back with me.---
tags:
  - gullicksonlaboratories
title: FindDay #18
link: http://jjg.2soc.net/2017/07/21/findday-18/
author: jgullickson
description: 
post_id: 3635
date: 2017-07-21 08:29:43
created_gmt: 2017-07-21 13:29:43
comment_status: open
post_name: findday-18
status: publish
post_type: post
---

# FindDay #18

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - DIY Toolbag Organizer

<https://www.instructables.com/id/DIY-Toolbag-Organizer/> https://www.youtube.com/watch?v=gUbms2_fJQE I have a number of toolboxes, but when I'm not working on a specific project I have a small toolbag I like to throw in the trunk that covers the basics and is a lot lighter than the boxes. The only problem with the bag is that it's just a pile of tools, so it's hard to know exactly what you're taking with. It's also hard to find the right tool when you're elbow-deep in whatever it is you're trying to fix. This simple organizer project solves both problems by making a place for everything in the bag without all the added weight of a full-blown toolbox. The best thing is that you can customize it to fit your particular bag and tool assortment (you could even make a couple that carry specific tools for specialized missions). (via [Instructables](https://www.instructables.com) newsletter)  

## 2 - 3D Printed Science Projects

<http://www.apress.com/us/book/9781484213247> Got a 3D printer but tired of using it to print useless trinkets? This book is a great way to put it to use and get smarter along the way. 3D Printed Science Projects is a collection lessons in science which use 3D printable models to provide tools and references to carry-out exercises or explain concepts that would otherwise require expensive (or non-existent) equipment. This would be a great way to offset the cost of adding a printer to the classroom and introduce students (or anyone) to the practical application of these machines.  

## 3 - Vinyl Soundtrack for "The Fifth Element"

<https://www.turntablelab.com/products/eric-serra-fifth-element-ttl-exclusive-colored-vinyl-vinyl-2lp> ![5thelement-ost-ttl_1800x](/wp/2017/07/5thelement-ost-ttl_1800x.jpg) One of my favorite movies is now a very cool piece of vinyl. I'm particularly impressed with how the design of the sleeve and the records themselves reflect the beauty of the film without simply reproducing the imagery verbatim. (via [Turntable Lab](https://www.turntablelab.com/) newsletter)  

## 4 - Makerwheel

<https://hackaday.io/project/25905-makerwheel> ![6781991499967454212](/wp/2017/07/6781991499967454212.jpg) Makerwheel is an open-source printable part designed to replace more expensive, harder-to-get mechanical components. The examples include a worm gear drive, rack-and-pinion and a linear slide but it's not hard to imagine other uses for this clever little wheel (via [hackaday.io](https://hackaday.io)  

## 5 - No Instruction Set Computing

<https://en.wikipedia.org/wiki/No_instruction_set_computing> When I first learned about [FPGA](https://en.wikipedia.org/wiki/No_instruction_set_computing)'s, this is one of the first applications I imagined for them. Most people I tried to explain this to didn't understand what I was talking about so I was very excited this week to learn that there's a name for what I was talking about. It's hard to describe without a lot of background in the subject matter, but in simplest terms, instead of compiling a program to run on a particular processor, this approach compiles a processor to run a particular program. The result is a piece of hardware optimized to run one piece of code, instead of the general-purpose processors used in most computers. This approach opens new realms of potential in terms of performance an efficiency. It also makes the power of hardware technologies like FPGA to a much wider range of programmers. I hope to set aside some time to explore all three soon.---
tags:
  - gullicksonlaboratories
title: FindDay #19
link: http://jjg.2soc.net/2017/08/04/findday-19/
author: jgullickson
description: 
post_id: 4232
date: 2017-08-04 09:46:46
created_gmt: 2017-08-04 14:46:46
comment_status: open
post_name: findday-19
status: publish
post_type: post
---

# FindDay #19

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._  

## 1 - Plug 'n Drive

<https://plugndrive.ca/> ![electric20vehicle20discovery20centre](/wp/2017/08/electric20vehicle20discovery20centre.png) Electric vehicles have become more common over the last few years but most people still have no direct personal experience with them. That's too bad, because a little time spent talking to someone who owns one and a little hands-on time behind the wheel can go a long way toward dispelling the myths around electric vehicles and removing the anxiety people sometimes feel about owning one. Plug 'n Drive aids in this by providing both fixed-location and traveling events where people interested in electric vehicles can get first-hand experience with the vehicles and learn all about the pros and cons of ownership. This is something I've thought about creating here (Plug 'n Drive is currently Canada-only) because I've met a lot of people who would be willing to consider purchasing an EV after they learn what it's really like to own and drive one.  

## 2 - KODAK EKTRA Smartphone

<http://www.kodak.com/consumer/products/ektra/default.htm> ![81gybgpdacl-_sl1500_](/wp/2017/08/81gybgpdacl-_sl1500_.jpg) If a typical smartphone is a phone with a camera built-in, then the KODAK ECTRA is a camera with a phone built-in. I've seen a few other devices like this but most of them try to replace high-end cameras which I think is a mistake because most people who buy high-end cameras are looking for a specialized, purpose-built piece of equipment that does one thing excellently. ![hills_bg_v1_1920x0](/wp/2017/08/hills_bg_v1_1920x0.jpg) The EKTRA on the other hand appears to be aimed at replacing a nice point-and-shoot camera (which itself has been largely replaced by smartphones). The difference is that the EKTRA puts the camera first. The phone component allows for things like a rich user interface (compared to the primitive ones found on most point-and-shoot digital cameras) and the conveniences we've become accustom to like auto-uploading photos to the cloud (remember having to use a cable to download photos to your laptop?) and sharing to cool social networks like Mastodon. I haven't handled one of these cameras personally and that's something I'd really like to do before recommending them, but if nothing else I'm excited about the fact that a company was willing to create something like this, and I hope to see more products like this in the future. You can order an [unlocked EKTRA from Amazon.com](http://amzn.to/2vzmPTl) now for $399.99.  

## 3 - Stir to Action Magazine

<https://www.stirtoaction.com/> I've recently become interested in co-ops and in understanding what their advantages are and why they're not more widespread. Stir to Action has a lot of well-designed resources that are accessible to non-business-oriented types like me. Definitely worth checking-out if you'd like to know more about how co-ops and other people-first business models might be applied in your community.  

## 4 - kdenlive

<https://kdenlive.org/> ![slide1](/wp/2017/08/slide1.png) For ages I've used a Mac and Apple software to edit films, but as part of my efforts to stop using closed, proprietary software I've had to find a replacement for these tools. Additional, I'm not a fan of the direction Apple has gone with Final Cut Pro, so I was very excited to come across kdenlive, a very well made and modern non-linear editing program for Linux. I'm still learning the ropes and haven't yet had a film to really test kdenlive out on yet, but I've got a couple of small video projects coming up that I'm going to try to use it on, and I'll definitely need something like this once I have some usable output from the [Open Digital 8](https://jjg.2soc.net/category/open-digital-8/) project to work with.  

## 5 - 3D Modeling on the Raspberry Pi

<http://www.raspberry-pi-geek.com/Archive/2016/17/3D-modeling-on-the-Raspberry-Pi-with-FreeCAD> ![figure-1_lightbox](/wp/2017/08/figure-1_lightbox.png) One of the greatest outcomes of the [Raspberry Pi project](https://www.raspberrypi.org/) is that it's now possible for almost anyone to have a computer of their own, even kids. This factor is one of the most important things that contributed to my own interest in computers and my ability to pursue the field. Having a [VIC-20](https://en.wikipedia.org/wiki/Commodore_VIC-20) at my disposal to do with as I pleased was the foundation of my work with computers. That's why I'm always excited to see examples of the Raspberry Pi being used for "real work" that traditionally required bigger, more expensive machines. This post from 2016 explains how to get [FreeCAD](https://www.freecadweb.org/) up-and-running on the Raspberry Pi and includes the details you need to know to make sure you're getting the most performance out of the hardware that is available. It also includes a basic introduction to using FreeCAD (which is how I found it as I'm learning FreeCAD myself), enough to let you decide if FreeCAD is a tool you'd like to learn more about. It might be hard to appreciate how significant something like this is, because in the past a CAD workstation would have cost thousands, perhaps more than ten thousand dollars. The fact that you could put together a usable CAD workstation with a TV set and about $50 in parts and open-source software is amazing, and it makes learning these skills accessible to a wide-range of people who would have been otherwise left out. I like to think of all the cool things that will exist when everyone has access to the tools that were once only available to the more privileged classes.---
tags:
  - gullicksonlaboratories
title: FindDay #1
link: http://jjg.2soc.net/2017/03/03/findday-1/
author: jgullickson
description: 
post_id: 428
date: 2017-03-03 08:45:57
created_gmt: 2017-03-03 14:45:57
comment_status: open
post_name: findday-1
status: publish
post_type: post
---

# FindDay #1

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - MP Select Mini RAMPS Upgrade

<http://www.instructables.com/id/Replace-Monoprice-Select-Mini-Controller-With-Ardu/> ![fr8c1xoiz6dksva-medium](/wp/2017/03/fr8c1xoiz6dksva-medium.jpg) My Friend [Jeff](http://www.instructables.com/member/jalex9/) replaced the stock electronics in his [MonoPrice Select Mini](http://astore.amazon.com/jjg00-20/detail/B01FL49VZE) 3D printer with a Reprap-style [RAMPS setup](http://astore.amazon.com/jjg00-20/detail/B0111ZSS2O). Not only have his prints improved, he [documented the whole process on Instructables](http://www.instructables.com/id/Replace-Monoprice-Select-Mini-Controller-With-Ardu/) too. Sharing FTW! 

## 2 - Biolite CampStove 2

<http://www.bioliteenergy.com/products/campstove-2> ![campstove2_1_1024x1024](/wp/2017/03/campstove2_1_1024x1024.jpg) I've owned an [original BioLite CampStove](http://astore.amazon.com/jjg00-20/detail/B00BQHET9O) for a few years. It's a great piece of kit and they are a great company, who are helping people throughout the world. I even made a few movies about it. https://www.youtube.com/watch?v=eU87lwl0_TQ This week Version 2 of the Campstove was released and it looks to be as good as the original, with a number of significant improvements. The biggest one to me is more power, as the original was great at charging electronic devices from the same era, but since then most phones, tablets, etc. have become a lot more power hungry. 

## 3 - Powerline

<https://github.com/powerline/powerline> Powerline is a very cool add-on for the vim editor. I won't try to explain everything it does, but to me two of the coolest things are that it shows the current mode using color, and if you're looking at a file stored in Git, it shows the current branch (how many times have you saved changes in the wrong branch?). ![screenshot_20170303_083955](/wp/2017/03/screenshot_20170303_083955.png) It's also respectful of vim's visual minimalism, and doesn't appear to have performance side-effects (at least on my hardware). If you use vim it's definitely work checking out, and if you use Debian, [there is a nice guide available](https://levlaz.org/installing-powerline-in-debian/) to set the whole thing up. (via [Linux Voice Podcast](https://www.linuxvoice.com/category/podcasts/) ) 

## 3 - Women of NASA LEGO

<https://ideas.lego.com/blogs/1-blog/post/121> ![2562096-o_1anri7p8hmfm1oa74nc1hh5in57-full](/wp/2017/03/2562096-o_1anri7p8hmfm1oa74nc1hh5in57-full.png) LEGO Ideas is a LEGO website where you can [submit ideas for new LEGO sets](https://ideas.lego.com/howitworks), and some sets with the most votes are turned into official LEGO products. For the second 2016 review period they have selected [Women of NASA](https://ideas.lego.com/projects/147876) by [20tauri](http://ideas.lego.com/profile/20tauri/profile). There's been a lot of cool pop culture icons turned into LEGO sets via LEGO Ideas, but it's exciting to see icons of science making the cut as well. (via [Adafruit](https://blog.adafruit.com/2017/03/02/women-of-nasa-wins-2016-lego-ideas-whm17-womenshistorymonth-womeninstem/) ) 

## 4 - Milling (home made) HDPE

<http://rasterweb.net/raster/2017/02/26/milling-home-made-hdpe/> ![mm-helmet-cnc-20-640x480](/wp/2017/03/mm-helmet-cnc-20-640x480.jpg) [Pete](http://rasterweb.net/raster/about/) is sharing his experience with a [cheap CNC mill](http://astore.amazon.com/jjg00-20/detail/B01LZ2K0Z2) and in the latest installment mills a design from a chunk of HDPE plastic that [he recycled himself](http://rasterweb.net/raster/2015/08/11/hdpe-sheet-cake/). This is a very helpful [series of posts](http://rasterweb.net/raster/tag/cnc/) for anyone who is considering dipping their toes into CNC milling with open-source tools, and using old milk bottles to do so is just icing on the cake. (via [Rasterweb](http://rasterweb.net) ) 

## 5 - The Walnut TX

<https://www.youtube.com/watch?v=OUGWnxQbQDg> https://www.youtube.com/watch?v=OUGWnxQbQDg A long-range radio transmitter in a walnut. I don't know a lot about HAM radio, but it's something I've considered dabbling in over the years. The fact that HAM radio people make things like this only serves as encouragement. (via [Hackaday](http://hackaday.com/2017/02/24/ham-goes-nuts-for-tiny-transmitter/) )---
tags:
  - gullicksonlaboratories
title: FindDay #20
link: http://jjg.2soc.net/2017/08/11/findday-20/
author: jgullickson
description: 
post_id: 4367
date: 2017-08-11 10:00:45
created_gmt: 2017-08-11 15:00:45
comment_status: open
post_name: findday-20
status: publish
post_type: post
---

# FindDay #20

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Super 73

<https://www.kickstarter.com/projects/lithiumcycles/the-super-73> ![9222dde30c0321f314fd9eaa6477e705_original](/wp/2017/08/9222dde30c0321f314fd9eaa6477e705_original.jpg) I ran across the Super 73 on YouTube, and it was so cool I was willing to suffer through 6 minutes of ultra-hipster-douchebag video just to check it out. Fortunately the videos produced by the people actually building the thing are more tolerable, so that's what I'll embed here. https://www.youtube.com/watch?v=jzLgcVOw200 I linked to the (now complete) [Kickstarter campaign](https://www.kickstarter.com/projects/lithiumcycles/the-super-73) page because there's not a lot to see of the Super 73 on the [manufacturer's website](https://www.lithiumcycles.com/products/2018-super-73) right now. They're currently pushing the [more affordable "Scout" model](https://www.lithiumcycles.com/), which is cool but I don't like it as much as the original 73. There is an updated version of the 73 in the works, but it goes the same "hub motor" route as the Scout which makes sense as a product but is less interesting to me since I'm not interested in the bike as a product, but as a design. You see, I've been noodling on building (or converting) and electric bike/motorcycle for years. Ever since I rode a [Zero](http://www.zeromotorcycles.com/) I've lost all interest in owning another gasoline-powered motorcycle, but given the cost ($10k+) it seemed like building one was more realistic, and building an electric bicycle might be a great starting point. Where I get stuck on this is deciding if I want to try to find a suitable frame and modify it, or if I want to try to build one from scratch. The former is kind of expensive and seems clumsy, but the latter requires design skills that I'm not sure I have yet. When I saw the Super 73 I was immediately reminded of my little brother's old minibike, but converted to electric power; perhaps you see the resemblance? [caption id="attachment_4375" align="aligncenter" width="600"]![img_4314](/wp/2017/08/img_4314.jpg) This isn't Justin's bike, but one like it[/caption] This got me thinking about how I could build a similar bike based around the proven design of the "[boonie bike](http://jleibovitch.tripod.com/id99.htm)", to create something like the Super 73 but less expensive and built from more commodity parts. This isn't a criticism of what Lithium Cycles is doing. Their bikes are expensive, but when you consider that they are essentially built by hand by people living in California, the cost is understandable (honestly if they are paying their workers a living wage in that part of the U.S., the only profit they probably make is whatever discount they can negotiate on parts). If you have the means I think it would be great to support their work, but in my case I can't justify spending $3200.00 on a bicycle (that's more than the cost of my first two motorcycles _combined_), and I'm probably not alone in that regard. So the Super 73 inspired me to create an electric bike drawn from the same inspiration, but designed to be built by people who have more time (and/or sweat) than money. I want to avoid incorporating fixed-cost parts in the design that limit the ability for a builder to offset cost with scavenging skills. This might be the perfect justification to resurrect my work on turning automotive alternators into traction motors... 

## 2 - Leonardo's eBooks

<http://www.bl.uk/manuscripts/FullDisplay.aspx?ref=Arundel_MS_263> ![canvas](/wp/2017/08/canvas.png) The British Library has scanned and published a number of Leonardo da Vinci's notebooks and you should probably drop what you're doing and spend some time exploring them. 

## 3 - Darktable

<http://www.darktable.org/> I'm trying to include at least one piece of excellent open-source software in each weeks list, and Darktable is a great addition to that tradition. ![screenshot-2](/wp/2017/08/screenshot-2.png) Simply put, Darktable is an open-source photography application along the same lines as [Adobe's Lightroom](https://en.wikipedia.org/wiki/Adobe_Photoshop_Lightroom) or [Apple's Aperature](https://en.wikipedia.org/wiki/Aperture_\(software\)) (RIP). I don't know enough about digital photography to say whether or not Darktable can replace these products (I'll leave that to experts like you) but based on comparing the features, and given the performance, stability (and of course reduced cost) that comes with being an open-source application that runs on Linux, I think it's worth checking out. It's also another example of a well-designed, polished open-source application and I'm excited to see more of these coming about. For the work I do, brutal, ugly tools are fine but I know that this alienates many potential Linux users. So it's very exciting to see more and more tools that not only look and work better, but also expand what Linux can do into new and creative areas. 

## 4 - Neurable

<http://spectrum.ieee.org/the-human-os/biomedical/bionics/brainy-startup-neurable-unveils-the-worlds-first-braincontrolled-vr-game> ![neurable-2](/wp/2017/08/neurable-2.jpeg) Aside from looking incredibly cyberpunk, Neurable's accessory for the [HTC Vive](https://en.wikipedia.org/wiki/HTC_Vive) VR system takes us one step closer to [William Gibson's](https://en.wikipedia.org/wiki/William_Gibson) vision of [the matrix](https://en.wikipedia.org/wiki/William_Gibson#Cultural_significance) by allowing the user to interact with the virtual world using only their mind. There have been a number of products released recently that allow some level of input using brainwaves, but due to the way these systems work they are limited to somewhat "coarse" levels of input that are more associated with the psychological state of the user than their deliberate intentions. Neurable is taking a different approach, using the same signals and similar sensors, but processing the data in such a way as to be able to extract choices the user is making and mapping this to actions inside the simulation. Based on the article linked, it actually works. It's hard to exaggerate the potential applications for this technology. Neurable is accepting applications for developers who would like to receive a kit to develop such applications.---
tags:
  - gullicksonlaboratories
title: FindDay #21
link: http://jjg.2soc.net/2017/08/25/findday-21/
author: jgullickson
description: 
post_id: 4402
date: 2017-08-25 10:00:47
created_gmt: 2017-08-25 15:00:47
comment_status: open
post_name: findday-21
status: publish
post_type: post
---

# FindDay #21

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Escape to Canada with Launch Academy

<https://techcrunch.com/2017/06/02/launch-academys-startup-visa-program-gives-entrepreneurs-permanent-residency-in-canada/> ![canada-flag-animated-gif-9](/wp/2017/08/canada-flag-animated-gif-9.gif) As America accelerates as quickly as possible away from creating an environment which cultivates innovation, Canada has been expressing it's commitment to cultivating the future of technology. Part of this is the [Start-up Visa program](http://www.cic.gc.ca/english/immigrate/business/start-up/index.asp) which is attractive, but there are some barriers that make it not an option for many who might want to bring their technology to The Great White North. Launch Academy is addressing this by providing a means to bridge the gap. The article covers the details but the short version is that if your idea qualifies for their program, they make it a lot easier for foreigners to pursue their dreams and become residents in a country which values progressive ideals as much as it values progressive technology. 

## 2 - Amish Q&A on Healthcare, Mutual Aid and progress

<http://amishamerica.com/an-amish-couple-answers-questions-health-care-mutual-aid-church-rules/> https://youtu.be/1f2-11nhg1A Insight on how the Amish address some of the hardest problems facing "English" communities today. _(via the [Amish America newsletter](http://amishamerica.com) )_

## 3 - AdoptABot

<http://printrbot.com/adoptabot/> https://youtu.be/jpD16qI08QA Printrbot is a cool company and one of the first companies to bring consumer 3D printers to the desktop. Brooks and team are now trying to put printers in front of more people by placing unused hardware in the hands of hackers who then donate the printers (and their knowledge) to their local community. 

## 4 - Crowdmatch

[https://snowdrift.coop/](https://snowdrift.coop/} With commercial crowdfunding showing it's true face (and in some cases, [alignment with genuinely disgusting entities](https://itsgoingdown.org/patreon-caves-to-tim-pool-alt-right-bans-igd/) ) I've been on the lookout for a way to leverage the many advantages of this method of funding work without the nasty aftertaste. Crowdmatch applies the co-op model to crowdfunding, and it looks pretty cool. I haven't tried it out yet (need to pick one venture to fund) but if you're currently using Patreon, etc. I recommend checking it out as an alternative. 

## 5 - Capture 3D models with your 2D camera with insight3d

<http://insight3d.sourceforge.net/> ![vila_prague](/wp/2017/08/vila_prague.png) insight3d is an open-source application that can create 3D models from a series of photographs. It does this using [Photogrammetry](https://en.wikipedia.org/wiki/Photogrammetry) which is short for "hard math and magic". While this is useful for small objects (because cameras are cheaper than 3D scanners), it's essential for large objects like buildings that don't fit inside your typical 3D scanner.---
tags:
  - gullicksonlaboratories
title: FindDay #2
link: http://jjg.2soc.net/2017/03/10/findday-2/
author: jgullickson
description: 
post_id: 665
date: 2017-03-10 07:27:32
created_gmt: 2017-03-10 13:27:32
comment_status: open
post_name: findday-2
status: publish
post_type: post
---

# FindDay #2

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Interactive Dancing Socks

<http://www.instructables.com/id/Interactive-Dancing-Socks/> https://youtu.be/jWIHbhfmYtw I've seen a few things like this but what I like about this one is how practical it is. Most of the other examples I've seen focus on the physical beauty of the implementation, which is cool, but what I like about this is that it's not held back because it doesn't have the fit-and-finish of the wearable tech you see in fashion magazines. (via [Adafruit](https://blog.adafruit.com/2017/03/08/interactive-socks-for-dancing-assistance-wearablewednesday/) ) 

## 2 - vi and vim tips from the CIA

<https://wikileaks.org/ciav7p1/cms/page_3375350.html> The vast majority of the information that has been leaked from the American CIA is horrifying. The CIA is literally waging a war against the privacy of America's own citizens as well as citizens of the world. However it appears that even members of the CIA fear [vim](http://www.vim.org/). 

## 3 - 3D printed resistor storage drawers

<http://www.thingiverse.com/thing:2144041> ![fc235d0cdd608ce23d0c5374182f9c84_preview_featured](/wp/2017/03/fc235d0cdd608ce23d0c5374182f9c84_preview_featured.jpg) This looks extremely useful. Now if only the resistors would sort themselves... (via [Adafruit](https://blog.adafruit.com/2017/03/09/resistor-storage-drawers-3dprinting-3dthursday/) ) 

## 4 - OpenStreetMap

<http://www.openstreetmap.org> While not new, OpenStreetMap is new to me.  In simplest terms, it is an open-source supply of maps and geolocation data that can be used by anyone for anything.  Think Google Maps but you can do anything you want with it, and it's created by users. The general-purpose value of something like this is kind of hard to explain in a few sentences, but I'll try to write more about the multitude of cool things that have been built using OpenStreetMap data in the future. 

## 5 - IoT Smart Jar

<https://www.hackster.io/smazee/iot-smart-jar-using-esp8266-arduino-and-ultrasonic-sensor-9d765b> ![f3lr1gvirlqh42q-large](/wp/2017/03/f3lr1gvirlqh42q-large.jpg) This is essentially a jar that knows when it is getting empty. While perhaps impractical in this form, it's not hard to imagine how in a few iterations the design could be turned into something that is inexpensive and durable enough to be useful for keeping many supplies stocked (one for a dog food bucket comes to mind...). Connected to the proper services, the jar could automatically order supplies to replenish itself, or set other non-commercial processes in motion (a planting schedule, chicken feeding, etc.) that would resupply the contents of the jar before they are exhausted.---
tags:
  - gullicksonlaboratories
title: FindDay #3
link: http://jjg.2soc.net/2017/03/17/findday-3/
author: jgullickson
description: 
post_id: 878
date: 2017-03-17 07:35:24
created_gmt: 2017-03-17 12:35:24
comment_status: open
post_name: findday-3
status: publish
post_type: post
---

# FindDay #3

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - SR-71 Flight Manual

![765px-lockheed_sr-71_blackbird](/wp/2017/03/765px-lockheed_sr-71_blackbird.jpg) <http://www.sr-71.org/blackbird/manual/> I ran across this manual via a long chain of web pages which began with a story from a pilot who experienced a [mach 3+ break-up](http://www.roadrunnersinternationale.com/weaver_sr71_bailout.html) while flying an SR-71 Blackbird (an excellent read in itself). I've been fascinated with the [SR-71](https://en.wikipedia.org/wiki/Lockheed_SR-71_Blackbird) since childhood, so it's really cool that so much of the information about it has become declassified. Even though I'll never fly one, it's interesting to see what's involved and I love the matter-of-fact tone of documents like this. 

## 2 - Darktable

![screenshot-2](/wp/2017/03/screenshot-2.png) <http://www.darktable.org/> I'll admit right away that I haven't spent much time with this piece of software (yet), but I'm always excited to see something in this application space being made for Linux (especially when it's an open-source piece of software). There's so much good open-source software for engineers and developers, and there's even a lot of good "productivity" software, but what I hear keeps a lot of people from switching are things like Photoshop that are just not available on Linux ([although](http://www.gimp.org/) [alternatives](https://krita.org/en/) [exist](https://inkscape.org/)). I'm not sure how Darktable compares to other similar products but, when I come across tools like this I try to raise awareness about them so that people who _are_ qualified to compare them are at least know they exist. (via [Linux Voice Podcast](https://www.linuxvoice.com/category/podcasts/) ) 

## 3 - NIO EP9 Supercar sets records without puny human brains

![m9p1xar7xzadqbpgpe8i](/wp/2017/03/m9p1xar7xzadqbpgpe8i.jpg) <http://jalopnik.com/how-this-electric-supercar-broke-a-track-record-without-1793178665> The main reason I like this article is that it gets into some of the challenges of pushing electric vehicle performance beyond the ([admittedly high](https://www.youtube.com/watch?v=3pCrA7F_Ghc) ) limits of production EV's. The autonomous part is interesting, but honestly making a car drive itself around a racetrack is not exactly rocket science compared to making it work on the street.  That said it's cool and probably safer to use robot test pilots for this sort of thing. (via [Jalopnik](http://jalopnik.com/how-this-electric-supercar-broke-a-track-record-without-1793178665) ) 

## 4 - The ALTAIR Shield

![9612471488994766508](/wp/2017/03/9612471488994766508.jpg) [https://hackaday.io/project/20011-altair-8800-front-panel-ardiuno-shield](https://hackaday.io/project/20011-altair-8800-front-panel-ardiuno-shield] What can I say, I'm a sucker for [blinkenlights](https://en.wikipedia.org/wiki/Blinkenlights) and I'll post everything new thing I find about the ALTAIR. This board is an [Arduino shield](https://www.arduino.cc/en/Main/ArduinoShields) that replicates the front panel user interface of the [ALTAIR 8800 personal computer](https://en.wikipedia.org/wiki/Altair_8800).  Pop this on your [Arduino Due](http://astore.amazon.com/jjg00-20/detail/B00A6C3JN2), load the [appropriate software](https://www.hackster.io/david-hansel/arduino-altair-8800-simulator-3594a6) and you yourself a pocket-sized personal computer revolution. (via [Hackaday](http://hackaday.com/2017/03/14/the-altair-shield/) ) 

## 5 - Sequencing the genes of the things that live inside you

<https://blog.adafruit.com/2017/03/12/thryveinside-microbiome-reports-and-probiotics/> I don't know a lot about the application for these things, but it appears that there are a number of companies who are sequencing the genome of the various things that live in your gut.  Given how mysterious the digestive system seems to be to doctors, more data from this area can only be a good thing. My only reservation is that I hope they make the data or results something the rest of us can use, and don't turn it into another piece of private intellectual property. (via [Adafruit](https://blog.adafruit.com/2017/03/12/thryveinside-microbiome-reports-and-probiotics/) )---
tags:
  - gullicksonlaboratories
title: FindDay #4
link: http://jjg.2soc.net/2017/03/24/findday-4/
author: jgullickson
description: 
post_id: 1165
date: 2017-03-24 09:13:24
created_gmt: 2017-03-24 14:13:24
comment_status: open
post_name: findday-4
status: publish
post_type: post
---

# FindDay #4

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Mooshimeter Wireless multimeter

<http://www.dragoninnovation.com/projects/34-mooshimeter> https://youtu.be/8AK3nu-59LQ This might sound illogical at first, but how many times have you had your scope, meter, etc. connected to something but then had to walk away from it to trigger the system you're monitoring? The most obvious example to me is when you're working on a car, but the same thing is true for anything big enough to block your vision (HVAC, washer/dryer, etc.). This little device lets you use your phone as the display for a basic multimeter/Oscilloscope. It's not the most high-performance device available, but it's inexpensive ($100) and looks pretty durable. (via [Hackaday](http://hackaday.com/2014/01/08/mooshimeter-the-why-didnt-i-think-of-that-multimeter/) ) 

## 2 LitePlacer

<http://www.liteplacer.com/> ![150594-76](/wp/2017/03/150594-76.jpg) While researching DIY pick-and-place machines I came across LitePlacer. It's the first sort of professional-grade device of it's kind aimed at the (large) desktop market that I've seen. What I'm looking for is something smaller and open-source, but I was impressed with the quality of the LitePlacer device, documentation and non-obnoxious website. 

## 3 Feather 32u4 FONA

<https://www.adafruit.com/products/3027> ![3027-01](/wp/2017/03/3027-01.jpg) I've been looking for something like this device for a long time. I've made several different runs at designing a device to replace my smartphone but all of these projects run out of gas because what I'm able to cobble-together out of discrete boards and modules ends up being more than I want to carry around in my pocket. This leads down the road of designing custom boards (with surface-mount parts) and while there's nothing wrong with that, so far I haven't made it through the other side of that tunnel. So when I came across the Feature 32u4 FONA I was very excited. The device combines a SIM800 cellular module, an ATMega 32u4 microcontroller and a lithium-ion battery charger into a single integrated board with all of the necessary circuitry to glue it all together. These three things make up the bulk of the parts necissary to make a phone-like device, minus the input/output components and any other specialized bits. The ATTMega 32u4 might be a little under-powered for some of my previous designs, so I've started an entirely new design that is centered around being the minimal viable replacement for my smartphone. I've got a couple other projects in-progress that I need to wrap-up before I dive into smoething new, but I've been waiting a long time for something like this so I'm hoping to get started on it soon. 

## 4 Retrofuturistic arcade cab

<http://technabob.com/blog/2015/05/23/retro-futuristic-arcade-cabinet-pixelkabinett-42/> ![pixelkabinett_42_arcade_cabinet_by_love_hulten_1](/wp/2017/03/pixelkabinett_42_arcade_cabinet_by_love_hulten_1.jpg) I ran across this while researching cabinet designs for the [Charity Arcade](https://jjg.2soc.net/category/charity-arcade/) project. The design wasn't right for my project, but it was so cool that I wanted to share it. 

## 5 Hangprinter

<https://github.com/tobbelobb/hangprinter> https://youtu.be/ULJqLSTriRY This is the most exciting thing I found this week. Every since I built my first 3D printer, I've wanted to build another one. I've started a few, but for various reasons I never seem to finish them. I think the biggest reason is that in the amount of time it takes me to build one, the technology improves and I loose interest in building whatever I started. The Hangprinter strikes a balance between simplicity (at least in terms of construction) and capability that I think will easily remain relevant long enough for someone like me to finish building it. With potentially unlimited build volume, the Hangprinter is unlikely to run out of build area even for the largest jobs. Paradoxically, since the entire printer is housed in the "print head", it's incredibly portable as well. The design isn't ideal for everything (fine details are not its strong point) but in terms of building large parts and doing it on-site, it would be hard to beat. It's also exciting to see some innovation in the 3D printing field as opposed to just evolutionary improvements on previous designs. (via [Hackaday](http://hackaday.com/2017/03/20/hanging-3d-printer-uses-entire-room-as-print-bed/) )---
tags:
  - gullicksonlaboratories
title: FindDay #5
link: http://jjg.2soc.net/2017/03/31/findday-5/
author: jgullickson
description: 
post_id: 1457
date: 2017-03-31 09:19:17
created_gmt: 2017-03-31 14:19:17
comment_status: open
post_name: findday-5
status: publish
post_type: post
---

# FindDay #5

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._  

## 1 - P.O.R.T.A.L

<https://github.com/grugq/portal> PORTAL is an open-source firmware for routers which aims to provide a hardware device that protects Internet privacy by sitting between your computer(s) and your ISP connection. It's sort of a do-it-yourself [Anonabox](https://www.anonabox.com/).  

## 2 - Ring

<http://www.linux-magazine.com/Online/Features/Time-to-Ring-In-Some-Changes> Ring is an open-source secure communications project that provides a more private alternative to things like Skype, Google Hangouts, etc. (via [Linux Magazine](http://www.linux-magazine.com) )  

## 3 - I2P

[https://geti2p.net/en/](https://geti2p.net/en/} The Invisible Internet Project (I2P) is not new, but I have a renewed interest in it in light of recent and ongoing events. The idea is to create a better Internet on top of the existing one. There's been a number of projects who have attempted this using various approaches, but I2P is a very mature project and is usable today. I like the low-level approach and the fact that it doesn't rely on a lot of cleverness to get the job done. I'll probably be talking more about it soon. (via [The Tin Hat](https://thetinhat.com/) )  

## 4 - Tesla Model 3 "cruising around"

<http://jalopnik.com/heres-tesla-model-3-release-candidate-cruising-around-1793607211> A few seconds of a Tesla Model 3 on the street. (via [Jalopnik](http://jalopnik.com/heres-tesla-model-3-release-candidate-cruising-around-1793607211) )  

## 5 - Moai 3D Printer

<https://www.kickstarter.com/projects/1554809440/moai-affordable-high-resolution-laser-sla-3d-print> ![header](/wp/2017/03/header.jpg) It' wouldn't be FindDay without at least _one_ 3D printer link, right? There's no shortage of Kickstarter 3D printer disasters, and this could be just another one but I've heard from several reliable sources that this project is legit. There's been a few resin-based open-source 3D printers in the last few years but most of them rely on DLP projectors which makes them somewhat expensive and limits their resolution to what's available from the projector. Where other printers use a projector, Moai uses lasers; what more needs to be said? (via [Hackaday](http://hackaday.com/2017/03/26/mrrf-17-laser-resin-printers/) )---
tags:
  - gullicksonlaboratories
title: FindDay #6
link: http://jjg.2soc.net/2017/04/14/findday-6/
author: jgullickson
description: 
post_id: 1505
date: 2017-04-14 10:39:39
created_gmt: 2017-04-14 15:39:39
comment_status: open
post_name: findday-6
status: publish
post_type: post
---

# FindDay #6

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Beautiful Programming

<http://beautifulprogramming.com/> ![Screenshot_20170414_103338](/wp/2017/03/screenshot_20170414_103338.png) This is a really cool collection of computer-generated art. One of the neatest parts is that each piece is interactive, and you can view the source code to see how they work. 

## 2 - Ballerina

<http://ballerinalang.org/> ![Screenshot_20170414_103456](/wp/2017/03/screenshot_20170414_103456.png) I've just started to scratch the surface of Ballerina but what I've seen so far is very exciting. I ran across it while searching for beauty in programming, and I have to say it's quite elegant. Ballerina aims to provide a better way of building server-side things like API's, scheduled jobs, etc. It does this through a specialized programming language and visual development environment. I haven't seen a lot of people talking about Ballerina yet, which is surprising. It appears to provide a very useful solution to a very common problem in developing web-based and other network applications, and I look forward to spending some more time with it and seeing what it can do. 

## 3- Rebble

<http://rebble.io/> I'm very excited about Rebble.io. I was very disappointed when I found out that Pebble was shutting down. I was also disappointed when I found out that what was left of them was getting bought by Fitbit. I've always thought that the "pivoting" of Pebble toward the fitness tracker market was a mistake and a squandering of the far-more-broad potential of the device, even if the market wasn't ready for that yet. Rebble is an ambitious effort to continue the Pebble platform by recreating the parts that went away (apps, the appstore, even firmware) as open-source software. They count several notable "Pebblers" amongst their ranks and have already made impressive strives toward these goals (there are even proposed hardware designs for future devices). 

## 4 - Siemens record-setting electric airplane

<https://blog.adafruit.com/2017/04/12/record-setting-electric-plane-tows-glider-up-into-the-sky-in-seconds/> ![isiemens-airbus-electric-plane-world-record-glider-3](/wp/2017/03/isiemens-airbus-electric-plane-world-record-glider-3.jpg) I'm a fan of pretty much anything EV, and also airplanes, so it's no surprise that I thought this was cool. There's a number of applications where the advantages of electric drive make sense for aircraft, and in some cases they are a lot more practical than electric cars. (via [Adafruit](https://blog.adafruit.com/2017/04/12/record-setting-electric-plane-tows-glider-up-into-the-sky-in-seconds/) ) 

## 5 - Mastodon

<https://en.wikipedia.org/wiki/Mastodon_%28software%29> ![220px-mastodon_social_logo](/wp/2017/03/220px-mastodon_social_logo.png) Over the last week I started experimenting with the Mastodon social network. Mastodon isn't new, and the underlying network tech. is even less new, but for whatever reason it's getting traction, and as an open-source, decentralized alternative to things like [birdsite](https://twitter.com/) I'm excited about it. I think the easiest way to understand what makes Mastodon different is that it works more like email, where there are multiple, independent servers (they even adapt the same name convention, user@domain.com) which interact with each other via a standard set of protocols. This makes the users on a given server akin to "neighbors", who can also follow users on other servers and as such distribute their messages. There's a lot more to say about Mastodon and its usefulness has less to do with the technology than the fact that there seems to be a lot of interest in it at the moment. Perhaps it's just a bubble but I'm always excited about the idea of people migrating away from closed, private, proprietary and centralized systems and moving to something open and more autonomous. I'm currently camping on a server hosted by a friend-of-a-friend (@jjg@resistodon.com), but if there's any interest I'm considering setting up a server of my own and welcoming any friendly refugees to join in.---
tags:
  - gullicksonlaboratories
title: FindDay #7
link: http://jjg.2soc.net/2017/04/21/findday-7/
author: jgullickson
description: 
post_id: 1914
date: 2017-04-21 08:00:00
created_gmt: 2017-04-21 13:00:00
comment_status: open
post_name: findday-7
status: publish
post_type: post
---

# FindDay #7

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Gobot

<https://gobot.io/> ![gopher](/wp/2017/04/gopher.png) I've been looking for an excuse to do some Go programming for years. A friend on [Mastodon](https://resistodon.com/web/accounts/2189) pointed out the [Gobot project](https://gobot.io/), which is designed to make it easy to use Go to power robots. I'm still in the process of learning enough Go to give this a try, but if I can get up-to-speed, I'm going to test it out when it comes time to add a brain to the [Reelbot project](https://jjg.2soc.net/category/reelbot/). (via [U+039B](https://resistodon.com/web/accounts/2189) )  

## 2 - Interplanetary File System (IPFS)

<https://ipfs.io/> I'll be honest, I ignored IPFS for a long time because I thought the name was dumb. However, after deciding to move past that and spend some time checking it out, it turns out that it is a lot like what I had in mind for [JSFS](https://github.com/jjg/jsfs) federation. From a technical perspective, it sets up a local HTTP server which you talk to from your local web browser, etc. and in turn it acts as a gateway to a distributed file sharing network. There's a lot of applications for such a thing, but the most obvious is that it is a way to publish web pages (and their associated resources) via a network of individual computers vs. using monolithic web servers. This opens up the possibility of returning the Internet to a more autonomous mode of operation. Personally, I'm going to be investing some time in learning about and applying IPFS to my daily operations, and I'll probably be writing more about how others can join me. This also may mean the end-of-the-line for JSFS. I don't expect IPFS to match JSFS in terms of performance, but in the long-run it appears to solve the same class of problems I wanted to solve with JSFS, so my energies may be better spent contributing to the development and application of IPFS.  

## 3 - Laser Network

<http://blog.svenbrauch.de/2017/02/19/homemade-10-mbits-laser-optical-ethernet-transceiver/> ![board_front](/wp/2017/04/board_front.jpg) Back in the Roaring 90's I dreamt of setting up a WAN with my neighbors on the west side of Madison powered by lasers, but never got around to it. Partly because back then it was prohibitively expensive, and partly because by the year 2000, most of us had access to broadband Internet at home. However, there's still reasons to setup a long-range wireless network, and this project is a very nice, clean example of how to do it with lasers. (via [Hackaday](https://hackaday.com/2017/04/19/go-wireless-with-this-diy-laser-ethernet-link/) )  

## 4 - Forth on Microcontrollers

<http://hackaday.com/2017/04/19/moving-forth-with-mecrisp-stellaris-and-embello/> ![had-forth-login21](/wp/2017/04/had-forth-login21.png) I've been interested in the [Forth programming language](https://en.wikipedia.org/wiki/Forth_%28programming_language%29) (and runtime environment) ever since I learned about it being a part of the [Canon Cat](https://en.wikipedia.org/wiki/Canon_Cat). It seems like a great fit for microcontrollers, and this post walks you through the process of doing just that. (via [Hackaday](http://hackaday.com/2017/04/19/moving-forth-with-mecrisp-stellaris-and-embello/) )  

## 5 - Printable Hydraulic Robots

<https://www.youtube.com/watch?v=3EAMCqH31Vo> https://www.youtube.com/watch?v=3EAMCqH31Vo By combining multi-material 3D printing and capturing fluid inside the print as its made, engineers from MITCSAIL are able to create complex, working machines that can be 3D printed and put to use with no assembly required.---
tags:
  - gullicksonlaboratories
title: FindDay #8
link: http://jjg.2soc.net/2017/04/28/findday-8/
author: jgullickson
description: 
post_id: 2059
date: 2017-04-28 08:41:35
created_gmt: 2017-04-28 13:41:35
comment_status: open
post_name: findday-8
status: publish
post_type: post
---

# FindDay #8

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - PiVPN

<http://www.pivpn.io/> There's been a lot of talk about VPN's recently due to changes in Internet privacy laws in the U.S. and U.K. (and certainly elsewhere as well). However, VPN's are notoriously difficult to setup and with good reason; a mis-configured VPN can give you a false sense of security. PiVPN aims to solve this by applying the philosophy of [PiHole](https://pi-hole.net/) (a network-wide ad-blocker that runs on Raspberry Pi) to setting up your own VPN. In addition to making it easy, PiVPN goes to great lengths to make sure your VPN is set up _correctly_ as well. If you don't know why you might want a VPN, that's a longer discussion, but if you've been considering setting up your own VPN but don't know where to start, this is a pretty cool option. **Note:** Like PiHole, PiVPN asks the user to run a script with superuser privileges from their website, and this is almost unilaterally a bad idea. This reason it's a bad idea is that if someone on the other end is malicious, they can essentially have their way with your computer. For this reason it's a good idea to read the script and make sure it's not doing anything you don't want it to. However, if you could read that script, you probably don't need PiVPN, so you kind of have to trust these guys. This is one of the advantages of using a dedicated device like a Raspberry Pi for something like this, because if it turns out to be evil, you can destroy it without mercy. (via [Linux Voice Podcast](https://www.linuxvoice.com/category/podcasts/) )  

## 2 - Reprap Helios

<https://hackaday.io/project/21355-reprap-helios> ![6894621492836289680](/wp/2017/04/6894621492836289680.jpg) I'm always excited to see a new [Reprap](http://www.reprap.org/) design. The "classic" machines get the job done, but innovation has always been one of the key the advantages of open-source designs. If you've seen other Repraps before, you'll notice there's something different about Helios. I remember seeing a [SCARA](https://en.wikipedia.org/wiki/SCARA)-based Reprap design a few years ago. I'm not sure if Helios is a descendant of those designs or something entirely new, but I'm glad to see progress being made in this direction. Aside from just being different, SCARA robots have multiple applications and it's at least theoretically possible that a SCARA Reprap could handle more than just laying down plastic, opening the door for expanding its self-replicating capabilities.  

## 3 - Fish-O-Matic

<https://www.hackster.io/the-fish-o-matic-team/fish-o-matic-364581> ![totaalfoto_6c6wmodbnl](/wp/2017/04/totaalfoto_6c6wmodbnl.jpg) The idea behind Fish-O-Matic is to grow food continuously using a self-contained apparatus containing both plant and animal life to create a symbiotic system. I've seen commercial implementations of closed-ecosystem farming like this, and I've run across a couple DIY projects as well, but what stood out to me about Fish-O-Matic is how complete the project description is. Of the ones I've seen so far, this one seems the most reproducible. (via [Hackster.io Newsletter](https://www.hackster.io) )  

## 4 - ARBox

<https://gitlab.s1.0x39b.fr/lambda/arbox> ![screenshot](/wp/2017/04/screenshot.jpg) ARBox is a simple augmented reality application that shows you what's in your storage bins without having to open them. If you're like us, you have shelves of these things and having to pull them off the shelf and open them to find something is a drag. While still in the experimental stage, I think there's a lot of potential in something like ARBox. The first thing that comes to my mind is a way to share the database of "stuff" with friends so once everything is inventoried, when you need something you could do a search and see if one of your friends already has what you need squirreled-away in a tub somewhere.  

## 5 - Void Star

<https://www.wired.com/2017/04/void-star-dystopia-computational-linguistics/> I haven't read [Void Star](https://us.macmillan.com/voidstar/zacharymason/9780374285067/) yet, but based on this article about the author, I am optimistic about there being a good contemporary science fiction book about AI.---
tags:
  - gullicksonlaboratories
title: FindDay #9
link: http://jjg.2soc.net/2017/05/05/findday-9/
author: jgullickson
description: 
post_id: 2272
date: 2017-05-05 08:39:24
created_gmt: 2017-05-05 13:39:24
comment_status: open
post_name: findday-9
status: publish
post_type: post
---

# FindDay #9

_[Every Friday](https://jjg.2soc.net/category/findday/) I share the five coolest things I've found on the web in the last week._

## 1 - Homemade Liquid Nitrogen Generator

<https://www.instructables.com/id/Homemade-liquid-nitrogen-generator/> ![fmi7p8rhvfl6hop-large](/wp/2017/05/fmi7p8rhvfl6hop-large.jpg) Who doesn't like some good-old-fashioned homemade liquid nitrogen?  Perfect for  cooling your superconducting electromagnets on a hot summer day!   

## 2 - DIY Segway

<https://www.instructables.com/id/Rideable-Segway-Clone-Low-Cost-and-Easy-Build/> https://www.youtube.com/watch?v=pSHOa9Gl2eM Remember the $8,000.00 Segway scooter?  Build your own and enjoy the pleasures of [Segway Polo](https://en.wikipedia.org/wiki/Segway_polo) for a fraction of the cost!   

## 3 - Machinekit

<http://www.machinekit.io> As the About page explains, Machinekit (a fork of [LinuxCNC]() ) is a complete platform for machine control.  I heard about Machinekit from my friend Dennis during a conversation about getting my Craigslist CNC mill online. Machinekit goes further than things like [GRBL](https://github.com/grbl/grbl) or [Marlin](https://github.com/ErikZalm/Marlin) and provide everything from stepper motor control signals all the way up to graphical tools for operating and monitoring CNC machines. I haven't experimented with it yet, but I'm hoping it will help me get over the hump of replacing the proprietary brains of my mill and I can start putting it to use.   

## 4 - Netherlands to Australia in a converted EV

https://www.youtube.com/watch?v=Ex585HR0wBs Watch a converted internal-combustion Volkswagen make an epic journey   

## 5 - Gizmoplans

<http://gizmoplans.com/> ![scoot-car_1](/wp/2017/05/scoot-car_1.jpg) I found this website while searching for information about this awesome electric scooter/car on the cover of the book "[101 Things That Go Fast](https://www.amazon.com/Popular-Mechanics-Things-That-Fast/dp/1618370820)" (frustratingly the "thing" on the cover isn't included in the book!!!). Gizmo, on the other hand, [has the plans](http://gizmoplans.com/how-build-electric-scoot-car). It's kind of frustrating that you have to pay for them, but I will say that the plans appear to be OCR'd into their PDF form which makes them a bit more accessible than if they were simple scans.  So some effort has gone into publishing these. There's a lot of plans available, and even though many of them may be found somewhere else online, I was happy to find somewhere I could find plans for the Scoot-Car.---
tags:
  - gullicksonlaboratories
  - rain
title: Fits and Starts
link: http://jjg.2soc.net/2018/02/17/fits-and-starts/
author: jgullickson
description: 
post_id: 5077
date: 2018-02-17 10:00:32
created_gmt: 2018-02-17 16:00:32
comment_status: open
post_name: fits-and-starts
status: publish
post_type: post
---

# Fits and Starts

I redesigned the 3d printed parts of the compute module so they can be printed within the build envelope of the [Monoprice Mini Delta](https://mpminidelta.monoprice.com/) (I really need to write a review of that printer sometime). The reason for this is that I need to iterate on these modules faster and due to it being winter the ambient temp in the lab makes it hard to print things this large and flat on my Reprap without curling. ![IMG_2045](/wp/2018/02/img_2045.jpg) I did some test fitting with the new modules and ran into a couple of problems that necessitate some changes to both the modules and the front panel design. Due to Ethernet switch interference, about 40mm of room on the front panel for module brackets is lost. Based on initial layout tests, this gap falls on the right-hand side of the chassis. Because of this, I can only fit 7 modules at the current module size (25mm). ![IMG_2066](/wp/2018/02/img_2066.jpg) ![IMG_2064](/wp/2018/02/img_2064.jpg) We also lose ~6mm on the left due to the screw mounting holes for the front panel. If I can get the module width under 24mm I might be able to make 8 modules fit, but it will probably require the left-end module to be tweaked to fit around the panel mounting screws, and will require a change to how I had planned to mount the power rails to the panel. While I was at it I fit the modules together on the power rails and noted a number of improvements to be made. ![IMG_2050](/wp/2018/02/img_2050.jpg) ![IMG_2053](/wp/2018/02/img_2053.jpg)

  * Add 1mm more gap for nut and power lugs
  * Reduce power rail hole by 1-2mm
  * Power lug interferes with board on the top, maybe push board back 1-3mm
  * Power lug interferes with the board at the bottom too, maybe sink board another 1-2mm into brace? Or, orient power lugs vertically, then they clear the board and provide a clearer path for the power wiring
  * Power rail "bosses"(?) could have thinner walls to make more room for the wire barrel part of the power lugs (they are OK when they print correctly but print errors make the fit too tight).
  * LED holes seem too small, but could be due to printer error
  * Switch hole is too small, could be print error or might just need .5mm more room
  * Bus connector could be raised about 1-2mm from bottom plate
  * USB connector clears the switch but barely. The electrical connectors on the switch are accessible on a regular Pine64 but may be occluded on a board with WiFi
  * Overall thicknesses could probably be reduced, the part is plenty strong and the stack-up on the front panel might be too deep for switches and LED's to poke-through at the current thickness
in addition to the changes above, I need to tweak the mounting holes for the front panel LED's and switches to match the layout of the lasercut panel. There's probably a clever way to figure this all out in software, but since I'm using two different programs to model the modules ([OpenSCAD](http://www.openscad.org/)) and front panel ([Inkscape](https://inkscape.org)) I'm not aware of a simple way to do this. Even if I could it would probably take some cut-and-try work because parts don't come off the printer exactly how they look on-screen, and I imagine there is some tolerance to consider in the lasercut parts as well. The good news is that I have everything I need to iterate on this design "quickly" in the lab, so with some luck I should be able to get this sorted in my free time and with any luck have something that fits together nicely by the end of the month.

## Comments

**[Shane](#139 "2018-03-22 16:36:43"):** Hi Jason! I hope you find the comments useful! I just put together a Delta printer- a Tevo Little Monster. I am still going through the process of bringing it up, but it seems to be pretty accurate. It has a crazy large build volume- 340mm circular by 500mm vertical. I have a high power rocketry hobby for which I have purchased the Tevo to make avionics bay components and general parts. Not sure I’m going to use it for aerodynamic parts, especially for Mach speed plus airframes. But I digress... 🤨 A material other than ABS that gains favor is many circles for its dimensional stability is PLA- poly lactic acid. It is supposedly stronger than ABS, but it is biodegradable- OK for chochkies, but I’m not convinced I want to bother with it if it can break down over time.

**[Jason J. Gullickson](#137 "2018-03-22 14:11:40"):** Thanks for the feedback Shane! I spent a lot of time doing this sort of calibration on the Reprap printer I built, but I have to admit I haven’t done much with the printer I made these parts with (a Monoprice delta). I could probably improve the tolerance if I took some time and learned to calibrate it (the delta is so much different I don’t know much about tuning it yet).

**[Shane](#135 "2018-03-21 16:21:41"):** The dimensional errors you are experiencing with the 3D printed parts may be due to not having the 3D printer calibrated. Try making a cube 12.5 2mm on a side (0.5”) in your CAD software and print it out. Get a good pair of calipers ($25 or so at a place like Harbor Freight) and measure the printed cube in the X, Y, & Z dimensions. My printers print this calibration cube accurately down to 3 decimal places. Once you know your printer is calibrated, make sure you allow for some tolerance for things like the diameter of the LED- make the hole about 1/3- to 1/2 mm larger than the LED’s diameter measured on the body, not the base of the LED. Same for the switches, screws, nuts, & bolts you are using. Also, be aware the ABS plastic has something like 3% dimensional change due to temperature from hot to cold, so make sure all your measurements are taken when the part has cooled to room temperature. This should help you reduce your interactions to bare minimum. As an example for encouragement, I can tell you that I now largely can use my first prototype as a “final production “ functional part. I wish you luck with your project!

---
tags:
  - gullicksonlaboratories
title: FOSS isn't capitalism
link: http://jjg.2soc.net/2018/06/13/foss-isnt-capitalism/
author: jgullickson
description: 
post_id: 5195
date: 2018-06-13 12:35:36
created_gmt: 2018-06-13 17:35:36
comment_status: open
post_name: foss-isnt-capitalism
status: publish
post_type: post
---

# FOSS isn't capitalism

It's storytime children. Once upon a time there were no personal computers. All the electronic computers (going forward simply "computers") were big and expensive and as such were owned by corporations, universities or the government. If you wanted to use one, you had to get permission and share. That changed around 1970 with the invention of the microprocessor. Intended for use in calculators, some creative hackers used them to build programmable "microcomputers" for a fraction of the cost of any existing "real" computer. Having an entire computer to yourself (even if it wasn't very powerful) made it a _lot_ easier to learn how to use it. Back then "using" a computer meant programming it first, so these early hackers started building tools to make programming easier and shared the programs they wrote with one another. Eventually this lead to most personal computers having the [BASIC](https://en.wikipedia.org/wiki/BASIC) programming language built-in. As companies began to make and sell these microcomputers, people bought them with the expectation that they could program them to do whatever they needed them to do, and the programs they wrote in BASIC were more-or-less portable between machines even if they were made by different companies and used different hardware. This is the era in which I learned to program. It was a very exciting time because for me, software was the first thing I could build that didn't require consumable (and therefore costly) supplies. The versatility of a general-purpose computer that you had all to yourself seemed infinite, and I learned everything I could about my machine. At the same time there were plenty of people who were writing software who wouldn't consider themselves programmers. These were people who ran small businesses, hotels, cleaning services, automobile service garages, etc. who for one reason or another saw how they might make their jobs easier or better by buying a computer and writing a program to automate their work. You still come across these machines occasionally in the wild and in most cases they are plugging-away at whatever task their owners originally programmed them for. In those days I might have a conversation with one of these people (typically adults who were friends with my parents) about a program they were working on or to share something I was experimenting with. We could have a conversation about the technology in the context of what they actually were going to _do_ with it as opposed to obscure details about the machine itself. In many public schools students were required to take computer classes and learn how to program as an _introductory_ course. However something went very wrong shortly after this. The generation who grew-up programming their own computers figured out that they could sell the software they wrote, and the harder it was to write the software, the more money they could make selling it. So instead of finding ways to make it _easier_ for regular people to write their own software, they made it _harder_. This disenfranchised so many people that eventually personal computers stopped shipping with any programming languages at all, and users came to accept their role as consumers of software written by "programmers" who were of course very smart and knew much more than the users did. I would argue that this was a "dark ages"-level event. The potential I saw in computers wasn't that it could make another stripe of "experts" rich, it was that with the advent of the personal computer the power of writing software could be experienced by _everyone_, and as such new and exciting applications for computers would spring-up all over the place. As the 1980's drew to a close the diversity of applications for personal computers shrank, only occasionally expanding when some new threshold of hardware performance was reached. There were of course some who operated outside of this mainstream and who saw the potential for the personal computer being squandered. These individuals, these "hackers" are where [free and open source software](https://en.wikipedia.org/wiki/Free_and_open-source_software) (FOSS) emerged which was seen by people like me as a path back to a time when the power of personal computers was in the hands of their owners. But we didn't quite make it back. By the time that usable open-source systems became available, most users had been convinced that you had to be a professional programmer to write software for a computer. To a degree this was correct, because the tools and languages used by open-source developers had become specialized and nothing as accessible as the BASIC of old existed any longer. So while FOSS made it _theoretically_ possible for users to create and modify their own software, there was both a psychological and skills gap that make this _impractical_ for most users. When the web came along there was a brief moment of hope that this new platform might be as accessible and compatible as the microcomputer of old, but it was quickly comoditized by commercial interests and followed the same path as all previous software, devolving into a labyrinth of specialists tools and "languages" in an endless parade of "fashionable" platforms, frameworks, etc. Now we find ourselves in a state where [criticisms of FOSS](https://medium.com/@cassolotl/on-foss-as-a-capitalism-like-structure-baf89973c6a1) are trying to convince users they are better served by commercial, proprietary software. This isn't because of something inherent in FOSS itself, but due to the fact that we haven't succeeded in making the FOSS solution accessible to all users. The implication is that this is something FOSS developers desire, to leverage their skill to wield power over the users of their software. I don't think this is true in general, but I can see how it might appear that way. Most of the FOSS developers I've met are selfless and write the software that they want or need. If it proves useful to others they will accommodate those users requests within the limits of their resources and/or desire to continue working on the software. These developers are rarely paid for this work, and often do it at significant personal expense. I don't think they are out to stick-it to users or capture and amass power as is the design of capitalism. That said I believe that we've failed to fulfill the freedoms afforded by free software by not finding ways to make creating and modifying the software more accessible. For this we deserve some condemnation. Some of us may not agree with this, but driving users back into the arms of proprietary software isn't good for any of us, so it is in our best interest to address this problem, and I think the best way to do so is to pour some engineering skills into empowering everyone to write the software they need, without having to become a "programmer".---
tags:
  - gullicksonlaboratories
  - rain
title: Front Panel Software
link: http://jjg.2soc.net/2018/03/14/front-panel-software/
author: jgullickson
description: 
post_id: 5113
date: 2018-03-14 10:03:00
created_gmt: 2018-03-14 15:03:00
comment_status: open
post_name: front-panel-software
status: publish
post_type: post
---

# Front Panel Software

The front panel of RAIN-PSC serves three essential purposes: 

  1. Show the status of each node
  2. Show the load on each node
  3. Look really cool
The panel is actually eight individual control panels (one for each node in the cluster). Each panel consists of five LEDs and a toggle switch. The switch selects between two display modes: **status** and **load**. When the switch is in the **status** position, each LED indicates the following: 

  * `boot` (**on** when the os has booted successfully)
  * `network` (**on** when the node has successfully connected to the network)
  * `temp` (**on** when the node temperature is too high to run at full-speed)
  * `user 1` & `user 2` (used to indicate custom status selected by the programmer)
When the switch is in the **load** position, the LEDs behave as a bar-graph displaying the [unix load](https://en.wikipedia.org/wiki/Load_\(computing\)) of the node. To provide this display the software needs to be able to: 

  * Poll the status of each monitored subsystem (os, network, etc.)
  * Read the system load
  * Read the toggle switch position
  * Turn the LEDs on and off
When I started putting the electronics together, I used some command-line tools to interact with the LEDs and switches (I think it's possible to write do all of the above in a shell script). In the long-run, I'll probably write this in something faster/more efficient (Rust?) but for now, I'm going to use Python to get the hang of talking to the new hardware. I'm installing a few things on top of the base Armbian to make this happen: 
  * python
  * python-dev
  * python-pip
  * python-smbus
The source code for the current version of the software [can be found here](https://gitlab.com/jgullickson/rain/blob/master/mark-ii/psc/software/panel.py). The script can be broken-down into three primary components: 

  1. Functions to gather system information
  2. Functions to read and update the front panel components
  3. A loop to periodically update the display
Gathering system information using Python is a pretty well-worn path, so I won't discuss that in detail here. Reading the position of the toggle switch and turning the LED's on and off is done using the [smbus Python package](https://pypi.python.org/pypi/smbus-cffi/0.1). This package interacts with the bus in much the same way as the command-line i2c tools. The hardest part of this for me is coming up with the best way to translate between the binary representation (the pins themselves), the boolean/decimal values I'm used to working with and the hexadecimal values that glue the two together. What I settled on was using hexadecimal internally to the functions which generate the display (`display_status()`, `display_load()`) and boolean/decimal values everywhere else. At some point I'll abstract all this away into a library or a module, but since I don't plan on using Python for this long-term I'll probably hold-off on that for now. [caption id="attachment_5117" align="aligncenter" width="783"]![octomonitor](/wp/2018/03/octomonitor.png) Remote hardware debugging thanks to Octoprint's webcam...[/caption] Finally, the main loop simply loops forever, calling `toggle_on()` to determine the position of the status/load switch and then calling `display_load()` or `display_status()` accordingly. Once the display is updated the loop `sleep()`s for one second and then starts over. In its final form this will need to update the panel much faster than once-per-second (potentially leveraging interrupts as well), but for this version this is probably fast enough. 

## References

  * https://pypi.python.org/pypi/smbus-cffi/0.1
  * http://raspberrypi.link-tech.de/doku.php?id=mcp23017
  * https://cdn-shop.adafruit.com/datasheets/mcp23017.pdf
  * http://cagewebdev.com/raspberry-pi-showing-some-system-info-with-a-python-script/
---
tags:
  - gullicksonlaboratories
title: gitprintable
link: http://jjg.2soc.net/2017/03/25/gitprintable/
author: jgullickson
description: 
post_id: 1403
date: 2017-03-25 08:12:46
created_gmt: 2017-03-25 13:12:46
comment_status: open
post_name: gitprintable
status: publish
post_type: post
---

# gitprintable

I started a little project about a week ago called [gitprintable](https://github.com/jjg/gitprintable). There's not really anything to it yet, but I wanted to post something about it so I don't forget about it again. The idea is simple: a way to browse and search for printable models people have uploaded to [Github](https://github.com). 

## Why?

There's a few reasons. One is that, while you can use the Github website to search for models there isn't a great UI for doing this, and in order to "see" (visualize the models in) the results you have to root around a bit. Another reason is that the specialized repositories for printable models (Thingiverse, [YouMagine](https://www.youmagine.com/), etc.) are proprietary, centralized and privately-owned systems. There are numerous potential reasons this is a problem (several of which have come to pass) and as such, providing an alternative that isn't owned and operated by a 3D printer company is a good idea for the community. If gitprintable can provide a more usable interface to Github for creators and consumers of printable models, that could help move things away from these proprietary repositories*. I'm not sure how swiftly this project will move along, but if there's any interest in it beyond myself I might try to dedicate some time to it over the next few weeks. _Of course, Github is also a proprietary, privately-owned centralized system as well but its financial interests are less entangled with the interests of those creating printable models than the alternatives. That said, I'm keeping this in mind and would like to minimize gitprintable's dependence on Github and focus on integrating with the version control system [Git](https://git-scm.com/) (which is a distributed system _and_ an open-source project)._---
tags:
  - gullicksonlaboratories
  - rain
title: Hackaday Prize Entry 2018
link: http://jjg.2soc.net/2018/04/06/hackaday-prize-entry-2018/
author: jgullickson
description: 
post_id: 5136
date: 2018-04-06 08:18:17
created_gmt: 2018-04-06 13:18:17
comment_status: open
post_name: hackaday-prize-entry-2018
status: publish
post_type: post
---

# Hackaday Prize Entry 2018

![logo](/wp/2018/04/logo.png) I've entered [RAIN Mark II](https://hackaday.io/project/85392-rain-mark-ii-personal-supercomputer) in the [2018 Hackaday Prize](https://hackaday.io/prize). This [isn't the first project](https://hackaday.io/projects/hacker/550) I've entered in the Hackaday Prize (I think I've only missed one year) but this is the first time I have a project which appears to have some traction in the competition. At first I had very low expectations for the competition, I wasn't sure if the project would connect with the Hackaday audience, especially the Mark II machine because it doesn't incorporate a lot of custom electronics (especially after adopting the [Clusterboard](https://jjg.2soc.net/2018/02/23/pine64-cluster-board/)), but it's received some positive feedback (they even wrote a [featured post about Mark II](https://hackaday.com/2018/03/21/everyone-needs-a-personal-supercomputer/) and at the moment is in the running to become a finalist. This is exciting for a number of reasons. The first is that it is confirmation that there is an audience for RAIN, even at this early stage in the project. I had intended to develop Mark II primarily as an experimental platform, something more manageable than Mark I's stack of full-size servers. I hadn't thought of creating a Mark II-class machine for anyone other than myself, but it seems that I may not be alone in having application for such a machine. I've also received several complements on the _aesthetic_ aspects of the project, something completely new to me. The other reason this is exciting is that winning a prize (even simply becoming a finalist) brings prize money which could accelerate the project substantially. Minimally, reaching the semi-finals would give me everything I need to complete a prototype of the Mark II machine. Going further would underwrite work on Mark III, as well as open the possibility of producing a kit (or perhaps fully-assembled) based on the Mark II design. I have to admit that having my own computer company is something I've dreamt of since childhood. It has always seemed like an impossible goal, especially to do so without the compromises associated with the typical "tech venture" process, so the idea that I could bootstrap something like this with nothing other than a compelling idea and some contest winnings is like a dream come true. In any event it's been worth the effort and has enforced some discipline on the project so regardless of how the contest goes, I think it was worth it. If you'd like to help it's not too late to [like RAIN Mark II](https://hackaday.io/project/85392-rain-mark-ii-personal-supercomputer) and help us reach the semifinals!
---
tags:
  - gullicksonlaboratories
title: Harbor Freight folding utility trailer
link: http://jjg.2soc.net/2017/06/12/harbor-freight-folding-utility-trailer-build/
author: jgullickson
description: 
post_id: 2809
date: 2017-06-12 09:16:15
created_gmt: 2017-06-12 14:16:15
comment_status: open
post_name: harbor-freight-folding-utility-trailer-build
status: publish
post_type: post
---

# Harbor Freight folding utility trailer

A few weeks ago, I assembled the [Harbor Freight Heavy Duty Folding Trailer](https://www.harborfreight.com/1195-lb-capacity-48-in-x-96-in-heavy-duty-folding-trailer-62648.html) (Item #62666) with my brother and father (and a little extra help, but more on that later). There's [a lot of information about this trailer on the Internet](https://www.youtube.com/results?search_query=harbor+freight+folding+trailer) so I'll focus on the things that I wasn't able to find out there. ![2017-06-10 08-27-21 0812.jpg](/wp/2017/06/2017-06-10-08-27-21-0812.jpg)

## The size of the shipping boxes

This seems like something that should be right on the Harbor Freight website, but for some reason it was really hard to find out how big of a box the trailer comes in. Given its final size, I assumed it would be huge. Turns out it comes in two relatively small (but heavy) boxes. The larger is 6', 20" long but not very big in the other dimensions (mine was kind of busted-up so I couldn't get a good measurement, but around 4x4"). The second box is about 2' x 2' x 1'. This box contains the wheels and is probably the heavier of the two. The total weight is around 275lbs and two people can lift each box fairly easily. It should fit in most small trucks/wagons/SUV's and probably even in a sedan with a folding rear seat (so long as the trunk is deep enough to hold the wheel box).  

## You probably need to buy a spare tire

Wisconsin requires by law that you carry a spare tire for your trailer. Your state may be different, but even if it's not the law, it's probably a good idea. This increases the cost of the trailer significantly (I [picked one up for $40](https://www.harborfreight.com/12-inch-spare-tire-and-rim-44144.html) with a coupon) so include this in your budget and don't forget to pick it up while you're at the store.  

## Make sure you have all the parts before you start

![2017-06-10 08-13-57 0809.jpg](/wp/2017/06/2017-06-10-08-13-57-0809.jpg)This advice is pervasive but it bears repeating. This thing has a lot of parts, and most of them are fairly big.  If you start building it and find out you're missing something, your choices are to spend a lot of time putting it all away (only to dig it out later when you have the missing part) or tie-up a large amount of your driveway/garage/living-room until the replacement part arrives. ![2017-06-10 08-27-25 0813.jpg](/wp/2017/06/2017-06-10-08-27-25-0813.jpg) We followed this advice, found one part missing and called Harbor Freight to ask for a replacement. They told us they would call us back later when they found one. We started building with the impression that we would hear back in a few hours, but after four or five had passed we decided to call _them_ back. The person I spoke to on the second call said they had to talk to corporate and the soonest we'd have a part is the following Monday (this was on a Saturday). ![2017-06-10 08-20-04 0811.jpg](/wp/2017/06/2017-06-10-08-20-04-0811.jpg) Since I needed to get this thing home and my brother probably didn't want me using two stalls of his garage for the next few days, we had to find another solution. Fortunately my brother knows a lot of people who know how to get things done, and after a short trip to Luther's shop we had a better-than-original replacement for the missing part. ![2017-06-10 13-46-35 0847.jpg](/wp/2017/06/2017-06-10-13-46-35-0847.jpg) Had Luther not been available to us, this story would have ended a lot sooner (and would have necessitated a sequel). It's worth mentioning that Harbor Freight never did call back, and I still don't have a replacement for the missing part (but I like Luther's better anyhow). Do yourself a favor and don't start this project until you have all the parts in-hand.  

## Get some help

![2017-06-10 10-25-18 0829.jpg](/wp/2017/06/2017-06-10-10-25-18-0829.jpg?w=3264) You could probably build this yourself with the right assortment of jacks, stands, etc. but it goes a lot faster and smoother with an extra set (or two) of hands. Also, the entire trailer weighs around 275 pounds when assembled and even some of the individual parts are quite heavy. It's not hard to imagine that you could get yourself stuck or hurt putting the thing together alone, and few things are more embarrassing than getting pinned under your own project until someone happens to find you. ![2017-06-10 09-17-38 0817.jpg](/wp/2017/06/2017-06-10-09-17-38-0817.jpg) If you look at other people's builds, the assembly time ranges anywhere from six hours to a couple days,t hat's quite a range. It's not hard to see how it could take days doing it by yourself, especially if you've never done something like this before. A few parts runs, or any amount of re-work and the time adds up. Add to that the fatigue and frustration that comes from doing it all alone and you can see how having a friend or two who can share the load, work in parallel and laugh along with the mistakes can make a big difference.  

## Book some time

As mentioned above the amount of time it takes to build this trailer varies wildly, but unless you've built one before, plan on at least an entire day. In our case it took about 8 hours of work and 12 hours of time (travel, food, parts runs, swearing, etc.). I was also working with two people who have a lot more experience than I do with this type of project, so if you are the expert among your crew, plan accordingly.  

## Plan on adding a deck

Something I didn't plan on doing immediately was adding a deck to the trailer. The kit doesn't include this and I thought I could transport the trailer from my brother's house to my home without it to save some time on build day.  However, my friend Justin (not to be confused with my brother Justin) pointed out that due to the folding nature of the trailer (as well as the bolt-together construction) the deck is really important to the structural integrity of the thing and I probably shouldn't be hauling it around without one. ![2017-06-10 15-46-49 0850.jpg](/wp/2017/06/2017-06-10-15-46-49-0850.jpg) There isn't much to the deck, just a sheet of plywood (4'' x 8' x 1/2" treated in this my case) but it has to be transported, cut, drilled and bolted-on, which adds some time and cost to the project. If you're building the trailer at home this is something that can be put-off to another day, but if you plan on taking the trailer on the road, reserve the time and money to put some sort of deck on it.  

## Re-pack the bearings

The wheel bearings that come with the trailer are alright, but the factory lubrication is piss-poor. Plan on repacking these yourself (and if you don't know what [repacking a wheel bearing](https://www.youtube.com/results?search_query=pack+wheel+bearings+by+hand) is, learn about that before you start building). ![2017-06-10 11-03-52 0835.jpg](/wp/2017/06/2017-06-10-11-03-52-0835.jpg)---
tags:
  - gullicksonlaboratories
  - rain
title: i2c Front Panel
link: http://jjg.2soc.net/2018/03/06/i2c-front-panel/
author: jgullickson
description: 
post_id: 5104
date: 2018-03-06 10:00:09
created_gmt: 2018-03-06 16:00:09
comment_status: open
post_name: i2c-front-panel
status: publish
post_type: post
---

# i2c Front Panel

Switching from individual full-size PINE64 boards to the [Clusterboard](https://jjg.2soc.net/2018/02/23/pine64-cluster-board/) necessitates a change to the electronics for RAIN-PSC's front panel. The original plan was to use GPIO pins on each board to directly interface with the LED's and switches on the panel, but the Clusterboard exposes far fewer pins for each compute module, so another approach is required. ![pi-2-connector](/wp/2018/03/pi-2-connector.png) ![clusterboard_2v2_header](/wp/2018/03/clusterboard_2v2_header.png) On the schematic, one [i2c bus](https://www.i2c-bus.org/) is exposed for each compute module.  This looked promising so I dug into using i2c to interface with a chip that could drive the panel LED's and read input from the panel switches. I settled on the [MPC23017](https://cdn-shop.adafruit.com/datasheets/mcp23017.pdf) port expander. This chip is probably overkill for the application (it has a lot more I/O's than I need) but I was able to find a fair amount of documentation on using it, and I can always switch to something more compact once I know more about exactly what I need. Since I'm waiting for the Clusterboard to arrive, I started working on getting this working with a full-size PINE64.  This took a bit of experimentation and I was hampered by the fact that I couldn't tell for sure if any of the pieces (computer, operating system, electronics, etc.) were correctly setup on their own. Holding more than one "variable" when troubleshooting is a rookie mistake, but since this is the first time I've really worked with i2c I didn't have much in the way of "constants" available in the lab to narrow things down. After tearing things apart and starting-over a few times I was able to identify primary source of the trouble. ![IMG_0029](/wp/2018/03/img_0029.jpg) In order for Linux to properly detect devices attached to the i2c bus, it is necessary to "pull-up" the SDA and SCL lines. These pins on the PINE64 include built-in resistors, but the value of these resistors is too high to work correctly in this case.  That means it's necessary to add resistors externally to tie pins 3 (SDA) and 5 (SCL) to pin 1 (3.3v) on the PI-2 connector. I learned this after sifting through a thread on the PINE64 forum about interfacing the PINE64 with an i2c-based Adafruit LCD module. [caption id="attachment_5108" align="aligncenter" width="2448"]![IMG_0024](/wp/2018/03/img_0024.jpg) Unfortunately, this wasn't the only bug[/caption] However in my case the problem was with the _value_ of these resistors. Based on what I had read the target value should be around 2k. I went with 2.2k because that's what I had on-hand, and figured this was close enough. It was not After eliminating everything else I could think of, I tried a pair of 200(k?) resistors (the same ones that I was using to limit current to the LED's) and viola, it started working! ![IMG_0033](/wp/2018/03/img_0033.jpg) This is far from the first time that I've been bitten by some basic electronics problem, and had I spent more time learning about the parts I was working with (schematics for the PINE64, datasheet for the MPC23017, etc.) I may have gotten it right in the first place. However, if I did this for everything I worked on my projects would take exponentially longer, and I'd get a lot less done. You could argue that doing things this way takes longer (due to spending time chasing avoidable mistakes) but this has the added side-effect of improving my troubleshooting skills, which have a much broader application than memorizing the technical details of specific components. That's not to say one approach is superior to the other. I think this approach comes natural to me and working this way preserves my flow and gumption, but I can see how it would be the opposite for someone else. This might be part of the reason I'm better at software than hardware, even though I think one is as fun as the other. Now that the hardware is in order, I'll dive into writing the software to do something useful with all this next time. 

## References

  * http://files.pine64.org/doc/Pine%20A64%20Schematic/Pine%20A64%20Pin%20Assignment%20160215.pdf
  * http://raspberrypi.link-tech.de/doku.php?id=mcp23017
  * https://forum.pine64.org/showthread.php?tid=2079&page=2
  * https://www.raspberrypi-spy.co.uk/2013/07/how-to-use-a-mcp23017-i2c-port-expander-with-the-raspberry-pi-part-1/
  * https://forum.pine64.org/showthread.php?tid=2948
  * https://gist.github.com/pfeerick/fe8ebd68623d6d775e7b654aa32ec6fa
  * https://forum.pine64.org/attachment.php?aid=1111

## Comments

**[Jason J. Gullickson](#151 "2018-04-20 09:13:17"):** Thanks for the suggestion pfeerick, I will check that chip out! It's cool to hear from another Clusterboard user. My original design used regular PINE A64 boards and more of a custom interconnect, but then the Clusterboard came along. It's not as flexible as my original design, but it saves a _ton_ of time and space and just happened to fit inside the case I had already selected for this phase of the project. I'm looking forward to spending more time on the software/application side of the work once I get the hardware to a point where I can fit it all back inside the box :)

**[pfeerick](#150 "2018-04-19 20:04:31"):** If you want a smaller I2C expander, have a look at the MCP23008... it's the smaller 8 I/O cousin of the MCP23017 and should work just as readily. I've just come across your blog, and will be eagerly following it as you're doing much the same as I want to do with a clusterboard... I have the board and all the nodes... just need to put it to use!

**[Peter Feerick](#149 "2018-04-19 20:03:23"):** If you want a smaller I2C expander, have a look at the MCP23008... it's the smaller 8 I/O cousin of the MCP23017 and should work just as readily. I've just come across your blog, and will be eagerly following it as you're doing much the same as I want to do with a clusterboard... I have the board and all the nodes... just need to put it to use!

---
tags:
  - gullicksonlaboratories
title: Independence Day
link: http://jjg.2soc.net/2017/07/04/independence-day/
author: jgullickson
description: 
post_id: 3201
date: 2017-07-04 08:00:24
created_gmt: 2017-07-04 13:00:24
comment_status: open
post_name: independence-day
status: publish
post_type: post
---

# Independence Day

![40-11-02/54](/wp/2017/07/eugc3a8ne_delacroix_-_le_28_juillet-_la_libertc3a9_guidant_le_peuple.jpg) A few years ago I began the slow process of reducing my dependency on Internet systems and services which I had no control over. This was a response to an increasing amount of "bad behavior" on the part of the companies who owned the services I depended on, and my inability to do anything to alter their course. It may be hard for some of you to imagine, but there was a time when the web was built on a vast array of independently owned and operated servers. It wasn't unusual for an individual person (let alone a business, school etc.) to run a small server to provide their email, website, etc. This took a little work, but unless you ran a very popular website it was a small investment in terms of time (and it made you smarter to boot). _Wow, I was just about to launch into an Internet history lesson, and this isn't the time or place for that._ The key part of that lesson is that we've come a long way from the independent, distributed web of the past. Today most of the content and communication on the Internet is mediated by systems owned by a handful of corporations whose only concern is profit. Needless to say, this isn't in your personal best interest.* I didn't want to live on that planet anymore. https://www.youtube.com/watch?v=HIWHMb3JxmE So I made a list of the Internet services I depended on and started replacing them with systems I had control over. Some were straightforward (if difficult) like email. When I made the jump it wasn't hard to setup an email server, but it was hard to set one up that was secure. Now there are open-source projects like [Mail-in-a-box](https://mailinabox.email/) which make the process almost automatic. Other services have been more difficult to replace. Social networking sites like Twitter and Facebook are particularly difficult. Not due to technical reasons, but due to the [network effect](https://en.wikipedia.org/wiki/Network_effect) which keeps the people you want to interact with entangled in them. I made a number of [attempts at creating my own alternative](https://github.com/jjg/beniceorleave) platform for these interactions without much luck. Eventually I came upon [Mastodon](https://joinmastodon.org/), which builds a global social network out of a federation of independently-owned-and-operated server nodes. I started using Mastodon by creating an account on [a friend's node](https://resistodon.com/about), then migrated to another when I [joined the co-op](https://social.coop/about) which operates a node of their own. Moving from node-to-node is supported by the Mastodon software and it's fairly painless. So if at some point in the future I don't like the way the co-op is running its node, I can always fire up one of my own and relocate without loosing the connections I've made on the global network. Yes, there is a cost to running your own services, but it's lower than you might expect. If you can find a few other pilgrims to share the load, it can be less than the proverbial "cost of a cup of coffee per month". All told it costs me about $13.00 US per month which I could reduce substantially by recruiting a few more "freedom enthusiasts" to utilize my servers. At this point almost everything I use daily comes from either a server I control (running open-source software) or a service where I have both input into how it is operated and the ability to leave without consequence. What remains are a handful of conveniences (YouTube, Google Docs, Github, etc.) that I have plans to replace but haven't quite had the time to change to yet. I'll admit that I've been dragging my feet on these a little. 

## That brings us to the idea of "Independence Day".

In order to motivate myself to cut these final threads, I'm going to commit to complete information independence by July 4th, 2018. On this day I will delete any remaining accounts I have on services and systems of which I have no control over. This will force me to take action to preserve any information these systems currently house and replace any service they currently provide with free and independent alternatives, or consign those resources to the big bit bucket in the sky. By committing to a date, I have something to look forward to when I need to summon the motivation to do the dirty work necessary to achieve this goal. When I started on this process it seemed it would be almost impossible, or that I would have to sacrifice a lot to be free from the tyranny of these corporations (and in some cases, governments). But by making a list and taking it one-step-at-a-time, I was able to turn a large climb into a series of short hops, and I've been able to change course along the way when the path forward wasn't visible from the base of the mountain. I suppose that's how all large problems are tackled, but it always amazes me how simple it seems when you reach a milestone such as this and look back at how much ground was covered to get there. If you've considered a similar journey I'd be happy to tell you more about the paths that I've walked in detail. In the meantime here's some links to a few of the open-source projects I've used to get this far: 

  * Mail-in-a-box:<https://mailinabox.email/>
  * Wordpress: <https://wordpress.org/>
  * Nextcloud: <https://nextcloud.com/>
  * Mastodon: <https://joinmastodon.org/>
  * Gitlab: <https://gitlab.com>
  * Pinebook: <https://www.pine64.org/?page_id=3707>---
tags:
  - gullicksonlaboratories
  - rain
title: It's time for me to Ki(Cad)
link: http://jjg.2soc.net/2018/04/09/its-time-for-me-to-kicad/
author: jgullickson
description: 
post_id: 5142
date: 2018-04-09 11:36:04
created_gmt: 2018-04-09 16:36:04
comment_status: open
post_name: its-time-for-me-to-kicad
status: publish
post_type: post
---

# It's time for me to Ki(Cad)

I've been putting-off learning to use an [EDA](https://en.wikipedia.org/wiki/Electronic_design_automation) for quite awhile. You see, I am old, and I learned how to read and draw schematics when I was young, so I learned how to do it on paper, and it's very hard to give-up doing something you can do fast and well for something that is slow, frustrating and you suck at. In the previous RAIN PSC design, I could get away with using the plentiful GPIO pins available through the [PINE A64](https://www.pine64.org/?product=pine-a64-lts)'s 40-pin connector to drive the front-panel LED's and switches.  However, adopting the [Clusterboard](https://jjg.2soc.net/2018/02/23/pine64-cluster-board/) demanded a different approach and I settled on developing an [i2c-based interface](https://jjg.2soc.net/2018/03/06/i2c-front-panel/). This has [worked well on the breadboard](https://jjg.2soc.net/2018/03/14/front-panel-software/), but it requires building an interface board for each node and fabricating something like that out of perfboard (or some other "homebrew" technique) repeatedly is not very practical.   [caption id="attachment_5146" align="alignnone" width="3260"]![2018-04-07-13-08-22.jpg](/wp/2018/04/2018-04-07-13-08-22-e1523291285957.jpg) 8 of these are probably not going to fit...[/caption] Regardless, I thought I could at least cobble-together a prototype of the driver board for a single node so I could at least pack everything inside the case for awhile. A couple of weeks have passed as I've tried various options and I've come to accept that designing a custom board is the only way to go, and the best way to go about that is getting my design into an EDA. I've put this off for a long time because I've made several attempts at learning several EDA's and it's always been frustrating. There's a lot of up-side to learning to use these tools, but paper and pencil is so fast and so natural for me that it's really had to give that up. Nonetheless, if I'm going to get boards made from my design the options are to either learn this or have someone else do it, and doing yourself makes you smarter so that's the way I decided to go. [caption id="attachment_5143" align="aligncenter" width="2448"]![2018-04-07 18.51.07](/wp/2018/04/2018-04-07-18-51-07.jpg) First attempt (schematic is OK, layout on the other hand...)[/caption] I could have learned a number of different packages but I choose [KiCad](http://kicad-pcb.org/) because it's open-source and one of the goals for RAIN is to be a completely open-source computer. There's a number of other open-source EDA's as well, but I got a lot of recommendations to use KiCad and even though it's considered more difficult to learn, I've been told it's worth it. I also knew that KiCad ran fairly well on my A64-based laptop, and this would allow me to design RAIN's hardware on the PSC itself. I started with [Getting Started in KiCad](http://docs.kicad-pcb.org/stable/en/getting_started_in_kicad.html) (seems like a logical place to start, right?) and slowly made my way through the tutorial, stopping whenever I became the least bit tired or frustrated. I've found this to be a good way to learn something I'm not looking forward to learning, because these forced stops cultivate some excitement and curiosity about returning to the task. [caption id="attachment_5144" align="aligncenter" width="2448"]![2018-04-08 10.16.46](/wp/2018/04/2018-04-08-10-16-46.jpg) Second attempt, much closer[/caption] I was able to maintain this discipline over the course of a weekend and while I wasn't able to finish the design, I made a lot of progress and learned a lot more than I expected about the tool. Based on what I've learned, I feel pretty confident that I will be able to design this board successfully, and continue to use an EDA for all of my future electronics projects. [caption id="attachment_5145" align="aligncenter" width="3264"]![2018-04-08 10.10.57](/wp/2018/04/2018-04-08-10-10-57.jpg) Test-fit confirms the form-factor, and also illuminates some design problems[/caption] There's still  work to do before I finalize the design and send it out to have a prototype board made, but what remains is squarely within my comfort zone. I need to determine whether or not the i2c pins on the Clusterboard need to be pulled-up to 3.3v like they do on the A64 (which would be a drag because the Clusterboard's pins don't supply 3.3v) and I need to sort-out some software problems on the SOPINE module just to confirm that the driver circuit will work the same as it did when I had it connected to the A64 for testing earlier. Once these two things are sorted I can finalize the design of the board and order a copy. With any luck it will work and I'll be able to pack everything back in the case and focus on the software side of things until I scrape-up enough cash to order more panel drivers and populate the rest of the slots on the Clusterboard. _If you like this project you can support my work on it by "like"ing it on [Hackaday.io](https://hackaday.io/project/85392-rain-mark-ii-personal-supercomputer) and helping me make it into the semifinals of the [2018 Hackaday Prize](https://hackaday.io/prize)._
---
tags:
  - gullicksonlaboratories
  - rain
title: Laser Time!
link: http://jjg.2soc.net/2018/02/14/laser-time/
author: jgullickson
description: 
post_id: 5066
date: 2018-02-14 10:07:23
created_gmt: 2018-02-14 16:07:23
comment_status: open
post_name: laser-time
status: publish
post_type: post
---

# Laser Time!

Last weekend I spent most of a Saturday at the [Idea Studio](http://www.fdlpl.org/ideastudio) working on Mark II's front panel. ![IMG_2085](/wp/2018/02/img_2085.jpg) A few weeks earlier I earned my [laser cutting badge](http://www.fdlpl.org/pdfs/IdeaStudioEQUIPMENT_6.1.2017.pdf), so I knew the basics of operating the laser but I've never actually used one on a project before. So the first step was learning the workflow and doing some experiments to dial-in the settings for the material I'm working with. ![IMG_2083.JPG](/wp/2018/02/img_2083.jpg) I picked-up some [3mm acrylic sheet](https://www.amazon.com/gp/product/B01BWFHQFI/ref=oh_aui_detailpage_o05_s00?ie=UTF8&psc=1) (transparent and black) to cut the panel from. Based on my research, this seemed thick enough to do the job but thin enough that the 40 watt laser should be able to cut it. Turns out the laser has no problem cutting this. I probably could have gone even thicker but even this thickness caused other problems I'll describe below. ![img_2087.jpg](/wp/2018/02/img_2087-e1518623219472.jpg) Based on these tests I settled on the following settings: 

  * **Cutting**: vector cut mode, power 100%, speed 25%, 10 passes
  * **Engraving**: raster engrave mode, power 100%, speed 100%, 1 pass
Once I had the settings for cutting and engraving figured out, I set the laser loose on a front panel design I made awhile back. While it was busy cutting that, I re-worked the panel based on the results of some fitting tests (I'll leave the details of that to future post). This ability to literally multitask (doing two things simultaneously) is one of my favorite things about working with robots. ![IMG_2088](/wp/2018/02/img_2088.jpg) The test panel came out pretty good. I learned that my drawing had some errors in it which resulted in doubling the number of passes when cutting-out the LED holes. I also noticed that the laser was liquefying the "plugs" left after cutting the holes. On top of that, because I left the film on the back of the sheet, these liquid plugs were being suspended (instead of falling out the bottom) and getting stuck to the inside of the holes. Not great. I fixed these errors in the updated drawing and set the laser loose on the uncut portion of the acrylic sheet. This time the results were better, but even with the drawing corrected, 10 passes was still overkill (especially with the protective film removed). I also noticed that the plastic around the hole was getting soft, either due to excess cutting passes or maybe because I still had the protective sheet on the back and it was conducting heat. Either way, I'll remove all the film next time. Just for fun I populated the panel to see how it looks (these parts will actually be mounted to the 3d printed compute modules). I'm pretty happy with how it's turning out, even Jamie thought it looked good. ![IMG_2110](/wp/2018/02/img_2110.jpg) However, this revealed that the thickness of the panel creates a few problems. First is that the mounting screws that came with the case are too short. This is easily addressed with longer screws, but the bigger problem is that the combined thickness of the compute module faces and panel is too thick for the controls to work properly. For this reason (and a number of others) I'm going to have to make some more adjustments to the printed parts. Once I've got the printed parts dialed-in I'll plan another trip to the Idea Studio to cut an updated version of the panel. Minimally there will be changes to accommodate mounting hardware for the modules and I'd also like to try some new aesthetic techniques for engraving the legends. I also need to design and cut a back panel... I had a great time working with the laser.  This is definitely a tool I'd eventually like to add to my lab, but for now I'm very fortunate to have access to one thanks to the Idea Studio.  If you are in the region I highly recommend you take advantage of it.

## Comments

**[Shane](#140 "2018-03-23 13:46:48"):** Hi Jason, No problem- everybody starts of green! I am fortunate to have had two formal classes in LASER theory as part of my Electrical Engineering degree way back when, and had access to the High Power LASER laboratory at New Mexico State University, through friend who were engaged in post graduate study and research. That experience gave me some rather unique insight into seriously high power lasers. I, as you, have attended a local maker space where I was fortunate to use a large LASER cutter to make some precise pieces from wood for a rocket project, and watched some local experts tuning the laser to make some plexiglass parts for others. The comments I provided to you were from the experts helping with those plexiglass parts. I’m enjoying following your personal cluster computer and look for to reading your future blog entries!

**[Jason J. Gullickson](#138 "2018-03-22 14:13:11"):** This is very helpful, thanks Shane! I’m super green when it comes to laser cutting so any feedback on how to improve the process is appreciated :)

**[Shane](#136 "2018-03-21 16:51:15"):** The LASER cutter is running at a much higher power than is needed to cut plexiglass. This is why you are experiencing melting around the holes, and the oozing of the cut-out material. Try a power level of 10-20% and only a single pass and see if you are getting clean cuts. The whole idea of the laser is to put in just enough power to VAPORIZE the plastic, you wish to be removed, but not so much that the resulting workpiece absorbs excess heat that melts the finished product. This is a balance of power and speed of cut. I recommend you experiment on a waste piece of plexiglass to ascertain how much power is needed to penetrate the plexiglass all the way through, and how long it takes to do so (called dwell time). Some laser cutters have control software that has a database of common materials and what parameters to use to get a good cut. Try to use the lowest power necessary to cut the piece through cleanly at the highest speed to keep the surrounding workpiece from absorbing heat and starting to melt/distort. Remember that materials can reflect, refract and disperse that laser light. A clean cut without melting minimizes reflection, refraction, and diffusing that laser energy, which means the beam does not stray far from the intended path specified, making for a cleaner cut and better looking result. Another aspect to keep in mind is that the laser beam is focused at the platen upon which the workpiece is resting, and there is a certain depth of focus, which should be taken into account if you cut thicker pieces of plexiglass/lucite. Best of luck with your project!

---
tags:
  - gullicksonlaboratories
  - rain
title: LINPACK is Back
link: http://jjg.2soc.net/2017/10/07/linpack-is-back/
author: jgullickson
description: 
post_id: 4816
date: 2017-10-07 10:00:19
created_gmt: 2017-10-07 15:00:19
comment_status: open
post_name: linpack-is-back
status: publish
post_type: post
---

# LINPACK is Back

It's been awhile since I've done anything with Raiden. This is partly because I've been swamped with other projects and partly because it was summer and running the cluster made things uncomfortably (dangerously?) warm in the lab. Now that [Makerfaire](http://jjg.2soc.net/2017/10/06/milwaukee-makerfaire-2017/) is past and winter is coming, I took some time to resume my pursuit of establishing the baseline performance of Raiden Mark I, which means round two of fighting with LINPACK. ![winter-is-coming-boot-the-supercompuer](/wp/2017/10/winter-is-coming-boot-the-supercompuer.jpg) For the uninitiated, LINPACK (specifically the [High-Performance Linpack Benchmark](http://www.netlib.org/benchmark/hpl/)) is a standard method of measuring supercomputer performance. It's what's used by [top500.org](https://www.top500.org/project/linpack/) to rank the world's fastest computers and while it's not an absolute measure of performance for all applications, it's the go-to compare the performance of supercomputers. Since the goal of the [Raiden](http://jjg.2soc.net/2017/08/08/raiden/) project is to determine if traditional Intel server-based supercomputer clusters can be replaced by less resource-intensive ARM systems, the first step is to establish a baseline of performance to use for comparisons. Since HPL is the standard for measuring supercomputers, it makes sense to use it here. The problem is that HPL isn't the easiest thing to get running. This is partially due to the fact that most supercomputers are specialized, custom machines. It's also a pretty specialized piece of software with a small userbase which means there's just not a lot of people out there sharing their experiences with it. When I first built-out [Raiden Mark I](http://jjg.2soc.net/2017/08/29/raiden-mark-i/) I kind of assumed that HPL would be part of the Rocks cluster distribution since benchmarking is a pretty common task when building-out a supercomputer. If it was included, I wasn't able to find it, and after spending a few hours trying to get HPL and its various dependencies to compile, all I had to show for it was a highly-parallel segfault. I'm not sure what's changed since then, but with fresh eyes I put on the white belt and tried building HPL from scratch. After reading the included documentation and looking over my own (working) MPI programs I was able to ask the right questions and found a tutorial that lead me to successfully compiling the software. Not only that, but I was able to do a test run on a single node without errors! [code lang="text"] ================================================================================ T/V N NB P Q Time Gflops \-------------------------------------------------------------------------------- WR11C2R4 29184 192 2 2 692.41 2.393e+01 HPL_pdgesv() start time Fri Oct 6 12:58:50 2017 HPL_pdgesv() end time Fri Oct 6 13:10:22 2017 [/code] (The interesting part of the output is the Gflops metric, which in this case is 2.393e+01) I quickly spun-up the compute nodes of the cluster and modified the `machines` file to run the benchmark across four nodes. However, for some reason only two nodes joined the cluster so I decided to run with only two and troubleshoot the missing nodes another time. [code lang="text"] ================================================================================ T/V N NB P Q Time Gflops \-------------------------------------------------------------------------------- WR11C2R4 29184 192 2 2 589.34 2.812e+01 HPL_pdgesv() start time Wed Jan 23 18:08:43 2008 HPL_pdgesv() end time Wed Jan 23 18:18:32 2008 [/code] The results were a bit disappointing (only about .5 Gflops faster).  I would have expected something closer to twice the performance by adding two more nodes to the cluster (as well as off-loading the benchmark from the head node). Based on these results I decided to take a look at tuning the HPL.dat file and see if I could optimize the parameters for a two-node cluster vs. of a single computer. [code lang="text"] ================================================================================ T/V N NB P Q Time Gflops \-------------------------------------------------------------------------------- WR11C2R4 41088 192 2 4 1333.90 3.467e+01 HPL_pdgesv() start time Wed Jan 23 18:25:19 2008 HPL_pdgesv() end time Wed Jan 23 18:47:33 2008 [/code] This made a significant difference. Not surprisingly, the benchmark responds strongly to being tuned for the hardware configuration it's running on. I knew this mattered, but I didn't realize how dramatic the difference would be. I'm very excited to have reached this point in the project. There's a number of reasons I'm anxious to move on to the Mark II version of the hardware and establishing a performance baseline for the Intel-based Mark I is a requirement for moving on to the next stage. There is still work to do, I need to get the other two nodes on-line and I need to spend more time learning how to optimize the settings in HPL.dat, but these are much less mysterious problems than getting the benchmark to compile & run on the cluster.
---
tags:
  - gullicksonlaboratories
  - rain
title: Mark IIb and breaking the 20 Gflops barrier
link: http://jjg.2soc.net/2018/08/13/mark-iib-and-breaking-the-20-gflops-barrier/
author: jgullickson
description: 
post_id: 5279
date: 2018-08-13 12:43:37
created_gmt: 2018-08-13 17:43:37
comment_status: open
post_name: mark-iib-and-breaking-the-20-gflops-barrier
status: publish
post_type: post
---

# Mark IIb and breaking the 20 Gflops barrier

Now that Mark II has been operational more regularly, I've noticed that working on it in the current case is really inconvenient. Something with a removable top would make a lot more sense (which is why machines of a similar design had this feature), but when I started designing Mark II I couldn't find anything like that which would work with the original [full-size PINE A64-based design](https://jjg.2soc.net/2017/11/25/raiden-mark-ii-chassis/). After switching to the [Clusterboard](https://jjg.2soc.net/2018/02/23/pine64-cluster-board/), I took another look and found a case with a removable top that might work. I over-analyzed the specs and finally broke-down and just ordered one to measure the fit directly. ![2018-08-13_12-27-08](/wp/2018/08/2018-08-13_12-27-08.jpg) The new case is considerably smaller in the "horizontal" direction, larger in the "vertical" one (I put these in quotes because the way length/width/height are measured makes this difficult). The Clusterboard and one [A64](https://www.pine64.org/?page_id=46823) do fit inside this new case, but it's a pretty tight fit. ![2018-08-08_08-41-07](/wp/2018/08/2018-08-08_08-41-07.jpg) I took this downtime opportunity to try a few other modifications I'd been putting-off while the machine was working. The first was to try a different Linux build to test my theories about performance limitations found previously. I've made a few attempts at "swapping out" the kernel of the Armbian images I've been using on the cluster nodes but the only result was "bricking" a couple of them. After that I decided it was worth trying the ["stock" Ubuntu build that PINE64 provides](http://files.pine64.org/os/SOPINE/ubuntu/xubuntu-xenial-mate-20170306-longsleep-sopine-8GB.img.gz) for the [SOPINE module](https://www.pine64.org/?page_id=1491) just to see what happens. I thought I read that this would be a non-starter due to issues with the network interface and the Clusterboard, but figured it was worth a shot. As it turned out, the image worked fine. I was able to boot-up a single node using the new image, configure it for the cluster and run HPL on it with no obvious problems. I _was_ however able to see that thermal management was working (unlike the previous build) and it was throttling-down the CPU to keep overheating at bay. This gave me a reason to try some active thermal management. I'd picked-up some fat little copper heatsinks for the SOPINE modules awhile back, but since the previous Linux build didn't produce any temperature feedback it wasn't clear that they would make any difference. Now that I can _see_ the CPU getting throttled due to overheating it makes sense to give these a try. ![2018-08-08_19-13-12](/wp/2018/08/2018-08-08_19-13-12.jpg) Using the heatsinks alone didn't reduce the incidence of throttling much, so I coupled that with a (very janky) fan setup and tested again. This made a **big** difference, and in fact completely eliminated the CPU throttling messages from the logs. ![2018-08-09_19-21-55](/wp/2018/08/2018-08-09_19-21-55.jpg) This looked pretty good so I pull the rest of the cluster nodes, re-imaged their SD cards and installed heatsinks. The only problem I ran into brining these on-line was the [hard-coded MAC address](https://gitlab.com/jgullickson/rain/blob/master/mark-ii/build_log.md#anchor-08092018) in the O/S image which resulted in only one of the nodes being accessible via the network. Once I identified the problem it was a straightforward fix to make each node re-generate it's MAC address and they all came-up correctly. Now that I had a recipe for setting-up the cluster nodes to run HPL, doing so went pretty smooth and I was able to run a full-power test in short order. The results speak for themselves; **22.12 Gflops**. This is still a ways from my target of 50 Gflops, but overcoming the previous ceiling is encouraging, and it's especially encouraging because I was able to correctly identify the cause of the limitation. Next steps will be to spend some time tuning the contents of this new O/S image as well as tuning the overall configuration now that the cluster nodes can run at full clock speed (or perhaps beyond...?). I'm also very happy with how the new case is working out, and I'm planning on re-designing the front panel to fit the new case. This design will be called "Mark IIb", and I have a number of changes in mind beyond simply resizing the panel, but more on that later...

## Comments

**[TheSec](#207 "2018-08-15 08:32:31"):** Isn't the 22 Gflops the Rpeak according to your own post ? or was the 22 Gflops Rpeak based on 408Mhz.

**[Jason J. Gullickson](#208 "2018-08-15 08:56:32"):** That's correct, the 22 Gflops Rpeak was calculated based on the 408Mhz clock speed limit set by the previous build. Now that the system can reliably run at 1.15Ghz, the Rmax should be 64 Gflops. If I can achieve about 80% efficiency (which I think is reasonable) the Rpeak should be about 50 Gflops so this is my target for this configuration.

---
tags:
  - gullicksonlaboratories
title: Microprocessors
link: http://jjg.2soc.net/2017/05/11/microprocessors/
author: jgullickson
description: 
post_id: 2393
date: 2017-05-11 07:06:55
created_gmt: 2017-05-11 12:06:55
comment_status: open
post_name: microprocessors
status: publish
post_type: post
---

# Microprocessors

Last weekend I started reading a book I found in the lab titled "Microprocessors" by Rodnay Zaks. I picked it up sort of randomly to have something to flip-through during reading time. ![2017-05-01 05-47-39 0426](/wp/2017/05/2017-05-01-05-47-39-0426.jpg) However, after reading the introduction I was excited by the premise of the book: 

## "Microprocessors, and other LSI chips, have made system design so simple that no significant scientific or electronic training is required."

I've studied these topics for most of my life, but this time I'm comprehending the microcomputer at a much more elementary level. I was familiar with the abstract idea of things like the accumulator, program counter, stack pointer, etc. but reading this book, at this point in my life, is giving me a "signals-level" understanding of how these things actually work. The closest I had come before was writing assembly language programs out of necessity for 8-bit Commodore and Apple computers. This gave me the experience of working with sub-CPU constructs like opcodes and registers, but I didn't really have to _understand_ them to get work done. I was still strictly in the world of _software_ (even if I sometimes needed a schematic to understand exactly what my software was doing). What I'm getting from this book is something entirely different. The first few chapters dive into the low-level operations inside the CPU, and how these internal components directly relate to the signals which appear on the CPU's pins. It's important to understand that this book was written in a time when microprocessors (and personal computers in general) were an embryonic technology. This perspective conveys the information as not doctrine, but as something that is evolving. It's also coming from a place where the CPU has only just proven itself as a practical product, so the book doesn't _start_ at the CPU, the CPU is the _destination_, the current "state of the art", and the book is not ashamed to discuss the CPU's humble origins (and related design failures) as essentially a desktop calculator part. I'll pause for a moment to note that even way back then (1979), Intel processors were making design compromises to preserve backward compatibility, _with a calculator chip_. Equipped with this more intimate understanding of the microcomputer, reading this book has reinvigorated my desire to build one of my own design. Today there is a lot of discussion of building computers, but few of them take an approach that is literally from the "chips up", and those that do usually involve a lot of "system on a chip" business. I'm not criticizing this per se (in fact, it makes a lot of sense), but to me there is something very romantic about designing a and building a computer from something as primitive as these early processors, or perhaps even designing the processor itself. Until about half-way through the book I was getting really excited about the idea of designing & building my own computer using what I've learned so far. I might still do this, but after reading the rest of the book and learning about how, from their very origin microprocessors have a legacy of compromise, I've lost a lot of enthusiasm for working with them. The whole experience has reinvigorated my interest in high-performance computer architectures, and drove me to take another look at my [Parallella](http://www.parallella.org) board. I'll be writing about the results of that adventure in the near future.---
tags:
  - gullicksonlaboratories
title: Milwaukee Makerfaire 2017
link: http://jjg.2soc.net/2017/10/06/milwaukee-makerfaire-2017/
author: jgullickson
description: 
post_id: 4739
date: 2017-10-06 09:10:46
created_gmt: 2017-10-06 14:10:46
comment_status: open
post_name: milwaukee-makerfaire-2017
status: publish
post_type: post
---

# Milwaukee Makerfaire 2017

A couple weeks ago DONOR-1 took a roadtrip to [Milwaukee Makerfaire](https://milwaukee.makerfaire.com/)! ![img_6320-e1507298900939.png](/wp/2017/10/img_6320-e1507299020451.png) The excitement began with a very nice write-up of the project (as well as some other things we've worked on) in a [Maker Spotlight post](https://makezine.com/2017/09/15/maker-spotlight-jason-jamie-liberty-kratz-gullickson/) on the [Make Magazine](https://makezine.com/) blog. Friday night before the faire we headed to [The Idea Studio](http://www.fdlpl.org/ideastudio) to pick-up the machine. The Idea Studio was a great place to host DONOR-1 and the crew there was sad to see it go. ![IMG_1525](/wp/2017/10/img_1525.jpg) After figuring out how to fit three people, the control panel, a dolly and assorted other bits into our little Jeep, we strapped DONOR-1 to [the trailer](http://jjg.2soc.net/2017/06/12/harbor-freight-folding-utility-trailer-build/) and headed home. Saturday morning we hit the road early to make it to the fairgrounds before 8:00AM. This was my first experience driving around Milwaukee with a trailer and it wasn't as bad as I expected. That said we still managed to get a little lost (Milwaukee likes to move their roads around) so we were a little bit late to the faire. ![IMG_1529](/wp/2017/10/img_1529.jpg) DONOR-1 was a stand-alone exhibit which means we got a chance to check-out the rest of the faire this year. That being the case we didn't get to meet everyone who checked it out, but we grabbed a few snapshots and received some photos from friends who played it after the faire. [gallery ids="4775,4776,4777" type="rectangular"] Sunday afternoon I headed back to the faire to pick-up DONOR-1 and was pleased to see it was not only operational, but there was even a line of people waiting to play! I waited about 45 minutes for a break in traffic to shut it down, but eventually I just had to get in line and wait my turn to switch it off and roll it out to the trailer. Overall it looks like DONOR-1 got a work-out and thankfully survived the weekend. I would love to know how many games were played (this is a feature we wanted to add but it hasn't been home long enough to get it implemented). ![IMG_1552](/wp/2017/10/img_1552.jpg) There were [so many other cool things](https://milwaukee.makerfaire.com/makers-exhibits/) at the faire that it would take several posts to enumerate our favorites. We're already looking forward to seeing what shows up next year. Now that DONOR-1 is back home we're planning to do a little maintenance and perhaps add a few features we didn't have time to add before it hit the road. That said we're looking to line-up DONOR-1's next fundraiser so if you'd like to host the machine to raise funds for a charity [let us know](http://jjg.2soc.net/contact/). One final note: DONOR-1 is entered in the 2017 [Hackaday Prize](https://hackaday.io/prize)! If you like the project please visit the [DONOR-1 Hackaday.io page](https://hackaday.io/project/27172-donor-1) and "like" (the skull & crossbones button below the photos) the project, thanks!---
tags:
  - gullicksonlaboratories
title: More Field Repair & a Makerfaire
link: http://jjg.2soc.net/2017/07/10/more-field-repair-a-makerfaire/
author: jgullickson
description: 
post_id: 3643
date: 2017-07-10 09:28:32
created_gmt: 2017-07-10 14:28:32
comment_status: open
post_name: more-field-repair-a-makerfaire
status: publish
post_type: post
---

# More Field Repair & a Makerfaire

Last week DONOR-1 required another service call to [The Idea Studio](http://www.fdlpl.org/ideastudio). This time it was squealing like a pig and with some help from Josh we were able to diagnose this as a problem with the coin acceptor. ![2017-07-05 13-50-08 1010](/wp/2017/07/2017-07-05-13-50-08-1010.jpg) We waited about a week for the replacement part and headed to Fond du Lac to verify the diagnosis. It took about an hour of testing to rule out other possibilities but ultimately we replaced the entire unit and from then on the squealing subsided. We're not sure exactly what went wrong with the original part, but it's back in the lab now and scheduled for dissection. If you'd like to learn more about DONOR-1, where the idea came from and [how we made it](http://www.fdlpl.org/blog/2017/06/couple-explains-how-we-made-it-jul-19), we'll be discussing just that at the [Idea Studio](http://www.fdlpl.org/ideastudio) on [July 19th starting at 5:00PM](http://www.fdlpl.org/blog/2017/06/couple-explains-how-we-made-it-jul-19). There will be a brief presentation (including a look inside the machine) followed by a Q&A session to answer any questions you may have about the idea, the machine or anything else you can think of. With any luck DONOR-1 will be up-and-running at the [Idea Studio](http://www.fdlpl.org/ideastudio) until the time comes for it's next mission. Speaking of which, DONOR-1's next stop will be the [2017 Milwaukee Makerfaire](https://milwaukee.makerfaire.com/maker/entry/55/)! DONOR-1 was accepted as a stand-alone exhibit which means you'll be able to check it out anytime during the duration of the faire. We won't be attending the machine all weekend, but we're planning to make-up some signage to describe the project.---
tags:
  - gullicksonlaboratories
  - rain
title: Names
link: http://jjg.2soc.net/2018/04/28/names/
author: jgullickson
description: 
post_id: 5170
date: 2018-04-28 09:59:21
created_gmt: 2018-04-28 14:59:21
comment_status: open
post_name: names
status: publish
post_type: post
---

# Names

Based on feedback, conversations and thoughts about the short and long-term goals of the RAIN project I've decided to make some changes to the naming convention of the series. I'm renaming RAIN Mark II from Personal Supercomputer to Supercomputer Trainer. ![heathkitcomputer](/wp/2018/04/heathkitcomputer.png) The term "[trainer](https://en.wikipedia.org/wiki/List_of_home_computers#List_of_hobby,_kit,_or_trainer_computers)" refers to education-oriented machines and systems (typical of the 70's and 80's) which I think suits both the form and function of the current iteration of the Mark II machine well. While the overall goal of the RAIN project is to produce an open-source supercomputer, there is a lot of dispute as to exactly what defines a supercomputer and using the term in an unqualified way to refer to Mark II machines appears to be [controversial](https://hackaday.com/2018/03/21/everyone-needs-a-personal-supercomputer/#comments). So, instead of wasting time arguing semantics I think renaming the machine puts an end to that discussion while at the same time making the objectives of the machine more clear. This also helps me focus on the objective of this stage of the project and might help the machine connect with the most appropriate audience as well. In addition to renaming the current project, I'm also going to begin using the term "Type" to refer to a specific incarnation of each machine in the series. For example, the Supercomputer Trainer will now be referred to as "Type 1".  I if another machine is designed as part of the Mark II series it will e refereed to as "Type 2". This makes it more clear where each machine belongs in the overall evolution of the RAIN project (now that I'm producing custom hardware and electronics I need a simpler way to relate parts to the machines they belong to). I've begun this renaming process which has resulted in some changes in the source repository. Updates to specific components (the front panel, for example) will be applied as new versions of the component designs are produced.
---
tags:
  - gullicksonlaboratories
title: Open Digital 8 - Alignment
link: http://jjg.2soc.net/2017/07/16/open-digital-8-alignment/
author: jgullickson
description: 
post_id: 3820
date: 2017-07-16 10:26:34
created_gmt: 2017-07-16 15:26:34
comment_status: open
post_name: open-digital-8-alignment
status: publish
post_type: post
---

# Open Digital 8 - Alignment

While waiting on parts, I did a number of experiments with the original Raspberry Pi camera module I have to see if I could capture some pictures through the Super8 camera's lens.  I started by simply trying to point the camera module at the aperture and ran into a few problems. ![2017-07-13 05-34-52 1085](/wp/2017/07/2017-07-13-05-34-52-1085.jpg) The first was that the aperture is not centered, and size of the camera module makes impossible to align the module's lens with the Super8 camera's aperture. I could kind of work around this by tilting the module which will be good enough for these experiments but long-term it's not a solution. ![2017-07-07 09-45-50 1020](/wp/2017/07/2017-07-07-09-45-50-1020.jpg) One work-around I came up with for this was to use a mirror to reflect the Super8's image so the sensor could be mounted at a right-angle. I think this will solve the alignment issue but it raises some potential packaging problems. ![2017-07-15 15-14-59 1117](/wp/2017/07/2017-07-15-15-14-59-1117.jpg) The second is that the camera module's [focal length](https://en.wikipedia.org/wiki/Focal_length) is too long to get the camera to focus on the image in the aperture. I was able to work around this somewhat by adjusting the focus on the camera module, but even at maximum adjustment the module can't be placed close enough to the aperture to fill the sensor with the image from the Super8's lens. I was able to get some footage through the Super8's lens by placing the camera module as close to the aperture as I could while keeping it in focus. ![2017-07-15-19-04-17-1121.jpg](/wp/2017/07/2017-07-15-19-04-17-1121-e1500217537505.jpg)This makes me wonder if it might be possible to use this alignment and simply discard the portion of the frame that doesn't contain the desired moving image? The sensor of the camera module has a lot more pixels than I plan to use in my final output, so this might be a simple solution. Based on this, I designed a few more iterations of the cartridge to test mounting the camera module the correct distance from the aperture. ![2017-07-15 22-58-09 1127](/wp/2017/07/2017-07-15-22-58-09-1127.jpg) The results were OK. I used [mjpeg-streamer](https://github.com/jacksonliam/mjpg-streamer) to get a live feed from the sensor so I could align it, and the picture didn't look too bad. However when I used [raspivid](https://www.raspberrypi.org/documentation/raspbian/applications/camera.md) to capture some video and reviewed it in [VLC](http://www.videolan.org/vlc/), it looked worse. ![2017-07-15 22-58-27 1129](/wp/2017/07/2017-07-15-22-58-27-1129.jpg) I'm not sure what contributed to this. It may have just been more visible on the higher-resolution output (the stream is only 640x480 compared to raspivid's 1080p outpt) or it could be differences in the way the two programs initialize/configure the module, but in any case it's something I need to learn more about. I was hoping this test would tell me whether or not this "cropping" technique would be a viable solution to the limited focus range of the camera module, but the results were inconclusive. If I can extend the focus range of the sensor, I might be able to get it close enough to the aperture to fill the sensor's frame with an in-focus image, but I won't be able to align the lenses if the sensor is that close, so I'll need to switch to something like the mirror technique mentioned earlier. If that route is taken, there may be packaging problems, and it's not clear to me if there is enough space for the camera module + lens extension + mirror combination... Of course, many of these problems could be solved using custom-designed parts, and in the long-run I plan on designing just such parts, but for now I want to stick to off-the-shelf components because it gives me more flexibility during the prototyping stage, and it also makes the project more accessible to others. I think the take-away from these experiments is that I seriously underestimated the optical component of this project. This is kind of a bummer, because I'm really looking forward to tweaking on the software and playing with the output video. The up-side is that it gives me an excuse to learn more about optics which is something I've always wanted to learn more about anyway.---
tags:
  - gullicksonlaboratories
title: Open Digital 8 - First Moving Pictures
link: http://jjg.2soc.net/2017/07/22/open-digital-8-first-moving-pictures/
author: jgullickson
description: 
post_id: 4075
date: 2017-07-22 10:00:06
created_gmt: 2017-07-22 15:00:06
comment_status: open
post_name: open-digital-8-first-moving-pictures
status: publish
post_type: post
---

# Open Digital 8 - First Moving Pictures

I spend some time experimenting with the software side of this project and made some progress capturing the portion of the camera module's image that I'm actually interested in. https://archive.org/download/jgullickson_od8_Bar/bar.mp4 As discussed [last time](http://jjg.2soc.net/2017/07/16/open-digital-8-alignment/), one approach to dealing with the fact that I can't get the Raspberry Pi camera module close enough to the aperture (should I be calling it the film gate?) is to take a larger picture and throw-away the non-moving parts. ![test2](/wp/2017/07/test2.jpg) There's a few ways to go about this. One way is to crop the image in the output file. Another way is to crop the input image by only using the portion of the sensor aimed at the interesting part of the frame. I have a theory that the latter approach is superior because it could reduce the amount of data that needs to be compressed for a given output resolution, therefore reducing compression artifacts, etc. After a little experimentation I was able to feed the right command-line arguments to raspivid to capture only the part of the frame which contained the moving images from the Super8 camera's lens.     [gallery ids="4115,4116,4117,4118,4119,4120,4121"] I'm not sure if this is producing higher-quality output than simply cropping the image in the output file. Looking at the two side-by-side on-screen, I can't see a significant difference. This could be attributed to many factors since I don't know yet enough about exactly what happens to the image between the time it hits the sensor and when raspivid writes the file to disk. Clearly I'm going to need to dig deeper into this pipeline before I can expect to get quality results. However, I think this demonstrates that the cropping technique is "good enough" to run with for a bit and I can turn my focus toward packaging. Right now I'm still using a full-sized Raspberry Pi and the original camera module, which means the I can't close the film door on the Super8 camera and the whole thing has to lie on it's side on the bench (hence the orientation of these photos & video). ![test10](/wp/2017/07/test10-e1500668442459.jpg) Now that I have an approach that seems to work (even if the quality isn't great), I think the time has come to work on cramming everything into the cartridge so I can close the door and have more flexibility to test the camera under various lighting conditions, etc. This will also make it easier to experiment with the camera's settings to work on improving the picture. ![2017-07-15 21-57-33 1125](/wp/2017/07/2017-07-15-21-57-33-1125.jpg) One thing that will also need to be in order before the cartridge can work like real film is a way to toggle recording using the camera's trigger switch. Ideally this would be coordinated with the film speed setting of the camera, (my original idea was to measure the movement of the prong that normally moves the film) but it might be simpler to measure the speed of the take-up reel drive hub. For now it could be even simpler and simply toggle on/off based on the movement of the hub, since at the moment I'm making no effort to coordinate the shutter between the Super8 camera and the Raspberry Pi camera module.---
tags:
  - gullicksonlaboratories
title: Open Digital 8
link: http://jjg.2soc.net/2017/07/10/open-digital-8/
author: jgullickson
description: 
post_id: 3603
date: 2017-07-10 23:07:04
created_gmt: 2017-07-11 04:07:04
comment_status: open
post_name: open-digital-8
status: publish
post_type: post
---

# Open Digital 8

I've often complained that no one seems to make a proper indie movie camera anymore. There are phones with cameras and there are still cameras that can shoot movies and there are even "action cams" which can be mounted anywhere and take enormous abuse, but there doesn't seem to be a modern equivalent to the [Panasonic DVX-100](https://en.m.wikipedia.org/wiki/Panasonic_AG-DVX100) or Canon XL-2 of the original digital filmmaking revolution. ![](/wp/2017/07/img_1068.jpg) So I sat down and made a list of things I'd like if I were designing such a camera, and interestingly the list looked a lot like features of the Super8 cameras I've experimented with. I really loved shooting Super8, but it's just too expensive and time-consuming (in terms of shoot-edit-release) for my projects. However thinking about this reminded me of an effort in the early days of digital phototography to create a "digital film" that could be used to convert a film camera to digital.  This got me thinking about applying the same idea to a Super8 film cartridge, and re-using Super8 cameras instead of building the entire thing from scratch. ![](/wp/2017/07/img_1071.jpg) It turns out someone else thought of the same idea, and designed a very attractive version called "[Nolab](http://nofilmschool.com/2013/12/nolab-digital-super-8-cartridge-make-film-cameras-go-digital)".  However it doesn't appear to have actually been produced, so it may have just been a prototype or a design study. Regardless, there are a few things I think are important that the Nolab design doesn't address.  First I want the design to be open (both hardware and software) so that it can be modified and improved by anyone without constraint.  Second I want to use as much off-the-shelf tech as possible so that hackers and filmmakers with more time than money can build their own and access the technology.  Third, I want anything that isn't a commodity part to be 3D printable so it can be re-created without a lot of special tools, materials, etc. Openness is very important to me, because I've owned a number of digital cameras whose hardware was capable of doing more than the factory software would allow.  I've often wished I could customize the behavior of the camera to suit my shooting style and that's just not possible with proprietary hardware and software. I started out with some [3D modeling](https://gitlab.com/jgullickson/open-digital-8) to attempt to re-create a Super8 film cartridge that I could use to see how much room there would be to work with.  This would have been much easier if I had a film cartridge in hand.  That not being the case, I took some measurements from the two Super8 cameras I have and iterated on the model a few times.  After awhile I had something that seemed to fit in both cameras. ![](/wp/2017/07/img_1027.jpg) The next step was to figure out what electronics I could cram in there.  I started with a Raspbery Pi because I happened to have one along with the Raspberry Pi camera module, but it was immediately apparent that a regular Pi wasn't going to fit. ![](/wp/2017/07/img_1032.jpg)![](/wp/2017/07/img_1031.jpg) I would have preferred to have used a Next Thing Co CHiP, but I'm not aware of a tightly-integrated camera module for the CHiP, so instead I decided to try a Raspberry Pi Zero W. While waiting for the new electronics I did a few basic tests with the Pi and camera I had on-hand.  This revealed another problem. ![](/wp/2017/07/img_1072.jpg) ![](/wp/2017/07/img_1036.jpg) The film of a Super8 cartridge is off-center, so the camera's apreature is offset to match.  Given the size of the Rapberry Pi camera module (and the fact that the lens is in the center of the module), it might not be possible to place the camera module in-line with the aperature as I originally imagined.  Determining this for certain will require additional testing, but if it's not possible it might be necessary to use a mirror or prism to place the camera module at a 90 degree angle to the aperature. Another problem with using a Raspberry Pi is that it has no facility for charging or running from a battery.  To address this I ordered a [li-ion battery charger/power supply](https://www.adafruit.com/product/1903) that will provide 5 vdc to power the Pi and camera board from a small li-ion battery I had on-hand.   ![](/wp/2017/07/img_1040.jpg) I think this will fit along with the Pi and camera module inside the printable film cartridge, but we'll know for sure once the parts arrive. At this point I have a good starting point for the printable case and a complete set of electronics for the project.  Aside from waiting on parts, there's a few immediate problems to solve: 

  * How to get the camera module aligned with the Super8 camera aperature
  * How to fit all of the electronics inside the film cartridge
  * How to synchronize the digital camera frames with the frame rate and mechanical shutter of the Super8 camera
Of course there are a lot of other problems to tackle but these are the ones I think need to be taken on next.  I'm hoping to do a few more tests aligning the optical path while I'm waiting on parts so I can have a working plan to place the camera module when the hardware arrives. I'm also doing research on how the software side of driving the Raspberry Pi camera works to see how the GPIO could be used to synchronize the digital camera with the mechanical camera's timing. For the curious I'm posting the design files and code on [Gitlab](https://gitlab.com/jgullickson/open-digital-8).  There's not a lot there yet, but that will change as the experiments continue and the project begins to take more shape---
tags:
  - gullicksonlaboratories
title: Open-Source Home Automation with Alexa
link: http://jjg.2soc.net/2017/02/21/open-source-home-automation-with-alexa/
author: jgullickson
description: 
post_id: 18
date: 2017-02-21 05:35:56
created_gmt: 2017-02-21 11:35:56
comment_status: open
post_name: open-source-home-automation-with-alexa
status: publish
post_type: post
---

# Open-Source Home Automation with Alexa

Alexa is cool, and I think one of the coolest things you can do with it is control stuff around your house. However, based on numerous experiences with what happens to proprietary, closed-source devices when their creators fail/sell-out/etc. (especially if those devices depend on a cloud service, etc.) I try to avoid becoming dependent on such devices. So I went looking to see if there were open-source alternatives to the kinds of devices (light switches, outlets, etc.) that I wanted to use and it turns out (as often is the case with open-source) someone's already done most of the work! Even better, this particular implementation is completely stand-alone; there's no back-end cloud service that needs to be running and accessible just to keep the devices working! The heart of these devices is the ubiquitous and inexpensive ESP8266, coupled with a piece of software called [fauxmoESP](https://bitbucket.org/xoseperez/fauxmoesp). This software turns the ESP8266 into something that can emulate the popular [WeMo devices](http://astore.amazon.com/jjg00-20/detail/B00BB2MMNE) which are supported natively by Alexa. Install fauxmoESP on an ESP8266 and it shows up in the Alexa app just like a WeMo device, no additional skills required. Here's how I got this working in about 30 minutes: 

## Stuff I used:

  * [ESP8266](http://astore.amazon.com/jjg00-20/detail/B01EA3UJJ4)
  * [VisiPort2 USB to FTDI adapter](https://www.tindie.com/products/Earth_People_Technology/ftdi-usb-to-serial-breakout-board-the-visiport2/)
  * Arduino IDE 8.1.8
![2017-02-17-13-50-22-0007](/wp/2017/02/2017-02-17-13-50-22-0007.jpg)

### Software

The first step is to setup the Arduino IDE to compile and install code on the ESP8266. This used to be complex, but now it's simply a matter of [downloading the IDE](https://www.arduino.cc/en/Main/Software) and installing ESP8266 support using the Boards Manager [described in detail here](https://github.com/esp8266/Arduino#installing-with-boards-manager). Once the installation completes, you can close the Arduino IDE and proceed to the hardware step. 

### Hardware

I'm using the cheapest ESP8266 board that I know of, so this was harder for me than it would be if you were to use something like [Adafruit's HUZZAH board](https://www.adafruit.com/product/2471). The board I used was selected because it's what was in the parts bin, and if you're buying parts for a project like this you can do a lot better (I'll discuss some of the more interesting options below). To prepare the ESP8266 for programming, connect it to the FTDI adapter like so: 

FTDI -> ESP8266

GND
->
GND

GND
->
GPIO0

VCC
->
VCC

VCC
->
CH_PD

TX
->
RX

RX
->
TX
Now the hardware can be connected to your computer and we can start preparing the firmware. 

### Code

fauxmoESP has a few library dependencies that we'll need to install before we can compile and upload the code to the ESP8266. I use `git` for this job. Check out the following into your Arduino/libraries directory: `git clone https://github.com/me-no-dev/ESPAsyncTCP.git` `git clone https://github.com/me-no-dev/ESPAsyncWebServer.git` ...and then finally `git clone https://bitbucket.org/xoseperez/fauxmoesp.git` Now you're read to re-open the Arduino IDE and try running the example. From the File menu select File -> Examples -> FauxmoESP -> FauxmoESP_Basic. The example sketch includes a credentials file you'll need to customize to match the settings of your WiFi network, so once you open the example, click-over to the `credentials.sample.h` tab and set your WiFi SSID and password. Then select File -> Save and save a copy of the sketch to your `Arduino` directory. Finally, make a copy of the `credentials.sample.h` file named `credentials.h`. That's all changes you need to make to the code to test things out. The next step is to compile and upload the code to the ESP8266. Make sure the device is connected via USB and select "Generic ESP8266 Module" (or whatever your particular module is called) from the Tools -> Board menu, and select the Port that the FTDI adapter is connected to as well. If the gods are with you, you should now be able to click the "Upload" button and the sketch will compile & install onto the ESP8266. This can take a few minutes but once it's complete you can switch the module over to "run" mode, restart it and use the Serial Console to see if it's working. To switch to run mode, remove the connection between GND and GPIO0, then remove and re-apply the VCC connection on the module. Select Tools -> Serial Monitor from the Arduino IDE (make sure the baud rate is set to 115200) and you should see messages from the ESP8266 about connecting to your WiFi network. Once the connection is complete, open the Alexa app on your phone and select "Smart Home" from the hamburger menu. Scroll down to the "Your Devices" section and tap "Discover Devices". After a few minutes this should come back with a list of new devices with names like "light one", "light two", etc. If you look at the code in the example sketch you'll see that these are the names of the devices configured on the ESP8266. If you say "Alexa, turn on light one", you should see a message in the serial console indicating that the command was received. 

## What's Next

From here you can modify the code to suit your application. The example code simply sets the `GPIO0` pin `high` or `low` regardless of what light is requested depending on whether an `on` or `off` command is received. With a basic ESP8266 you could connect the GPIO0 pin to a relay or MOSFET and switch high-power devices like lights, fans, etc. on and off. For a less DIY-approach I was excited to find out that there are a number of [off-the-shelf devices](https://www.amazon.com/dp/B01FUC9U58/) available on that incorporate the ESP8266 along with the required electronics and switching components that are inexpensive and ready to use. All that you need to do is configure and install the FauxmoESP firmware on them and you're ready to go. Ultimately, using this approach may save you some money on automating your household, but more importantly it gives you control over the devices that you would be giving up if you used a proprietary device. It's a little more work, and a little less convenient to configure, but once they are setup you know that they will continue to work even if the company who made the hardware goes out of business or is bought by a competitor. Additionally, these devices will work even without an Internet connection, and should Amazon retire Alexa (or turn it into something incompatible with these devices), you can easily modify the firmware and use them with another system.---
tags:
  - gullicksonlaboratories
title: Pietenpol Annual Reunion
link: http://jjg.2soc.net/2017/07/24/pietenpol-annual-reunion/
author: jgullickson
description: 
post_id: 4159
date: 2017-07-24 07:30:02
created_gmt: 2017-07-24 12:30:02
comment_status: open
post_name: pietenpol-annual-reunion
status: publish
post_type: post
---

# Pietenpol Annual Reunion

Last weekend I visited the [annual Pietenpol reunion](http://www.pietenpols.org/annual-pietenpol-reunion/) for the first time. I've been meaning to go for years (it's only about an hour and a half away), but something always came up. This year I made an extra effort to go because my friend Dave (who is building a [Pietenpol](https://en.wikipedia.org/wiki/Pietenpol_Air_Camper) of his own) was going to be giving a presentation. [caption id="attachment_4179" align="aligncenter" width="3264"]![Photo 17-07-22 10-20-46 1196](/wp/2017/07/photo-17-07-22-10-20-46-1196.jpg) Dave's Pietenpol[/caption] Dave's presentation was great (I can't wait to try powdercoating something!), but he also gave me a very complete tour of the event, including some amazing non-Pietenpol aircraft that is part of the [Kelch Aviation Museum](http://www.kelchmuseum.org/) (thanks Dave!). Here's a few photos from the event, I wish I could have captured more but honestly I was too busy taking it all in. [gallery ids="4184,4185,4186,4188,4190,4191,4193,4195,4197,4199,4201,4203,4205,4206,4212,4215,4218,4220,4222,4223" type="rectangular"] If you or someone you know has any interest in flying I highly recommend checking out this event. Pietenpol's really epitomize the essential elements of flying and while I've yet to experience what it's like to be inside one in the air, seeing the aircraft in person and meeting the people who build and fly them was a great way to learn what flying might have been like at the dawn of aviation.---
tags:
  - gullicksonlaboratories
  - rain
title: PINE64 Clusterboard
link: http://jjg.2soc.net/2018/02/23/pine64-cluster-board/
author: jgullickson
description: 
post_id: 5097
date: 2018-02-23 10:00:44
created_gmt: 2018-02-23 16:00:44
comment_status: open
post_name: pine64-cluster-board
status: publish
post_type: post
---

# PINE64 Clusterboard

This week I learned that [PINE64](https://www.pine64.org) (the company who makes the boards used in Mark II) released a [new piece of hardware](https://www.pine64.org/?product=clusterboard-with-7-module-slots-include-one-free-sopine-module-during-promotion-period) that could accelerate my work on RAIN PSC considerably. ![cluster-board-600x600](/wp/2018/02/cluster-board-600x600.jpg) The [PINE64 Cluster Board](https://www.pine64.org/?product=clusterboard-with-7-module-slots-include-one-free-sopine-module-during-promotion-period) combines a gigabit Ethernet switch with seven SODIMM slots (along with the associated support components) into a single mini-itx sized circuit board. The SODIMM slots are designed to hold [SOPINE](https://www.pine64.org/?page_id=1486) modules which incorporate the essential components of the [same single-board computer](https://www.pine64.org/?page_id=46823) I'm using in RAIN Mark II. The result is a very compact 7-node cluster. 

## So what does this have to do with RAIN?

Much of my work on RAIN so far has been focused on cramming 8 full-sized PINE64 boards into a box. The cluster board, while not as flexible as my own designs solves this problem much faster and more compactly than I can. The board doesn't _scale_ the way my 3d printed design does, but that's not a problem for the Mark II PSC. The existence of this cluster board also makes the idea of exploring the SODIMM compute module form-factor more compelling. This is something I had originally considered, but didn't explore initially because I knew it required working with components and materials that would slow me down at this stage. With something like the cluster board available, I could design other modules based on that form factor (RISC-V compute, FPGA application-specific logic, etc.) without having to take-on the big-bite of designing the entire system, fabbing large boards, etc. Another exciting aspect of the existence of this board is that [there are others](https://github.com/ayufan-rock64/cluster-build) working on the low-level details of running a cluster based on it. The SOPINE has 128MB of on-board flash which makes for a very slick netboot configuration. There's already a working implementations of network booting the nodes on this board which means there might be one less thing for me to do on the software side of things. Finally (and to me most importantly), the design (like all other PINE64 designs) is open-source hardware. This means that if PINE64 decides to abandon it, I can still continue to use it. This also means that I can improve or expand the design as necessary. 

## What's Next?

I've placed an order for one of these cluster boards as well as one SOPINE module. My plan is to see how the cluster board physically fits into the current PSC case (given the mini-itx form-factor, I think it will work). Assuming the cluster board fits, my plan is to pair it with a single full-size PINE64 board which will serve as the front-end node and optionally provide a terminal interface (HDMI video, USB keyboard & mouse) to preserve the stand-along capability of the PSC. This will however necessitate a change to the way the control panel works since fewer GPIO's are exposed for each SOPINE module. I've already begun working on an I2C implementation of the LED's and toggle switch inputs. While I'm waiting for the cluster board to arrive, I'll be focused on prototyping this interface change as well as coming up with a working Linux build for the front-end node that can be used to netboot the SOPINE module(s) on the cluster board. All of these changes (if successful) will allow me to move much more quickly into working on the software side of Mark II, so I'm very excited to see how these next steps pan-out.
---
tags:
  - gullicksonlaboratories
title: Post-Privacy (or - How I learned to stop worrying and love Alexa)
link: http://jjg.2soc.net/2017/03/10/post-privacy-or-how-i-learned-to-stop-worrying-and-love-alexa/
author: jgullickson
description: 
post_id: 315
date: 2017-03-10 09:37:59
created_gmt: 2017-03-10 15:37:59
comment_status: open
post_name: post-privacy-or-how-i-learned-to-stop-worrying-and-love-alexa
status: publish
post_type: post
---

# Post-Privacy (or: How I learned to stop worrying and love Alexa)

This was going to be a long and detailed post explaining how I, as a "privacy enthusiast", can be comfortable having an Alexa device in my home. However, in light of [recent and seemingly perpetual](https://wikileaks.org/ciav7p1/) leaked government documents, I've boiled it down to one sentence: 

# "If you are an American you have no privacy; so why not enjoy it?"
---
tags:
  - gullicksonlaboratories
title: Project Oberon
link: http://jjg.2soc.net/2017/02/22/project-oberon/
author: jgullickson
description: 
post_id: 70
date: 2017-02-22 10:45:40
created_gmt: 2017-02-22 16:45:40
comment_status: open
post_name: project-oberon
status: publish
post_type: post
---

# Project Oberon

I've written several times about [Project Oberon](http://www.projectoberon.com/), I guess I'm kind of obsessed with it.  There is just something delicious about a computer that is so open, all the way down to the code that generates the processor itself. But it's not just that.  The Oberon workstation also embodies a method of computing that represents an alternative time-line along the development of user interfaces and models of personal computing itself.  The only other system I'm aware of that followed a similar path was [Jef Raskin's](https://en.wikipedia.org/wiki/Jef_Raskin) [Cat](https://en.wikipedia.org/wiki/Canon_Cat) and the related [Swyft](http://www.digibarn.com/collections/systems/swyft/index.html) devices.  This being a text-oriented user interface (not to be confused with a terminal or command-driven interface). [caption id="attachment_101" align="aligncenter" width="430"]![canon-cat-screen3](/wp/2017/02/canon-cat-screen3.jpg) Canon Cat help screen (more at [oldcomputers.net](http://www.oldcomputers.net/canon-cat.html))[/caption] I won't attempt to explain the interface in detail here, but the gist is that you interact with the computer by typing text into a document, and execute commands by selecting the commands (and their operands) from the document.  This sounds simple enough but it's such a departure from the way we use computers now that it's hard to understand the power this approach lends to users. So I periodically return to the desire to have a Project Oberon-based computer, and in particular a portable computer akin to a laptop.  There are a handful of people making hardware that is capable of running Oberon natively (including the synthesized CPU implemented in an [FPGA](https://en.wikipedia.org/wiki/Field-programmable_gate_array) ), but all of the ones I've found are oriented toward desktop workstation use.  This is somewhat due to the nature of the Oberon workstation reference design which uses a VGA interface for the display and PS/2 interfaces for keyboard and mouse. [caption id="attachment_96" align="alignnone" width="640"]![pepino_v1-1](/wp/2017/02/pepino_v1-1.jpg) [Pepino](http://www.saanlima.com/pepino/index.php?title=Pepino_Oberon), a development board designed for use as an Oberon workstation[/caption] While it's possible to create adapter to interface these ports with the components you would use in a portable computer, it's awkward, and seems somewhat silly when you consider the fact that one of the objectives of the computer is that it is an open design.  A more reasonable approach would be to adapt the hardware design to incorporate the necessary components to make it suitable for portable use. Of course this is usually where my ambition begins to become depleted.  The idea of having a portable Oberon workstation is a very romantic one, but in the face of the complexity and time investment necessary to design, fabricate and assemble a custom system board, I start to second-guess how confident I am that I'll be able to put the system to practical use.  This is based less on the merits of the system and more on my perception of the "opportunity cost" of building a system from the "chips up" vs. something with broader applications and lower risk (perhaps an ARM computer with a Linux-based O/S). The problem with this type of "being reasonable" is that it doesn't move the state-of-the-art forward the way a fresh approach does*.  While it's significantly simpler and more practical to build a portable computer around Linux, why bother when similar machines already exist?  Sure incremental improvements in size, portability, quality, etc. can be made, and a machine more suited toward an individuals preferences is possible, but ultimately it only moves things along in an incremental fashion, and it does little to open-up new territory for different modes of work, liberation from legacy hardware architectures or innovative new applications. So while it doesn't make as much "sense" to sink time into developing something like a portable Oberon workstation (when you could be building something that people want instead), doing so does a lot more to advance the state-of-the-art of personal computers.  Is that enough to motivate me to make the investment (in both time and money)?  I guess we'll see.---
tags:
  - gullicksonlaboratories
  - rain
title: Pump the Brakes
link: http://jjg.2soc.net/2018/05/21/pump-the-brakes/
author: jgullickson
description: 
post_id: 5189
date: 2018-05-21 10:52:53
created_gmt: 2018-05-21 15:52:53
comment_status: open
post_name: pump-the-brakes
status: publish
post_type: post
---

# Pump the Brakes

![2018-05-19 08.55.40](/wp/2018/05/2018-05-19-08-55-40.jpg) Looks like work on RAIN Mark II will be slowing-down a bit for a couple of reasons: First, the snow has receded which means work that can only be done during the month we call "summer" takes priority over anything that can be done inside (plus there's just lots of fun things to do outside...). Second, it looks like I misunderstood how the [2018 Hackaday Prize](https://hackaday.io/prize) works. I assumed that having your project in the top 20 positions of the [leaderboard](https://hackaday.io/prize) when a challenge concluded was what they meant by _"The top twenty projects from each challenge will be awarded $1000 and will move on to the finals..."_. Instead, they selected [20 projects](https://hackaday.com/2018/05/02/these-twenty-amazing-projects-won-the-open-hardware-design-challenge/) using some other process, and RAIN Mark II didn't make that cut. This is a bummer, because I invested time and effort promoting the competition with the misguided idea that doing so could result in injecting some resources into the project (which could have dramatically accelerated its progress). I should have spent this time _working_ on the project instead. But it's a good reminder to me of the pitfalls of competition, and that any project whose success relies on it is vulnerable to competition's inherent inefficiencies. I'm glad to have had an opportunity to have these inclinations put in-check while the stakes are lower than they would be later on in the project. The whole experience has caused me to reflect on the purpose of the project itself and I have a renewed focus as a result. The next step for Mark II will be to complete the assembly of an ARM-based cluster with the same node count as the Intel-based Mark I machine. Once this is complete, I can duplicate Mark I's run of the [hpl benchmark](http://www.netlib.org/benchmark/hpl/) on Mark II and have an apples-to-apples comparison of the performance difference between the two architectures. This was the original purpose of building Mark II, and once this is known it will be possible to describe an ARM-based system with equivalent power to an Intel-based system and determine at what scale ARM outperforms Intel in terms of processing power vs. total system efficiency (cost, power consumption, cooling, physical space, etc.). This will complete the work on the hardware side of Mark II. I can then move-on to both the software aspect of Mark II, as well as using the Mark II hardware as a platform for the development of Mark III hardware components. I'll need around $250.00 worth of hardware to get the system to a point where these tests can be run. I'm selling-off the Mark I hardware in an effort to cover it, but it's indeterminate how long this will take and as such progress will stall until this is complete. Thank-you to everyone who took the time to support the project on [Hackaday.io](https://hackaday.io/project/85392-rain-mark-ii-supercomputer-trainer).
---
tags:
  - gullicksonlaboratories
title: Q2K
link: http://jjg.2soc.net/2017/05/20/q2k/
author: jgullickson
description: 
post_id: 2482
date: 2017-05-20 23:46:54
created_gmt: 2017-05-21 04:46:54
comment_status: open
post_name: q2k
status: publish
post_type: post
---

# Q2K

I'm building a [quantum computer](https://en.m.wikipedia.org/wiki/Quantum_computing).  Not just for me, but for everyone. Modeled after the microcomputer revolution (which took the power of classsical computers from a few large corporations and universities and put it in the hands of everyone), the goal of Q2K is to ensure that the power or quantum computing is available to anyone, not just the richest companies, governments or universities. I've been interested in quantum computing for a long time, but it wasn't until I saw the binary computer [George Stibitz](https://en.m.wikipedia.org/wiki/George_Stibitz) built on his kitchen table that I saw a path toward creating one. ![](/wp/2017/05/img_0704.jpg) This rudimentary device kicked-off digital computer development at AT&T and started the ball rolling toward the computers we have today. I know that building a complete, useful quantum computer is a daunting task, but seeing this device made me wonder if I could create the quantum equivalent?  If so, it could be the catalyst for a more sophisticated quantum computer that could be developed outside of existing corporate or academic environments where most work on quantum computing currently takes place. This thought led to the Q2K project.  Simply put, I want to develop an open-source quantum computer with at least 2,000 [qbits](https://en.m.wikipedia.org/wiki/Qubit) capacity at a costs of less than $2,000.00 US. The reason for this is to gradually raise awareness of quantum computing and place its power into a larger and more diverse number of hands. Instead of creating a small number of powerful but purpose-specific quantum computers, I want to create a basic design which is inexpensive enough to be accessible to individuals or small organizations.  One that is open and flexible enough for a range of applications that have not even been imagined yet. The Q2K project defines a dynamic number of milestones toward accomplishing the goal of making quantum computing available to everyone: 

  1. Cobble together a functional qbit by any means necessary
  2. Select a single useful application of quantum computing to demonstrate the utility of a quantum computer
  3. Build enough qbits to assemble a quantum computer capable of executing the application selected in step 2
  4. Refine and improve the process and materials used to build qbits as necessary to complete step 3
  5. Select additional quantum computing applications which can be executed on the assembled computer
  6. Design both a guide and kit that others can use to build the computer and execute these applications
  7. Release the above as open source
  8. Use the kit described above to generate revenue for the next phase of development, as well as to begin seeding the world with quantum computer developers and hobbyists
  9. Use everything learned above, along with community support, to iterate toward a complete Q2K design
This plan deliberately follows an evolutionary path set by the microcomputer.  By releasing the embryonic quantum device as a kit, a community of curious and dedicated hobbyists can begin to form.  This same community can then begin to explore quantum computing and dream of new applications in parallel with development of the final Q2K machine.  By inviting this community to collaborate on each iteration, there is a greater chance or producing a useful general-purpose device.  This is likely to help build a dedicated fan base and create excitement leading up to the release of the completed machine. Q2K is the latest of my personal "grand challenges", and as such it is expected to consume the majority of my hacking resources.  That means that some or all of my other projects may slow down or get mothballed until Q2K is complete (or at least until each milestone is reached). Like previous grand challenges, I have framed the schedule around my own age.  My next birthday (02/16/2018) has been selected as the deadline of achieving the milestone of creating a working qbit.  My 50th birthday (02/16/2024) has been selected as the target for completing the entire project.  Additional deadlines will be selected as each step of the project has been completed. Quantum computing is to computing what nuclear weapons are to warfare.  In a world where computing power is increasingly more useful than military might, the nations/companies/etc. who possess quantum computers will hold threats and advantages over those who do not.  I believe that allowing such power to concentrate in the hands of a single or small number of entities is not in the best interest of humanity.---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden Mark II - Chassis
link: http://jjg.2soc.net/2017/11/25/raiden-mark-ii-chassis/
author: jgullickson
description: 
post_id: 5019
date: 2017-11-25 10:36:58
created_gmt: 2017-11-25 16:36:58
comment_status: open
post_name: raiden-mark-ii-chassis
status: publish
post_type: post
---

# Raiden Mark II - Chassis

_**Note**: This is a long one.  I've been neglecting documenting the project so this post is a catch-up for the last few weeks of work.  Future updates will be more frequent and bite-sized._ When I think about how to arrange and contain the single-board computers (SBC's) that make-up the computational part of Raiden Mark II, there are some potentially conflicting requirements: The key areas of improvement over Mark I are 

  * Smaller
  * Quieter
  * Lower power consumption
  * Less expensive
To this list I also add "repeatable", because I want to come up with a design that others can use.  I don't want to rely on any unusual, exotic or hard to fabricate components.  I also want a design that gives a nod to the computers which inspired it, but not at the cost of the above requirements. After some noodling, sketching and looking at old photographs I settled on an external design inspired by very early personal computers like the [Altair 8800](https://en.wikipedia.org/wiki/Altair_8800), [IMSAI 8080](https://en.wikipedia.org/wiki/IMSAI_8080), etc. ![IMG_1700](/wp/2017/11/img_1700.jpg) Aside from looking cool, there are some practical aspects to this design. The front panel laden with switches and [blinkenlights](https://en.wikipedia.org/wiki/Blinkenlights) is used to display status information for each node in the cluster, and the switches are used to control each node individually.  These tactile, real-time feedback features are very useful when writing and testing software for a system like this. While the same could be accomplished via software, it's often faster to be able to glance at an LED to see if there is a problem, or throw a switch to reset a single node. This also removes the need for any kind of video or keyboard connection to each node which is something that was a significant inconvenience when working on Mark I. Another advantage to this design is that the largest part of the chassis (the cabinet) can be found fairly inexpensively on Ebay.  This eliminates the need to fabricate large, strong parts and goes a long way toward making it easier to produce more than a one-off build. ![IMG_1714.JPG](/wp/2017/11/img_1714.jpg) Once I settled on the outside, I had to figure out how to arrange and connect all the components on the inside. I knew I wanted something modular and to minimize the amount of loose wiring.  I determined that all the connections I needed to make (aside from network connections) could be found on the 40-pin GPIO header (the "Pi-2 Bus" as it is sometimes called). This being the case I could mount each SBC to a "backplane" using these connectors, similar to how PCI cards work in a regular desktop PC. My first iteration of this was to design a printable backplane which would hold 8 40-pin IDE-style connectors. From these connectors I would run wires to a single connector used to make connections between the backplane and the LED's, switches, etc. mounted in the front panel. [caption id="attachment_5023" align="alignnone" width="3264"]![IMG_1709.JPG](/wp/2017/11/img_1709.jpg?w=3264) Above: the guts of an 8-port Gigabit Ethernet switch which will be used inside Raiden Mark II.  Below: 3D printed "board" to verify size and connecting points for the backplane model.[/caption] This backplane was also designed to align with the mounting holes of the 8-port Ethernet switch I'm including inside the case. The idea was to stack the backplane on top of the switch, that way all of the interconnected pieces could be removed from the chassis as a unit. However, after experimenting with a few different layouts the part became somewhat large and exceeded the size in which my printer is capable of printing reliably. After a few failed attempts at making a printable backplane, I went back to the drawing board. ![IMG_1720](/wp/2017/11/img_1720.jpg) My second idea was to use a printed circuit board (pcb) for the backplane. This would not only eliminate the need to print a large part, but it would also allow my to "hide" the wiring between the sbc's and the front panel connector. This seemed like a great solution, however after looking into several options for creating the boards, I ruled it out due to cost, complexity and lack of flexibility (I'm still at the experimental stage and I don't know exactly how I want all the connectors wired together yet). In parallel with work on the backplane I was giving thought to the front panel. I knew what I wanted it to look like, but I wasn't exactly sure how to pull that off. The panel requires many holes for the various indicators and switches and it also requires a number of "legends" as well. I really want to re-create the look of the old machines so the ability to apply these legends to the panel using the right lines and fonts is important, and the sheer number of holes and the accuracy of their locations rules-out my typical approach of grabbing a drill and free-handing it. Also I wanted to come up with a process that could be repeated by others, and might be applicable to some level of volume production. It seemed like using a laser might be the right tool for the job, but the chassis is steel and I know that steel-cutting lasers are not the most accessible tool. So I started thinking about how both tasks might be accomplished using more common 40-50 watt cutting/engraving lasers. [caption id="attachment_5031" align="alignnone" width="828"]![mark-ii-front-panel](/wp/2017/11/mark-ii-front-panel.jpg) Initial front panel layout (subject to change)[/caption] The first step in this process is to draw the panel using a tool which can output something the laser can consume. I knew [Inkscape](https://en.wikipedia.org/wiki/Inkscape) was capable of this (since I had used it to create drawings for my first CNC project, a [really crappy laser engraver](https://www.youtube.com/watch?v=VNuxN39d9xs) ). After a bit of a learning curve I was able to produce a drawing of the panel that came off my laser (printer) while preserving the scale. Aside from scale the hardest part was reliably capturing the dimensions of the hole & attachment points the panel would need. ![IMG_1729](/wp/2017/11/img_1729.jpg) At this point I need to learn more about using a laser and do some experiments to proceed with the panel. I did a lot of reading and decided at some point I want to add a [K40 laser](https://www.reddit.com/r/lasercutting/wiki/k40) to the laboratory, but for now I'm going to setup some time to visit the [Idea Studio](http://www.fdlpl.org/ideastudio), learn how to use their laser and run some tests that I'll elaborate on in a future post. Now that work on the panel is blocked, I switched back to thinking about the backplane. One problem the previous backplane designs don't address is how the components of the _panel_ are mounted. In the old machines, there was a pcb for the backplane and a second one for the panel, and then holes were cut in the panel where the components mounted on this second board emerged. This means two large pcb's and a lot of commitment to the connections between parts. Since both of these facts are not ideal for this project (at least at this stage) I started noodling on alternatives to the pcb backplane approach, and came up with something new.
---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden Mark III - greater RISC,  greater reward
link: http://jjg.2soc.net/2017/12/15/raiden-mark-iii-greater-risc-greater-reward/
author: jgullickson
description: 
post_id: 5048
date: 2017-12-15 13:18:36
created_gmt: 2017-12-15 19:18:36
comment_status: open
post_name: raiden-mark-iii-greater-risc-greater-reward
status: publish
post_type: post
---

# Raiden Mark III - greater RISC,  greater reward

I selected ARM processors for Raiden Mark II for a number of reasons, but one of the most important ones was that I was under the impression that there was an open-source implementation of the ARM CPU and by selecting this as the foundation of Raiden, I could maximize flexibility (as well as control) in future iterations. As it turns out, that was not completely correct. There are indeed open-source ARM processors (which can be implemented in silicon or synthesized via FPGA, etc.) however the "modern" ARM CPU (including the ones I'm using in Raiden Mark II) are _proprietary_, and there is no real hope that these designs will become open in the future. This doesn't change my immediate plans because a proprietary ARM processor still achieves the primary goals I have for Raiden Mark II, but it _does_ impact plans I have for the next iteration of Raiden, Mark III. The good news is that as I was discovering that ARM isn't going to provide everything I need, there is another RISC architecture that does, and it will be available as a component similar to the ARM SOC I'm using in Raiden II. This new architecture is [RISC-V](https://en.wikipedia.org/wiki/RISC-V), and it is a completely open design. RISC-V processors are currently available however they are aimed at the embedded market (think Arduino). That said, there is already [Linux kernel support](https://github.com/riscv/riscv-linux) for RISC-V, and a [four-core system-on-a-chip](https://www.sifive.com/products/risc-v-core-ip/u54-mc/) capable of running Linux is expected to ship in the first quarter of 2018. In addition to production chips realized in silicon, there are RISC-V cores available which can be [synthesized via FPGA](https://github.com/SpinalHDL/VexRiscv), and this is of particular interest to me because I have planned to make FPGA fabric a key component of Raiden Mark III since the beginning. It's not clear to me yet whether a hybrid SOC+FPGA or pure FPGA RISC-V implementation is the right choice for Raiden Mark III, but what is clear is that going forward, the CPU architecture of choice for Raiden will be RISC-V.
---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden Mark II - Resume From Suspend
link: http://jjg.2soc.net/2018/01/10/raiden-mark-ii-resume-from-suspend/
author: jgullickson
description: 
post_id: 5057
date: 2018-01-10 17:00:31
created_gmt: 2018-01-10 23:00:31
comment_status: open
post_name: raiden-mark-ii-resume-from-suspend
status: publish
post_type: post
---

# Raiden Mark II - Resume From Suspend

The end of 2017 had a few surprises in store for us which preempted work on Raiden. With that behind us, I've been able to resume the project and work on getting things back on-track. Cold weather always makes my printer temperamental but thanks to the [Idea Studio](http://www.fdlpl.org/ideastudio) and their [January Makeathon event](http://calendar.fdlpl.org/eventsignup.asp?ID=21719&rts=&disptype=info&ret=eventcalendar.asp&pointer=&returnToSearch=&num=0&ad=&dt=mo&mo=1/1/2018&df=calendar&EventType=ALL&Lib=&AgeGroup=&LangType=0&WindowMode=&noheader=&lad=&pub=1&nopub=&page=&pgdisp=) I was able to run-off a copy of the (redesigned for PINE64) compute module chassis. ![IMG_1976](/wp/2018/01/img_1976.jpg) ![IMG_1983](/wp/2018/01/img_1983.jpg) I also had a chance to get my "badge" to operate their laser cutter which I'm planning to use to cut & engrave the (also redesigned) control panel. [caption id="attachment_5061" align="aligncenter" width="883"]![Mark II Front Panel v2](/wp/2018/01/mark-ii-front-panel-v21.png) what happened to my cool font...?[/caption] Based on the fit of the print, I'm planning to drop one row of switches from the control panel. At first I was disappointed about this because let's face it, panel switches are cool, but since the original design I've come up with an alternative to having an individual power switch for each node that is ultimately superior. I've also re-purposed the second switch from being a momentary "boot/shutdown" switch to toggling the LED array between displaying system status (boot, network, temperature warning and two user-definable indicators) and unix load. I'm also considering exposing the two USB ports of the "head node" through the front panel and revealing an HDMI connector through the back. This would allow the computer to be used in a stand-alone fashion by connecting a keyboard, mouse & monitor. I'm on the fence about this for a couple reasons. One is that I've always thought of Raiden as a specialized machine tuned for high-performance computing applications and as such would be managed by an external workstation like most supercomputers. I also don't like what it does to the aesthetics. On the other hand, I can see the value of making it a stand-alone machine especially given the developer-oriented audience of Raiden Mark II. While I'm concerned that putting a full-blown desktop on the head node might impact performance or otherwise "pollute" the system, I want to consider the possibility that by doing so, there may be users who are able to justify owning the machine if it doesn't require an additional workstation to use it.

## Comments

**[Etienne](#132 "2018-03-15 14:29:25"):** Suggestion: On the bottom, move your text in an inch on both sides so the screws don't go through the text.

---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden Mark I
link: http://jjg.2soc.net/2017/08/29/raiden-mark-i/
author: jgullickson
description: 
post_id: 4521
date: 2017-08-29 21:57:02
created_gmt: 2017-08-30 02:57:02
comment_status: open
post_name: raiden-mark-i
status: publish
post_type: post
---

# Raiden Mark I

I wrote an [introduction to the Raiden project](http://jjg.2soc.net/2017/08/08/raiden/) awhile back, but it's been too long since I've done an update, and a lot has happened in the meantime. I've been keeping a fairly detailed log (which may be part of the reason I've neglected to post updates) but in terms of progress there's a lot of log entries without a lot of results. So in this post I'll focus on the highlights and talk a little about how the progress so far fits into the overall project. Along the way I'll mention some of the key objectives of the project and outline the major stages as some of this has come into focus since my original post. Around the beginning of this month I started to put together the hardware that will make-up what I'm referring to as **Raiden Mark I**. I chose this configuration for a number of reasons but the overall goal is to build a supercomputing cluster (albeit a small one) which resembles the typical Linux supercomputer. This will be used to establish a baseline for performance, power consumption, cost, etc. ![IMAG0017_1](/wp/2017/08/imag0017_1.jpg)

## Original Raiden Mark I Hardware Configuration

Qty Device Processor Clock RAM Storage

8
HP DL360 G5
Dual-core Xeon
3Ghz
8GB
30GB

1
Cisco Catalyst 3500 XL
N/A
N/A
N/A
N/A
The other rationale for this configuration is that it's what I have on-hand. The most obvious bottleneck is the fast-ethernet switch, which could be cheaply replaced with a gigabit one however I saw no point in buying more hardware until I could at least get the system operational with what I already have. The one downside to this is that I had to figure out how to partition the switch to isolate the cluster traffic, but after digging through some dusty PDF's that was all set. I found some very [nice IBM tutorials](https://www.ibm.com/developerworks/linux/library/l-cluster1/) on building simple Linux clusters which in turn led me to [Rocks Linux distribution](http://www.rocksclusters.org/). Rocks takes care of a lot of the grunt work common to most Linux-based clusters and while it took a little experimentation to cover some gaps in the documentation, the end result was a much faster and more consistent setup than I could have done by hand. I was hoping Rocks would get the system to a point where I could run [LINPACK](https://en.wikipedia.org/wiki/LINPACK) and start measuring the systems performance (starting with just one front-end and one compute node) but some of the dependencies were not where I expected to find them, and after an evening of attempting to build them from source (resulting in my first parallel segfaults), I set the system aside for awhile to work on other projects. When I had time to come back to it, I decided to start over and learn how to write my own parallel programs to test the system. Once I knew more about building and running things on the cluster, I'd give LINPACK another try. This was more successful. In fact, in just a couple hours I was writing my first [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) programs and watching them run across the cluster. With this success under my belt I was ready to take a swing at adding more nodes to the system. I had picked up a few more bits of information since the last time I installed Rocks and this time around figured out I didn't even need the installation CDs to add more processing nodes. All I had to do was re-arrange the boot order a little and blow-away any existing bootable installation on the storage array and like magic the new nodes booted from the network! This moved things along double-quick until everything went dark. Literally, the lights went out. Around the time I booted the fifth node in the cluster a circuit breaker tripped and everything stopped. On one hand this shouldn't have been a surprise, I had the cluster on the same circuit as a bunch of other equipment in the lab and clearly 8 servers would be capable of overwhelming a 15 amp circuit. On the other hand I was kind of surprised that the system would pull that much power when it was for the most part idling, but in any event it was clear that the cluster needed its own circuit. So I pulled a heavy-duty extension over to an an outlet on its own breaker and tried again. Things went well, in fact faster than the first time. I wasn't sure what state things were in due to the power interruption so I rebuilt all the compute nodes from scratch. I was nearing the end when everything got quiet again, another power fault but this time it was only the cluster that went dark. This time it was one of the power strips I was using to feed the cluster. Again this probably could have been predicted but at this point I'd had my fill and decided to call it a night. However as I ascended the stairs I heard the roar of all eight nodes firing up simultaneously. Apparently the breaker that tripped was the kind that resets itself and I knew that this would become a loop of reset-trip-reset if I didn't interrupt it manual so I ran back down to the lab and manually switched-off all but the front-end node. Later that night I ran some numbers and realized that the amount of work involved in getting enough power to the cluster to run it (with some extra headroom for safety) would probably outweigh any benefits of having eight nodes in the system. There's no specific reason I wanted to start with eight nodes other than that is what I have and it seemed more interesting than something smaller. However, in light of the power distribution problems (as well as other associated costs), I've decided to scale the system back to five nodes (one front-end and four compute). This will let me run the compute nodes on a single 15 amp circuit and move the front-end node (which is harder to replace if I corrupt it) to a separate circuit. This also reduces the hardware cost for the next phase of the experiment, "Mark II" (more on that in a future post). For now it's back to the lab to once again setup compute nodes (hopefully without another blackout). After a little rewiring the front-end node came up without issue, so I started bringing up the four selected compute nodes slowly, one-by-one. ![ganglia_no_compute_nodes](/wp/2017/08/ganglia_no_compute_nodes.png) Unfortunately I ran into trouble with the first compute node. It wasn't showing up in Ganglia (the cluster monitoring system) and when I attached a monitor there was nothing but a black screen.  Looks like round three deleting and re-installing the compute nodes. But then, after I cold-booted the node it started-up and launched into setup.  After about 15 minutes it rebooted and showed up in Ganglia! ![ganglia_1_compute_node](/wp/2017/08/ganglia_1_compute_node.png) The same process repeated for each node.  I'm not sure if setup was just incomplete on the compute nodes when the power went out, or if something else toasted them but this "self-healing" behavior is pretty handy and another great feature of the Rocks distribution.  Within an hour, the supercomputer should be fully operational... So close! Another power fault.  This time I can only suspect the power strip I'm using for the four servers.  I would assume it can handle 15A, but looking at it I can't see any indication as to it's capacity.  After a few minutes it reset itself and all four servers roared to life.
---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden
link: http://jjg.2soc.net/2017/08/08/raiden/
author: jgullickson
description: 
post_id: 4363
date: 2017-08-08 08:10:22
created_gmt: 2017-08-08 13:10:22
comment_status: open
post_name: raiden
status: publish
post_type: post
---

# Raiden

_Experimental Linux Supercomputer_ In the past I've built a number of Linux-based cluster supercomputers primarily for my own edification and entertainment. I've been able to apply some of what I've learned from this to my professional work, but I've yet to have a job that demands a large, general-purpose supercomputer. However, in the last five years or so there have been a number of applications which benefit from high-performance computing architectures, and there has also been a resurgence in interest of classical HPC applications as well. This is exciting for me because it creates an opportunity to apply more of my supercomputing experience to my professional work. That possibility, along with a few secondary stimulus gave me the motivation to build another cluster and brush-up on my HPC skills. I also happen to have some idle hardware laying around that I can put to use in a project like this. Once I started learning about the state-of-the-art in high-performance Linux clusters I came up with another motivation for building this system. During a conversation about building a similar system out of Raspberry Pi's, I was asked if there were any practical applications for such a computer beyond education. My immediate answer was "probably not", since there are inherent limitations in the Raspberry Pi architecture (notably the speed of the network interface) that would prevent it from challenging systems made from more formidable components. Furthermore, the number of nodes required to build a supercomputer capable of doing the kind of work considered "practical" would be a lot larger than most people in the Raspberry Pi audience would be willing to construct. But then I thought about it some more and realized that I really don't know how the performance of a four-core ARM chip like that in the Raspberry Pi 3 compares to something typically used to build a large cluster such as the Xeon-based servers I'm using for Raiden. So aside from gaining knowledge and experience in building & operating a supercomputer, I'm going to use this project to do some direct comparisons between traditional server-based clusters and less-expensive, lower-power-consumption SBC-based designs. The methodology for these comparisons will be somewhat informal but I think the findings will be useful in determining how to design a lower-cost, lower power consumption supercomputer architecture. This new architecture could be explored and scaled-up to potentially achieve the performance necessary to take-on practical workloads. I believe that the availability of a supercomputer like this will become valuable as the number of high-performance computing applications increase. As the demand for these applications grow, the performance limits of cloud-based clusters will become more apparent and the need for dedicated supercomputer hardware will increase. By developing an architecture which reduces both purchase and operating cost (as well as reducing harm to privacy and the environment), the results of this experimental system may be in a unique position to serve this ever-increasing audience.
---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden -> RAIN
link: http://jjg.2soc.net/2018/02/16/raiden-rain/
author: jgullickson
description: 
post_id: 5085
date: 2018-02-16 10:00:53
created_gmt: 2018-02-16 16:00:53
comment_status: open
post_name: raiden-rain
status: publish
post_type: post
---

# Raiden -> RAIN

I've decided to rename the Raiden project to RAIN, or Redundant Array of Integrated Nodes1. The primary motivation behind this is that I found out there's already [another supercomputer named Raiden](https://www.top500.org/system/179088), and it's on the [GREEN 500 list](https://www.top500.org/green500/lists/2017/11/) (which I hope to one day join). Since they made the list first it feels like bad form to steal their name, so it's time for a change. As fond of the name as I am, it was chosen rather arbitrarily so I'm OK with giving it up. I started thinking about this change a few months back and settled on RAIN a week or two ago. Names are always hard, but based on some feedback from fans of the project I'm satisfied with RAIN. In the following weeks I'll be updating references to the project with the new name as well as renaming repositories, etc. _1RAIN is also something of a pun, as it's what you get when a cloud falls down..._
---
tags:
  - gullicksonlaboratories
  - rain
title: Raiden Roadmap (10/2017)
link: http://jjg.2soc.net/2017/10/28/raiden-roadmap-102017/
author: jgullickson
description: 
post_id: 4943
date: 2017-10-28 10:00:40
created_gmt: 2017-10-28 15:00:40
comment_status: open
post_name: raiden-roadmap-102017
status: publish
post_type: post
---

# Raiden Roadmap (10/2017)

I've written a little bit about the overall goals of the Raiden project [in the past](https://jjg.2soc.net/2017/08/08/raiden/). Now that I've spent more time thinking about it (and especially because I've spent more time _working_ on it) I have a more crystallized vision of what lies ahead. 

## Mark I

The purpose of Raiden Mark I was to construct and measure a traditional commodity supercomputer. The goal was to learn what goes into building, operating and programming one of these systems and measure the baseline performance of a system scaled similarly to what is planned for Mark II (to allow for a more apples-to-apples comparison between architectures down the road). Most of what I set out to do with Mark I is now complete. Many things did not go as planned and I learned a lot about what the practical challenges are to building & operating this class of supercomputer. Admittedly the results of my performance tests didn't measure-up to my expectations and I would like to continue tuning the machine to see if more potential is there, but for the purpose of establishing a point-of-comparison what has been accomplished is sufficient. 

## Mark II

In simplest terms Mark II is designed to replicate Mark I using commodity ARM components. Mark II will use the same interconnect (gigabit Ethernet) and physical topology (an array of stand-alone computers running a complete operating system, etc.) but in place of Intel-based server hardware, Mark II will use inexpensive ARM-based single-board computers (SBC's). The goal of building Mark II is to understand the performance difference between large, expensive, loud and power-hungry Intel-based server hardware and small, inexpensive and low-power ARM computers. Once this difference is understood, a test will be conducted to see if Mark II's hardware can be scaled to match the performance of Mark I without diminishing Mark II's advantages (lower cost, lower power, smaller size, etc.). A secondary goal of Mark II is to collect and potentially develop the software components necessary to construct a usable ARM-based supercomputer. Minimally, this will result in a "recipe" which uses existing operating systems, system modules and development tools to produce a software package that can be deployed and maintained on a Mark II-compatible system without specialized HPC knowledge and minimal ongoing maintenance. Maximally it may include new operating systems and components along with development tools which make developing high-performance computing applications more accessible to a wider range of programmers. Most of the components for Mark II have been selected and barring any new developments (or component end-of-lives), Mark II hardware will consist of the following components: 

  * 8x PINE A64 single-board computers ([dual-core, 1GB](https://www.pine64.org/?product=pine-a64-board-1gb), [quad-core 2GB](https://www.pine64.org/?product=pine-a64-board-2gb) preferred but would increase build cost by a third and supply is limited)
  * 1x [WiFi interface](https://www.pine64.org/?product=wifi-802-11bgn-bluetooth-4-0-module) for front-end node (Ethernet preferred but no dual-ethernet available on the PINE boards)
  * 8x SD cards (whatever size has the best price-point at time of purchase)
  * 1x commodity 8-port gigabit Ethernet switch
  * 8x gigabit Ethernet cables
  * 1x power supply (single device, 5/12VDC)
  * 1x cabinet
  * Misc. wiring, connectors, switches and indicator lamps

## Mark III

There are three major differences between Mark III and previous iterations of Raiden: 

  * ARM+FPGA
  * Interconnect
  * Custom electronics
The first big change is the addition of FPGA to support dynamic, software-defined-hardware acceleration. My current direction with this is to use SOC processors which combine ARM cores with some FPGA fabric on the same chip (something along the lines of [Zynq](https://www.xilinx.com/products/silicon-devices/soc/zynq-7000.html) by Xilinx). The second change is moving away from an Ethernet-based interconnect. Ethernet has a number of advantages which make it a natural choice for small and low-cost systems. Gigabit Ethernet switches are inexpensive and Gigabit Ethernet interfaces are available on several SBC devices. However using Ethernet for larger clusters has significant limitations. Clusters larger than 8 nodes require rack-mount switches which limit the minimum size of the machine. While Gigabit Ethernet interfaces are fairly common, the faster forms are less common and considerably more expensive. Finally, while Ethernet is entirely suitable for moving files from place to place, it doesn't provide any functionality found in interconnects designed for supercomputing applications. Exactly what will be used in place of Ethernet is not yet set in stone, but I'm currently leaning toward [InfiniBand](https://en.wikipedia.org/wiki/InfiniBand) (perhaps implemented in some of the FPGA fabric discussed earlier). Finally, Mark III is set apart from previous systems because it will require the design of custom electronics to combine the features above. Exactly what form this will take will decide on final selections of processors and interconnect, but it will likely require custom circuit board design and fabrication. This will be the first iteration that isn't built from off-the-shelf components, but it may still incorporate some of these components if they are available (for example, one implementation might be a backplane for off-the-shelf SBCs containing ARM+FPGA SOCs, if such a board is available). 

## Timeline

As each generation builds on the lessons learned from the previous one, so it's difficult to estimate the functionality and chronological timeline for the completion of each iteration. For example, the hardware and O/S for Mark I came together much faster than expected while the _software_ (specifically the HPL benchmark) took far longer to get working. I expect similar unexpected surges and delays in building future generations of the machine. I would like to construct Mark II this winter and retire Mark I before the end of 2017. Most of the design work is complete so this will depend primarily on whether or not I can pull-together the time and funding to carry-out the build. My preference would be to keep Mark I operational until Mark II has met (or exceeded) the performance of Mark I, but various constraints may rule that out. Once Mark II is operational I'll begin formal work on Mark III. At this point I'll make my initial estimates as to when I think Mark III will being to take shape and when it might be operational. The amount of knowledge and effort that will go into building Mark III will be exponentially greater than what will be needed for Mark II, and from the perspective of today it appears to be a daunting task, but the same was true of Mark II from a pre-Mark I vantage point so I assume that things will look a lot more realistic once I have Mark II under my belt. Along the way I'm hoping to garner interest from others in this work. I don't expect a project like this to raise a large or mainstream audience, but having a few other people who are interested in seeing it succeed (especially people who would like to put it to work) makes it much easier to justify the work and frankly makes it more excited to reach each milestone.
---
tags:
  - gullicksonlaboratories
  - rain
title: RAIN and WebAssembly
link: http://jjg.2soc.net/2018/02/19/rain-and-webassembly/
author: jgullickson
description: 
post_id: 5092
date: 2018-02-19 10:00:37
created_gmt: 2018-02-19 16:00:37
comment_status: open
post_name: rain-and-webassembly
status: publish
post_type: post
---

# RAIN and WebAssembly

While reading a recent [Mozilla blog post](https://hacks.mozilla.org/2018/01/making-webassembly-even-faster-firefoxs-new-streaming-and-tiering-compiler/) it occurred to me that several of the challenges described in the post overlap with common challenges faced by high-performance computing clusters. Specifically the problem of granularity and distributing processing load across multiple processing units. I've largely ignored [WebAssembly](https://en.wikipedia.org/wiki/WebAssembly) for a number of reasons but as it turns out, some of my assumptions were incorrect. After reading this Mozilla post, I spent some time learning about WebAssembly, how it's structured and how it's written. As it turns out there are a number of reasons why it might be of use in high-performance computing. 

## Granularity

Breaking a program down into the right size is one of the most constant (and difficult) challenges to writing fast, efficient software for supercomputers. Too big and the program can't utilize all available processing resources; too small and the overhead of communicating between the pieces. Apart from choosing the right size it's often harder for a programmer to decide _how_ a program should be broken-apart. The holy grail of course is to let the computer make these decisions for you, and attempts at this have enjoyed varying degrees of success. However in many cases this is a task left to the programmer and as such excludes many programmers from creating highly-parallel programs (due to the specialized knowledge and experience needed to develop these skills). Mozilla's WebAssembly compiler takes a swing at this goal in order to execute browser code across multiple cpu cores. This happens automatically and as I understand it (I still have more to learn) does so at the _function_ level. This is a clever optimization, and it happens automatically without any direct input from the programmer. Of course optimizing the utilization of cores in a single processor is a different matter from optimizing them across many nodes in a large parallel machine (which undoubtedly has a slower interconnect and no real shared memory) but there may be some lessons here on how to build a runtime/compiler that makes writing efficient parallel programs accessible to programmers with experience in more traditional computing environments. 

## Code + Data

WebAssembly defines a "binary" format that bundles both code and data. Keeping data close to where it will be processed is a cornerstone of high-performance clusters and there may be some potential to leverage this aspect of WebAssembly's "container" format to these ends. 

## Streaming compilation and iterative optimization

Another clever optimization presented in the post is the ability to begin compiling the assembly code before it's completely transmitted across the network. Here again we have a traditional cluster bottleneck (the interconnect) in common with web applications. It might seem like compiling this late in the process might not be an issue for high-performance applications (compilation happening long before runtime) however if we can optimize this step it opens some interesting doors for heterogeneous systems (for example, delivering the same assembly to CPU/GPU/etc. nodes and compiling a hardware-specific binary at runtime). Along with streaming compilation a non-blocking, multi-pass compilation is performed allowing the browser to start running a faster to compile (but less optimized) binary while off-loading optimization to a second thread and switching to the optimized version as soon as its ready. It's not hard to imagine how this multi-layered approach might apply to high-performance computing applications as well. 

## When worlds collide

I haven't invested enough time yet to say whether or not any of these examples warrant the potential trade-offs of using something like a web development programming environment for high-performance computing, but I think it warrants serious consideration. Aside from the potentially "hard" benefits described above, doing so might help bridge skills gap between "mainstream" developers and specialists whose skills and experience allow them to write efficient supercomputer applications. This not only increases the pool of talent available to fill the existing need for high-performance application developers, but creates an environment where programmers with new and interesting ideas can utilize supercomputers to realize new and innovative applications.
---
tags:
  - gullicksonlaboratories
title: Recent Advances in Machine Intelligence
link: http://jjg.2soc.net/2017/04/14/recent-advances-in-machine-intelligence/
author: jgullickson
description: 
post_id: 1692
date: 2017-04-14 09:26:44
created_gmt: 2017-04-14 14:26:44
comment_status: open
post_name: recent-advances-in-machine-intelligence
status: publish
post_type: post
---

# Recent Advances in Machine Intelligence

There's a lot of discussion recently about the increasing number of advances in Artificial Intelligence and Machine Learning. Based solely on the number of products and projects that are yielding results, it would appear that progress in these fields is accelerating.  Pursuit of Artificial Intelligence has been a staple of computer science since the beginning, so I wondered why we are suddenly seeing so much progress? I've studied AI since childhood and when I look at what is considered cutting-edge today, I don't see a lot of new ideas. There are small-grained improvements, but for the most part today's Machine Learning systems are incremental improvements on classical AI techniques.  So, innovation in terms of a radical new approach to machine learning doesn't seem to explain it. Another explanation is that the computing hardware we have available now is finally powerful enough to make the existing techniques practical. While the amount of computing power available to the average person has grown, I would argue that the _peak_ power available in the world has changed much less. Even though your personal computer is many orders of magnitude faster than the personal computers of the 1980's and 1990's, big corporations and government agencies had access to computers at that time with performance that, when applied to specialized tasks such as AI, could rival (and in some ways, exceed) the computing power we apply to these problems today. So how do we explain what appears to be a sudden increase in progress toward computers that think like humans? I have a theory: 

## Machines are not suddenly becoming more _human_; humans are suddenly becoming more like _machines_.

Consider this: When you are in a room with another person, you are communicating with them even before you begin to speak. Your senses gather data about the other person when they are across the room, and your mind begins to retrieve memories you may have of this person (or alternatively, begins to form an internal model of who this person is). When you finally begin to exchange words, you are able to respond verbally based on this model even before you have been formally introduced. It's very challenging to provide this amount of sensual input to a machine.  The most acute example might be the sense of smell. As humans, smell has a huge impact on how we respond to our environment, and smell has traditionally been one of the hardest senses to create machine interfaces for. If you were interacting with a machine in this environment, it would become immediately obvious; there would be almost zero chance that you would mistake it for another human, no matter how clever its code or how powerful its processor. However, since the creation of the written word we have been reducing the "fidelity" of human interactions, and in the case of writing, almost all senses are removed from the communication process. While this goes back millenia, only in the last few decades has this become the dominate form of communication for most humans. In these reduced-fidelity interactions, it becomes easier for machines to appear more like us. By reducing the majority of our interactions to these environments, we level the playing field between people and machines. The result of this is that by framing valuable human interaction within the limited fidelity of written communication, we make it more possible for machines to generate the same value. This is similar to how independent media producers can now create content on-par with mass media companies; the mass-media has reduced the quality of its content (as a cost-reduction measure) to a point where it has no quality advantage over the less-well-funded indies. Once there is a value proposition, the commercial world becomes interested. This explains the sudden interest from capitalists in machine intelligence. This influx of capital results in incremental advances in the technology that appear to be leaps if only because the applications are less abstract (the quest for thinking machines) and instead more tangible (the extraction of value from humans). As long as people continue to interact with one-another in the diverse ways we are equipped to, there is no reason to fear being replaced or overtaken by machines. However, there is a distinct possibility that we may simply assimilate ourselves into "machine culture", and quietly be made obsolete by things that more naturally occupy that culture.  

## Afterward

In the late 1980's it became apparent to me that what was holding-back progress in AI was that all of the work I was familiar with was constrained by the limitations of _how_ machines interacted with the outside world (in almost all cases this was via written communications). I theorized that even what appears to be dead-end approaches might demonstrate more promise if they could be supplied with the ability to interact more directly with the world around them and without the monumental barrier of written language (after all, how long did it take humans to learn to read and write?). Fortunately at the time a solution to this was on the horizon: Virtual Reality. A virtual world would put machines and humans on a level playing ground without the barrier of language, and with a richness of experience much closer to the physical world. I reckoned that this virtual experience, coupled with a means of connecting machines and people across physical space (some kind of global network...) could result in exponential advances in artificial intelligence, and yield applications which could benefit humanity in diverse and exciting ways. Unfortunately, when that network became available to the general public it was via a much lower-fidelity, 2-dimensional interface known as the World Wide Web. This was much easier to capitalize on than Virtual Reality, and well, you know the rest of the story...---
tags:
  - gullicksonlaboratories
title: Reelbot
link: http://jjg.2soc.net/2017/04/18/reelbot/
author: jgullickson
description: 
post_id: 1871
date: 2017-04-18 08:12:32
created_gmt: 2017-04-18 13:12:32
comment_status: open
post_name: reelbot
status: publish
post_type: post
---

# Reelbot

For a few years I've wanted to build a robot lawnmower. Not for any practical reason (our lawn is not big enough to demand it), mostly just because I like robots. I've seen a few robot lawnmowers, but for the most part they are large and kind of like remote-control riding mowers. What I have in mind is something more compact. Since about a year after we owned our house we've used a "reel" mower. We started out with a typical gas mower, but the yard was is small that it was overkill and it took up a lot of space in the garage. It also requires fuel, oil and a lot more maintenance than the reel mower does. Plus, reel mowers just look cooler, all those spinning parts and blades and such, anyway... ![2017-04-16 09-43-19 0298](/wp/2017/04/2017-04-16-09-43-19-0298.jpg) This led me to design a robot lawnmower around the reel mower  instead of the "deck" or riding mower designs more common today. The design is rather simple: take an existing reel mower, remove the handle and add two motors (one to drive each wheel). This provides both drive and steering, tank-style. Having taken-apart our mowers several times for cleaning, sharpening, etc. I noticed that the inside the wheel is a gear which drives a pinion gear that in turn spins the reel. This makes it very easy to interface a drive motor to the wheel by simply creating another pinion gear and attach it to an electric motor. Looking at this particular mower (a Scotts Classic 20SG), there appears to be enough room to mount a suitable motor to a flange which protects the gear-side of the wheel from the outside world, so this approach is at least _theoretically_ possible. ![2017-04-16 09-45-39 0303](/wp/2017/04/2017-04-16-09-45-39-0303.jpg) The first job is to model a suitable drive gear. For this I turn to [OpenSCAD](http://www.openscad.org/). There is a [gear library](http://www.thingiverse.com/thing:5505) which I've used before and works very well for creating bespoke gears. ![Screenshot_20170418_080849](/wp/2017/04/screenshot_20170418_080849.png) A better engineer could probably do this in one take, but for me an iterative approach is preferred. First, I'll try to model a copy of the original gear. Then, when I know I can make a working copy, I can tweak it to match the motor/gear ratio/etc. [caption id="attachment_1891" align="aligncenter" width="3264"]![2017-04-16 11-03-21 0316](/wp/2017/04/2017-04-16-11-03-21-0316.jpg) Not quite...[/caption] The downside to the iterative approach is that it requires running a print off for each variation, which takes time. I don't mind this though because I have other problems to solve while I'm waiting for the printer. [caption id="attachment_1885" align="aligncenter" width="2448"]![2017-04-16 10-32-19 0311](/wp/2017/04/2017-04-16-10-32-19-0311.jpg) Once more around the horn...[/caption] Once a suitable drive gear has been created ,a motor needs to be selected. There's a lot of things to consider when selecting a motor for a robot, but my starting point is almost always dependent on what I have on-hand. Today what comes to mind are a pair of RS-540 motors that currently live inside another robot ( [Sux0rz](https://github.com/jjg/sux0rz) ). [caption id="attachment_1888" align="aligncenter" width="3264"]![2017-04-16 13-42-49 0320](/wp/2017/04/2017-04-16-13-42-49-0320.jpg) Much better![/caption] It only took two tries to get a gear that looks like it will do the job.  The next task is to render a version that can mate with the selected motor, and decide on a gear ratio. I'm tempted to cut the tooth-count of the drive gear down as low as possible to maximize the torque available.  These motors seem to prefer high-speed operation, and they will be moving a lot of metal so I'm willing to sacrifice top speed to ensure there's enough power to drive all the moving parts. Along with the practical limits on how few teeth the gear can have (and still function properly), there is another mechanical limit based on the position of the motor on the flange that covers the gear side of the wheel.  The flange has a bevel (or flare?), and the motor will need to be mounted in such a way that it mounts against the flat, un-flared part of this flange.  This puts another limit on how small the drive gear can be.  Some additional experimentation may be in order to determine the balance between these constraints. ![2017-04-16 13-46-05 0321](/wp/2017/04/2017-04-16-13-46-05-0321.jpg) In the next installment I'll be iterating on a gear design that meets these criteria (along with mating to the selected motors) and exploring power & control systems. As always, design files and code are open-source and can be found on Github here: <https://github.com/jjg/reelbot>.---
tags:
  - gullicksonlaboratories
title: Remember MS-DOS?
link: http://jjg.2soc.net/2017/10/31/remember-ms-dos/
author: jgullickson
description: 
post_id: 4947
date: 2017-10-31 23:23:32
created_gmt: 2017-11-01 04:23:32
comment_status: open
post_name: remember-ms-dos
status: publish
post_type: post
---

# Remember MS-DOS?

Over the weekend I rebuilt my [cheap laptop](https://www.pine64.org/?page_id=3707) and installed a version of Linux with no graphical interface. The machine boots-up to a text-mode login prompt and once logged-in, you have a big black screen and a command prompt at the bottom. Maybe this doesn't sound that exciting, but it's been at least 20 years since I've tried to work on a computer with _no_ graphical interface. Sure, I use terminals every day, but there's something very different about having an array of terminals in little windows on a screen where you can instantly call-up say, a web browser, or a graphical file manager or some other visually-oriented application when you forget the command you need (or more likely, the arguments for the command). One thing that you would expect to be especially crippling is not having access to The Web, but it turns out that's not actually the case. Recalling my days of setting up Linux-based DNS servers (back when this was strictly a cost-savings measure) I remembered the `<a href="https://en.m.wikipedia.org/wiki/Lynx_(web_browser)">lynx</a>` text-mode web browser. This was good enough to get the job done back in 1998, but you wouldn't expect it to be of any use in the post Web 2.0 world. Turns out you would be wrong. Firing up lynx and pointing it at <https://duckduckgo.com>, I was quickly searching (and finding) commands and arguments to my hearts content. lynx not only worked, it worked _well_, at least within the narrow use-case for which I required it. One of the first things you'll notice using a computer this way is how _fast_ it is. We've really gotten used to slow computers, and as hardware has stepped-up to address performance issues, software has grown fatter and flattened each additional Mhz to the point where often, a 2018 laptop running a modern GUI doesn't feel much snappier than a 1988 Macintosh... ...but strip-away that GUI and everything happens _instantly_. Sure network latency might cause a noticable delay between tapping the right-arrow key in lynx and seeing the text of the requested web page fill the screen but even this is orders of magnitude faster than watching the same page load in a typical web browser. Anything you do _locally_ happens so quickly that you may question whether or not you've initiated the command at all. Even running console commands and applications is measurably slower when they are running inside a terminal window vs. running directly on the screen like this. Each character emitted by the program goes through miles of code before it becomes illuminated points on the screen. While some processing does occur in "text mode" (since modern computers no longer have a native text mode in the traditional sense), the responsiveness of console applications running directly in the "framebuffer" is entirely different from running them inside a window under the overhead of a "window manager". It seems impossibly long ago, but well within a lifetime this is how most personal computers were used. Yes, graphical user interfaces existed as far back as the dawn of the 1980's but text-mode applications like Wordperfect were still being taught at the technical college I attended in the early 1990's and one reason why is that the graphical word processors of the day were unable to keep-up with the typing speed of these students. While this area of performance may have improved since then (although my phone frequently gets so far behind my typing that it looses charachters) I don't think that Windows/MacOS/KDE/etc. feel much faster now than Windows NT felt in 1994. Now that we live in an era where power consumption is as much (more?) of a hinderance than most other aspects of computers, imagine the kind of reduction of power demand that would accompany switching work which can be done with a text-mode interface over to the very low-power (but still incredibly fast compared to computers of the 1990's) [single-board computers](https://www.pine64.org/?product=pine-a64-board) and system-on-a-chip hardware we have today? Aside from the environmental impact and financial savings, imagine what kind of runtimes would be possible. A laptop like the one I'm using to write this post (which by the way is way overpowered for the task) could likely run for weeks or more on a single charge if used in this way. A change like this would also make computers more accessible, due to both the lower initial cost and the higher secondary-market value. A text-mode computer and its applications become "obsolete" at a dramatically slower rate, and continue to be useful for much longer, than its graphically-oriented counterparts. It's also worth mentioning that the cost of developing and maintaining text-only software is substantially less expensive than GUI (even web-based) application software Certainly there are applications which are not suitable for a text-only user interface, but if you think about how you use a computer each day, how much of the _value_ generated by that use could be generated without the use of graphics, images, etc. Akin to inexpensive electric cars, enough people could do enough work with text-only systems to make a significant difference in the world. ...all this and they are faster as well

## Comments

**[Jason J. Gullickson](#92 "2017-11-01 09:00:28"):** Thanks for the note @steve. I really appreciate the encouragement. I've gotten used to the fact that the work I have an affinity for may not be something most people can relate to and as such I don't expect a lot of external acknowledgement, but it's nice to know I'm not just "broadcasting into the void" as well :) I'm always surprised at the pieces people connect to. Almost without exception the things I write that strike a chord with someone are the ones I don't expect anyone to get, and the ones that I think will have more widespread appeal go unnoticed. If nothing else it's a testament to the fact that you should do what you like because that was you'll have at least one fan.

**[Jason J. Gullickson](#93 "2017-11-01 09:46:11"):** This conversation has me thinking about the potential for a Linux distribution based around text-only productivity applications. Essentially an open-source equivalent to a 1980's DOS business machine. I know of a lot of terminal-based sysadmin/developer/communication software but I'm less familiar with anything that would fall into the "productivity" label such as word processors, spreadsheets, databases (think FoxPro not MySQL) so I'd have to do a little research to see what's available. Might be a fun thought experiment if nothing else.

**[Jefferson Carpenter](#89 "2017-11-01 00:28:01"):** If there's no GUI, how do you watch Star Wars? Truthfully I haven't done much web browsing via text-only browsers, but from the little I did, one thing I had trouble with was being able to fluently interact with all of the stuff on the page. For instance, search engines spit out multiple, multiple results. How do you jump to a result in the middle or at the end of the results list? Or how do you select and click on a hyperlink in the middle of a Wikipedia page? Web forms are another thing. On a page with a form, how do you select form boxes so you can enter data into them? (In particular the first; from there you can tab from one to the next). (It would be neat if the form element's "name" attribute was displayed next to it so you could type that in to bring it into focus, but that might interfere with the other page content.) Sent from my Firefox web browser

**[Jason J. Gullickson](#91 "2017-11-01 08:50:59"):** @jefferson There's different ways to navigate a webpage in Lynx, but the way I do it right now relies primarily on three keys: down arrow, right arrow and spacebar. The down arrow selects each link (or form field, etc.) in turn beginning with the first one on the page. Running with your search engine example, I would type: lynx https://duckduckgo.com The page loads and the first link on the page is automatically selected. I press the down arrow key once to select the next element (the text entry field), type in my query and press enter. Since this is a text-entry field pressing enter selects the next element which is the search button. With the search button selected, I press right arrow to execute the search (right arrow is equivalent to clicking a button, link, etc.). The search results page is displayed with a list of matching results. I can use the down arrow to move through each link on the page (linking to pages matching the search criteria) or if I don't see what I'm looking for on the first page, I can press the spacebar to advance to the next page. Once I find a result that looks interesting, I select the link using the down arrow and press right arrow to visit the page. If you've used other Linux/Unix tools some of this might already sound familiar. There are many other ways to do the same thing (likely more efficient ones as well), but with these three keys you can do most of what you'd need to do in a web browser. I also remember hearing about a newer text-mode browser which might be better than Lynx, but I can't remember the name off-hand so I haven't tried it yet. I remember hearing that it has image (and perhaps video) support, rendering images as ascii art...

**[Steve](#90 "2017-11-01 02:49:25"):** Dropping a line, more than anything, to let you know that people ARE taking advantage of your wisdom and reading your posts. When it comes to tech stuff, you are orders of magnitude above where I will ever be... but this essay makes sense to me, and made me nostalgic as hell. I may have to attempt to follow your lead.

**[kelbot](#96 "2017-11-07 22:18:44"):** Wordgrinder is one I've heard good things about. http://cowlark.com/wordgrinder/

---
tags:
  - gullicksonlaboratories
title: Repreposterous?
link: http://jjg.2soc.net/2018/10/09/repreposterous/
author: jgullickson
description: 
post_id: 5297
date: 2018-10-09 09:38:29
created_gmt: 2018-10-09 14:38:29
comment_status: open
post_name: repreposterous
status: publish
post_type: post
---

# Repreposterous?

Reading about [Low-Tech Magazine's solar powered website](https://solar.lowtechmagazine.com/2018/09/how-to-build-a-lowtech-website.html) got me thinking about [Preposterous](https://gitlab.com/jgullickson/preposter.us) again, specifically in regard to the low-overhead nature of Preposterous's architecture. When I created Preposterous, I was "scratching a personal itch", but as a side-effect created a modestly-powerful blogging platform with exceptionally low resource consumption and impressive scalability. While reading the about the Low-Tech Magazine site, I started to think about how Preposterous could be used to build a similarly energy-efficient website, as well as tackle some of the limitations LTM is running into with their implementation. For example, at the moment LTM's solar-powered site doesn't implement a comment system (a side-effect of static content). Preposterous had the same limitation (as its content was static as well), but I did come up with an alternative to traditional comment systems that not only worked within these constraints, but also provided a means of automatic moderation. The idea is simple: comments on one user's post are in the form of a post to your own blog. In other words, if you post something, and I want to comment on it, my comment becomes a post on my own blog. The system links the content so it can be displayed inline with the original post (like a typical comment), but it also appears on my blog (linked to the original post for context). The beauty of this design is that it imposes some "personal responsibility" because your comments become featured on your blog; your behavior on the site is displayed in-line with the content you want to share (instead of hidden in a thread somewhere). At the same time, this works within the constraints of Preposterous's existing static-content architecture. Another aspect of Preposterous's design that makes it attractive for the solar-power model is that transient outages (due to lack of sunlight) don't interfere with authoring. Since user's author content in their local email clients, they can create new content whether the server is up or not, and thanks to the store-and-forward architecture of email they can submit this content when the server is off-line and know it will be accepted and posted automatically when the power comes back on. This also makes it easy to distribute content to several servers with almost no additional overhead. This makes it easy to distribute these servers geographically in such a way that it's always sunny over one of the servers, resulting in potentially zero downtime. I've been getting frustrated with Wordpress and feeling-out alternatives that better suit my needs. I didn't have any luck finding exactly what I wanted, and I wasn't excited about building another blog platform for myself (again), but reading about LTM's solar-powered setup has me seriously considering "rebooting" Preposterous and turning it into a hybrid of these ideas.---
tags:
  - gullicksonlaboratories
title: Review - Docooler Pen Mouse
link: http://jjg.2soc.net/2017/03/11/review-docooler-pen-mouse/
author: jgullickson
description: 
post_id: 43
date: 2017-03-11 12:36:46
created_gmt: 2017-03-11 18:36:46
comment_status: open
post_name: review-docooler-pen-mouse
status: publish
post_type: post
---

# Review - Docooler Pen Mouse

I don't remember where but somewhere I heard the term "pen mouse" and I was surprised to see that there were a number of inexpensive ones available. I added the [least expensive one I could find](http://astore.amazon.com/jjg00-20/detail/B00BS67S78) to my Amazon Wishlist and received one a few weeks later from my mother on my birthday. ![2017-03-10 13-27-57 0063](/wp/2017/03/2017-03-10-13-27-57-0063.jpg) This was excellent because it was a completely experimental selection. I had no idea whether or not I would like it so it was unlikely that I would every buy it on my own. This is what I consider the perfect conditions for selecting a gift. There's not a lot to say about the device in terms of setup and use. The mouse requires one AAA battery and comes with a small plastic stand and a USB adapter. Once the adapter is inserted into a USB port, the operating system on every device I tested on immediately recognized the pen as a mouse. 

## The Good

Using the mouse comes naturally to me (and I would assume anyone else whose done any handwriting). It takes a little getting used to to get grip where the mouse buttons fall under the thumb and finger easily, and I imagine the comfort of this can vary depending on the size of your hand and length of your fingers. One thing that's nice is that the position of the pointer on-screen "tracks" fairly accurately with the position of the pen on the table (even if you lift it). This doesn't let you "scoot" the pointer the way you might by picking up and setting-down a mouse, but I found it made it a lot easier to make predictable, precise movements with the pen vs. a standard mouse. It takes a little getting used to, but after about 20 minutes I found myself enjoying the pen mouse more than a standard one, and I imagine for tasks that involved drawing or writing it might be very useful. 

## The Bad

The big downside for me is the lack of a third button, which is a big deal for Linux users. Unlike most "wheel mice", the wheel on the pen can't be depressed to produce a third button click. I also found it difficult to double (or especially) triple-click with the mouse. Depressing the "left" mouse button is done with the forefinger near the point of the pen and tends to disturb the mouse position enough that the second and third clicks don't register in the same location as the first. There is a DPI adjustment button that might help with this but I haven't noticed a significant difference in the different DPI settings myself. My last complaint is that it's a bit awkward switching between the mouse and keyboard with the pen mouse. It has to be returned to its stand if you want to switch to two-hand typing which feels more time consuming that lifting your hand off a standard mouse. This is even more pronounced if you're used to using a trackpad or trackpoint that's built right into the keyboard. Alternatively you can just hold on to the mouse while you're typing, which worked better than I would have guessed (the mouse doesn't go crazy when you hold the pen sideways) but it definitely takes a bite out of your typing speed. 

## The Verdict

While the complaints outnumber the praise above, I have to say I recommend this mouse. When used for mousing-around it's very pleasant to use and if you don't have to switch between mouse and keyboard it's a lot of fun. If it were more expensive I would give it a harder time, but for about $10.00 it provides enough value to be worth a purchase, and I think that as I use it for more appropriate applications (drawing, etc.) I'll like it even more. No third mouse button is a bummer, but the fact that it worked with Linux at all was a pleasant surprise. I don't know how long the battery will last, but so far it's held up and replacements are cheap and available. It also fits in a pocket a lot better than a standard mouse.---
tags:
  - gullicksonlaboratories
title: Seeing the future
link: http://jjg.2soc.net/2017/10/24/seeing-the-future/
author: jgullickson
description: 
post_id: 4938
date: 2017-10-24 23:07:56
created_gmt: 2017-10-25 04:07:56
comment_status: open
post_name: seeing-the-future
status: publish
post_type: post
---

# Seeing the future

Seeing the price of [Bitcoin](https://en.m.wikipedia.org/wiki/Bitcoin) tonight, I’m lamenting how I could be eliminating swaths of debt had I followed my own advice and invested in mining more ‘coin many years ago.  I knew then that Bitcoin would either explode in value or fail completely in a year or two. I’ve always had a knack for seeing the future when it comes to technology.  I don’t think it’s any sort of “market savvy” or other such financial thing, just a sensitivity to quality which happens to often correlate with commercial succes (over a long enough timeline). I saw the same potential in [Tesla Motors](https://www.tesla.com) (the only thing I recommend people invest in other than Bitcoin), and I saw it in NeXT & Objective C (realized when the iPhone came to market).  I saw it in REST-enabled IoT hardware (creating my own implementation in [RESTduino](https://github.com/jjg/restduino) years before it became a standard) and the list goes on. Now I see the same potential in my [Raiden](https://jjg.2soc.net/category/raiden/) project.   I haven’t shared a lot of the details about my long-term vision for the project, and so far it has been focused on establishing a baseline to measure performance of future work against. But I’ve learned so much already and combined with my knowledge of industry trends and decades of high-performance computing experience, I’m uniquely suited to divine a high-quality solution to many current and near-future challenges facing high-performance computing applications. While I feel like I could get more out of the Mark I system, I also feel like I’ve learned enough to move on to Mark II. Any additional time spent on Mark I, while perhaps enjoyable, is probably throw-away optimization work that will teach me little about improving the performance of future generations. Also I’m anxious to get started on some of the new problems to tackle in building Mark II and beyond. I’m not sure how to execute the remaining phases of the project in a way that will allow me to realize the potential it has in a timely fashion or in a way that will ensure it reaches the target audience. On one hand this makes me feel like I should be spending more time on publicizing the project and securing resources so I can dedicate more time to the work. On the other hand I’ve seen so many projects die because their creators got so tied-up chasing those things that they lost sight of the actual work and never got around to building anything. Since funding Mark II is probably something I can do alone, and I can probably fit working on it into my “spare time”, I plan to keep marching toward that goal alone. With any luck I’ll find some interested collaborators along the way who can help me find a way to dedicate more time and resources into the effort when it’s time to begin work on Mark III.---
tags:
  - gullicksonlaboratories
title: Smartgal
link: http://jjg.2soc.net/2017/12/29/smartgal/
author: jgullickson
description: 
post_id: 5052
date: 2017-12-29 12:49:02
created_gmt: 2017-12-29 18:49:02
comment_status: open
post_name: smartgal
status: publish
post_type: post
---

# Smartgal

A recipe for semi-autonomous art galleries. I know a lot of people who like to buy art, and I know a lot of people who make art. Until we implement [basic income](https://en.wikipedia.org/wiki/Basic_income), artists need a way to make a living in order to keep making art, so it seems like a natural fit for artists to sell their work to people who want art to facilitate making more art (and you know, surviving). However few artists want to run a business, and even the ones who do will usually say that making their art into a business impacts the work negatively. On the other hand business people love running businesses, but the non-commodity nature of art isn't an ideal product from a strictly business perspective. Some attempts have been made to use technology to address this problem. Websites like [Etsy](https://www.etsy.com/) remove some of the overhead of operating a business but much of the burden still falls on the artist. Worst yet, the "scalability" of sales platforms like Etsy mean that if an artist's work becomes popular, the work required to manage an Etsy shop quickly consumes any efficiency that was gained through automation. Finally, it's worth noting that for most art there is something important about being able to experience it in person, and the on-line experience while important for sales, leaves a lot to be desired. This is a problem I think about a lot because I know how much being surrounded by art has improved my life. I'm privileged to live with two artists whose copious output occupies most of the free walls in our home and I would love for everyone to be able to enjoy this experience. I would also like to see more artists able to concentrate on their work without the distractions of running a business or relying on other employment. What I propose as a solution is applying technology to the problem (after all, I _am_ a programmer) but with a much more aggressive approach than I've seen tried before. The goal is to collect these ideas into a "cookbook" which could be used by anyone with access to the necessary resources and an interest in facilitating the connection between art creators and art consumers. At the most basic level a "Smartgal" is a physical gallery space which functions as a consignment shop for art. Artists and patrons participate as members of the gallery (perhaps as a [co-op](https://en.wikipedia.org/wiki/Cooperative)), and the gallery collects a small commission from sales to cover overhead. This overhead is where the automation comes in. Artists place their work in any available display space in the gallery. Cameras in the gallery continually scan the space and automatically recognize when new pieces are placed. Each piece is photographed at high-resolution and machine-learning algorithms are used to analyze the work and produce extensive metadata about the piece. This includes not only basic metrics like size, colors, etc. but also analysis of the subject. Facial recognition identifies people or other faces that appear in the work, their number, relative sizes, perhaps deeper analysis (complexion, attire, etc.). The context of the work is identified (landscape or portrait, rural or metropolitan). This analysis is continually improved as better algorithms become available. If the artist has produced additional work this is cross-referenced in the database and analysis of this work is composited to develop a profile of the artists catalog (a similar catalog analysis is performed across all works available in the gallery). The result is a deep queryable body of metadata about each piece of work in the gallery which is generated automatically just by hanging the work on the wall or placing it on a shelf. This metadata, along with the photographs captured are used to list each piece for sale on-line as well as in the gallery itself. The sale price can be specified by the artist or calculated by the gallery and may be fixed or dynamic. Patron members of the gallery can purchase pieces by visiting the gallery in person or on-line. In-person purchases are completed by removing the desired piece from it's place and presenting the patron's membership card as payment (first-time purchasers pay with typical electronic payment and can become members automatically). In addition to paying via card, patron members can opt to passively by allowing the gallery to identify them by sight. This would allow patrons to simply select a piece, take it home and be billed automatically by the fact that they left the gallery with a piece in-hand. Once a piece has been purchased, the artist who created it receives payment and the artist members are notified that the space is available for a new piece. The piece is automatically de-listed from the on-line store as well. Beyond facilitating basic transactions this automation opens the door for some interesting features. For example, patron member who have purchased a piece from an artist can be notified when the artist places a new piece in the gallery. The ML analysis of the pieces can facilitate cross-selling based not only on the artist but shared themes contained within the pieces itself. 360 degree photography can be used to generate a virtual environment which can allow on-line visitors to explore the gallery in a more direct way. Security is obviously not a problem given the amount of data that is gathered continually to support automatic nature of the gallery. This means that it's unnecessary to staff the gallery at all, in fact it could be open 24/7 so long as someone is available to respond in the event of an emergency. This all might sound fantastic and expensive, but all the technology needed to create a gallery such as this is not only available now, but it can be implemented using commodity parts and open-source software. Some experimentation would be required to discover the limits of contemporary technology and find a "sweet spot" between the cost of implementing & operating the gallery and the range of pieces it would work best with. Additionally the mechanics of managing members, artists, art pieces and sales would require some experimentation to determine the ideal configuration for these parameters (as well as discover new and interesting applications for the metadata, etc.). These experiments could be conducted in a small dedicated retail space and in 6-12 months a standard configuration could be established which could then be documented, scaled and repeated. This post just scratches the surface of what's possible and there are many variations on the basic concept that could be explored. If you find this interesting or would like to participate in (or provide resources for) an experimental pilot of such a gallery, [get in touch](http://jjg.2soc.net/contact/).

## Comments

**[Puff-poof-woof (@ToDiaspora)](#104 "2017-12-29 13:08:15"):** One major issue I'll note though is that I wouldn't want said smart gal to collect information about me other than the facts of my purchase. I'd also only contribute to places where the art itself was actually done by a human being, but the business end is done through automation.

---
tags:
  - gullicksonlaboratories
title: Stepbot and runmyrobot.com
link: http://jjg.2soc.net/2017/03/09/stepbot-and-runmyrobot-com/
author: jgullickson
description: 
post_id: 607
date: 2017-03-09 07:40:41
created_gmt: 2017-03-09 13:40:41
comment_status: open
post_name: stepbot-and-runmyrobot-com
status: publish
post_type: post
---

# Stepbot and runmyrobot.com

As mentioned in earlier posts, one of my goals for reviving the Stepbot project was to [get it working with runmyrobot.com](http://jjg.2soc.net/2017/02/25/stepbot-revival/). When my friend Chase introduced me to [runmyrobot.com](http://runmyrobot.com/) I was excited because they provide a platform for allowing anyone to control telepresense robots through a web interface, and they provide source code for the robot firmware that interfaces with the system. Casually looking at the site, it seemed to fill the need I have for providing a control interface and protocol for my telepresence experiments, which could save me some time at this stage and let me focus more on the robot work itself. After running into some [roadblocks](http://jjg.2soc.net/2017/03/03/stepbot-brain-drain/) due to the hardware I selected, I upgraded Stepbot's brain and I was able to get everything working. http://www.youtube.com/watch?v=XK73iqsq3Pc As you can see, it's technically "working", but the results leave much to be desired. I haven't analyzed the results enough to say whether or not the performance of the system has to do with my robot, my re-implementation of the firmware or the platform itself.  Regardless, in its current state, it's not very usable. The control interface I built for Sux0rz had similar problems which, for the most part, boil down to lag.  I assumed that in Sux0rz case this was due to the old-fashioned REST protocol I was using to send control signals and the simplistic video streaming (an old Linux webcam tool).  This arrangement, while also plagued with lag, out-performed  what you see in the above video. The up-side of this experience is that it shows the problems I was having with my home-grown system are not unique, and that there wasn't necessarily something fundamentally wrong with the way I was going about it. Lag (in both directions) is the key problem to solve in developing a usable, general-purpose telepresense platform. This isn't a surprise, but having more than one point-of-reference makes it easier to justify focusing energy in this area, as it is not yet a solved problem.---
tags:
  - gullicksonlaboratories
title: Stepbot - Brain Drain
link: http://jjg.2soc.net/2017/03/03/stepbot-brain-drain/
author: jgullickson
description: 
post_id: 559
date: 2017-03-03 14:53:49
created_gmt: 2017-03-03 20:53:49
comment_status: open
post_name: stepbot-brain-drain
status: publish
post_type: post
---

# Stepbot - Brain Drain

As I've mentioned before one of my ambitions for Stepbot is to make it work with [runmyrobot.com](http://runmyrobot.com). Using code from their [Github repository](https://github.com/runmyrobot/runmyrobot) as a guide, I merged this with [Stepbot's firmware](https://github.com/jjg/stepbot) and after a bit of tweaking, success! https://www.youtube.com/watch?v=oIGh1R1_qiA However, I ran into a bit of a snag. Although control is working well, I had less luck getting the video feed to work. After re-working the sending script a bit it turned out that ffmpeg (or in my case, avconv) needs more horsepower than my old Raspberry Pi Model B can muster. That, coupled with it being a single-core processor results in the whole system being brought to its knees whenever I try to fire-up the video feed. So, it looks like I'll have to perform another brain transplant if I want to make this work. The ideal candidate would probably be a [Raspberry Pi Zero Wireless](https://www.raspberrypi.org/products/pi-zero-wireless/), but if the difficulty of getting a hold of a $5 [Raspberry Pi Zero](http://astore.amazon.com/jjg00-20/detail/B01GEHPI0E) is any indication, that would probably delay the project indefinitely. Instead I'll probably just use the Raspberry Pi 2 from Sux0rz until I can find something more suitable. I might also take a look at using my [Next Thing Co. CHIP](https://nextthing.co/pages/chip) (although I'm not sure it can provide enough power @5VDC to drive the motors...hmm).---
tags:
  - gullicksonlaboratories
title: Stepbot
link: http://jjg.2soc.net/2017/02/23/stepbot/
author: jgullickson
description: 
post_id: 107
date: 2017-02-23 06:03:19
created_gmt: 2017-02-23 12:03:19
comment_status: open
post_name: stepbot
status: publish
post_type: post
---

# Stepbot

![2017-02-17-11-38-11-0003](/wp/2017/02/2017-02-17-11-38-11-0003.jpg) Stepbot is designed to be a simple, general-purpouse robot platform designed around [inexpensive unipolar stepper motors](http://astore.amazon.com/jjg00-20/detail/B01CP18J4A).  The design was inspired by the ["turtle" robots](https://en.wikipedia.org/wiki/Turtle_%28robot%29) that were used used in conjunction with the [LOGO language](https://en.wikipedia.org/wiki/Logo_%28programming_language%29) to teach programming in the 1980's. ![turtle](/wp/2017/02/turtle.jpg) I had forgotten about this project until about a week ago when I was organizing the lab and came across a box labeled "chalkbot" (the first application I had in mind was to attach a piece of chalk to the chassis and make it draw things on the sidewalk).  Based on the [Github repository](https://github.com/jjg/stepbot), I started this project about two years ago. Originally, I intended to use an [Atmega328p breakout board](https://www.tindie.com/products/mwhelectronics/atmega328p-breakout-board/) as the brains of the operation, and develop the firmware using the [Arduino IDE](https://www.arduino.cc/).  The rest of the electronics consisted of the stepper motor drivers that came with the motors (based on the [ULN2003APG IC](http://www.superdroidrobots.com/product_info/ULN2003APG.pdf) ) and a battery power supply.  Revisiting the project now I think something more sophisticated like a Linux-based [SBC](https://en.wikipedia.org/wiki/Single-board_computer) might make the robot more useful, both in terms of expanding the potential applications and making development more approachable via a wider range of tools, languages, etc. Personally, I'm interested in exploring the idea of using the design to produce an inexpensive telepresense robot, perhaps something capable of interfacing with [runmyrobot.com](http://www.runmyrobot.com/). In any event I've decided to revive the project and work on giving it a new brain.  At the moment I'm planning to experiment with driving the motors using a [Raspberry Pi](http://astore.amazon.com/jjg00-20/detail/B01CD5VC92) (because that's the platform of choice for runmyrobot).  But, given the size of a standard Raspberry Pi (and the cost of getting a properly-outfitted Raspberry Pi Zero), I'm probably going to choose something else in the long run, perhaps Next Thing Co.'s CHIP, or some other Open Source Hardware ARM-based Linux SBC.  Regardless of the final choice, it needs to be small, low-power and obtainable.---
tags:
  - gullicksonlaboratories
title: Stepbot - Revival
link: http://jjg.2soc.net/2017/02/25/stepbot-revival/
author: jgullickson
description: 
post_id: 212
date: 2017-02-25 09:39:58
created_gmt: 2017-02-25 15:39:58
comment_status: open
post_name: stepbot-revival
status: publish
post_type: post
---

# Stepbot - Revival

In a [previous post](http://jjg.2soc.net/2017/02/23/stepbot/) I introduced the Stepbot project and discussed the original motivations, ideas and the resulting implementation.  In this post I'll be documenting the process of moving from an Arduino-based control system to using a Raspberry Pi. The first step is preparing a Raspberry Pi to use as the brains of the robot.  I happen to have an older Raspberry Pi Model B that's not in use, and since the computational demands for this robot are pretty low (it was originally designed to use an 8-bit microcontroller after all), this should be more than enough for now. 

## Raspberry Pi Setup

Setting up the Pi amounts to downloading the latest Raspbian image, writing it to an SD card and booting-up the board attached to the network via Ethernet.  Once it's up, ssh into it (I use my router's management tools to find the IP address) and then configure it (`raspi-config`).  Finally, configure the WiFi adapter so we can make the whole thing mobile. Once the operating system is setup there's a few additional pieces of software needed to run the code that will talk to the robot's hardware: [code lang="text"] sudo apt-get update sudo apt-get install -y python3 python3-pip python-dev sudo pip3 install rpi.gpio [/code] 

## Hardware

Make the following connections between the Raspberry Pi's GPIO pins and the ULN2003 board that came with the stepper motors.  Raspberry Pi pin numbers are [BOARD numbers](https://sourceforge.net/p/raspberry-gpio-python/wiki/BasicUsage/), not BCM.  For now we'll just connect one motor: 

Raspberry Pi ULN2003 breakout

2
VCC

6
GND

22
IN1

16
IN2

18
IN3

22
IN4
![2017-02-25-06-49-46-0030](/wp/2017/02/2017-02-25-06-49-46-0030.jpg) I'm not sure if powering the motors via the Pi's 5v supply is a good idea in the long run, but for testing it should be OK. 

## Code

First a simple program to test the connections: [code lang="text"] import RPi.GPIO as GPIO import time GPIO.setmode(GPIO.BOARD) coil_A_1_pin = 24 coil_A_2_pin = 16 coil_B_1_pin = 18 coil_B_2_pin = 22 GPIO.setup(coil_A_1_pin, GPIO.OUT) GPIO.setup(coil_A_2_pin, GPIO.OUT) GPIO.setup(coil_B_1_pin, GPIO.OUT) GPIO.setup(coil_B_2_pin, GPIO.OUT) def forward(delay, steps): for i in range(0, steps): setStep(0,0,1,1) time.sleep(delay) setStep(1,0,0,1) time.sleep(delay) setStep(1,1,0,0) time.sleep(delay) setStep(0,1,1,0) time.sleep(delay) def backwards(delay, steps): for i in range(0, steps): setStep(0,1,1,0) time.sleep(delay) setStep(1,1,0,0) time.sleep(delay) setStep(1,0,0,1) time.sleep(delay) setStep(0,0,1,1) time.sleep(delay) def setStep(w1,w2,w3,w4): GPIO.output(coil_A_1_pin, w1) GPIO.output(coil_A_2_pin, w2) GPIO.output(coil_B_1_pin, w3) GPIO.output(coil_B_2_pin, w4) while True: delay = raw_input(&quot;Delay between steps (ms)?&quot;) steps = raw_input(&quot;How many steps forward?&quot;) forward(int(delay)/1000.0, int(steps)) steps = raw_input(&quot;How many steps backwards?&quot;) backwards(int(delay)/1000.0, int(steps)) [/code] You'll have to experiment a bit to figure out what works for the specific motors you're using. 2ms delay is the lowest that works with the stepper motors I'm using but experiment to see how fast you can make them go. You may also have to experiment with the step "cadence" (the order in which the coils are energized) to find the one that moves your motor forward and backward when expected. With this configuration I was able to get my first Raspberry Pi-controlled moves from Stepbot: https://youtu.be/QwLnC3114JA 

## What's Next

Now that one motor is working the next step is to wire-up the second motor and extend the test software to control them both. With this setup it should be possible to drive the robot around a little, and if I can bundle the Raspberry Pi and battery onto the chassis the whole thing should be completely mobile. At that point what remains is iterating on the chassis to accommodate the Pi + battery (or reducing the size of the Pi + battery, probably some of both), adding sensors (minimally the Raspberry Pi camera module) and improving the software by adding some kind of reasonable user interface; possibly integrating with the [runmyrobot.com](http://www.runmyrobot.com/) code. As always, sourcecode for the software and hardware can be [found on Github](https://github.com/jjg/stepbot). 

## References

  * https://learn.adafruit.com/adafruits-raspberry-pi-lesson-10-stepper-motors/overview
  * http://www.winkleink.com/2013/04/raspberry-pi-unipolar-stepper-motors.html?m=1
  * https://pinout.xyz/#
  * https://www.raspberrypi.org/documentation/usage/gpio/
  * https://sourceforge.net/p/raspberry-gpio-python/wiki/BasicUsage/

## Comments

**[〖ℳ¡₭℮〗 (@mikeputnam)](#2 "2017-02-25 11:21:51"):** Cool!

---
tags:
  - gullicksonlaboratories
title: Stepbot - Untethered!
link: http://jjg.2soc.net/2017/03/01/stepbot-untethered/
author: jgullickson
description: 
post_id: 436
date: 2017-03-01 15:59:12
created_gmt: 2017-03-01 21:59:12
comment_status: open
post_name: stepbot-untethered
status: publish
post_type: post
---

# Stepbot - Untethered!

With a little double-sided tape and a complete disregard for aesthetic beauty (and reasonable electrical insulation practices), Stepbot is able to move around as a self-contained unit! https://www.youtube.com/watch?v=L5ypEzL1I9o Obviously there is work to do in terms of getting the parts to fit inside the shell (or alternatively, enlarge the shell a bit) but the cool thing about reaching this stage is that I can spend some time focusing on the software side of the project, which tends to go faster for me. I've already begun to experiment with better motor control schemes and working on plans for a Python module to hide some of the complexity involved in driving the stepper motors. I'm also going to take another look at the [runmyrobot.com](http://runmyrobot.com/) source and see how hard it would be to get that running on Stepbot as well (once I add a camera of course).---
tags:
  - gullicksonlaboratories
title: Sugar's Journal
link: http://jjg.2soc.net/2017/05/02/sugars-journal/
author: jgullickson
description: 
post_id: 2260
date: 2017-05-02 08:33:11
created_gmt: 2017-05-02 13:33:11
comment_status: open
post_name: sugars-journal
status: publish
post_type: post
---

# Sugar's Journal

One of the coolest features of the [Sugar](https://wiki.sugarlabs.org/go/Sugar_Labs/FAQ) platform (originally the "operating system" that shipped on the [One Laptop Per Child project](http://one.laptop.org/) computer) is the [Journal](http://write.flossmanuals.net/sugar/the-journal/). As the name would imply, Sugar's Journal automatically keeps a log of a user's activity. Since it's an operating system-level component, it only only journals activity in a specific application, but it also creates an record of interactions between applications and other users as well. ![sugar-journal](/wp/2017/05/sugar-journal.png) Before you get paranoid, it's important to know that this record is intended to serve the _user_ as opposed to an _administrator_ (or some other authority). While it can be used, for example, by a teacher to observe and assess activity of a class, its primary purpose is to provide the user with a chronological record of their activity. I think this could be an incredibly useful feature outside of the classroom as well. Aside from the obvious timekeeping and accounting applications, having a time-line of what I've been working on, in what applications and with which other people would be very useful for documenting projects and remembering how to get back to things I've lost. It would also help me understand where I'm spending my time and where it might be most useful to improve my tools, skills or habits. It would be incredibly interesting to be able to view "slices" of Journals from other peoples projects as well. Aside from being a great learning tool for others, the implementation of the Journal induces almost no drag on the user's workflow, which I think would result in a lot more information being shared about projects. I've seen similar software for other platforms but for the most part they are geared toward surveillance and do not prioritize their utility toward the user. Additionally, without operating system-level integration the ability for these tools to do the job is limited. It might be possible to implement something like Sugars Journal in an open-source operating system, but it would be hard to achieve the level of seamless integration that is possible when the Journal is a foundational component of the system design. Regardless of how you feel about the OLPC project, it's worth taking a close look at Sugar. It might seem like just another "children's interface" for a computer but there's a lot of subtle gold in there, and I think there's a lot that could be applied to personal computers in general.---
tags:
  - gullicksonlaboratories
title: Summertime and Lazy Days
link: http://jjg.2soc.net/2017/06/30/summertime-and-lazy-days/
author: jgullickson
description: 
post_id: 3179
date: 2017-06-30 15:26:56
created_gmt: 2017-06-30 20:26:56
comment_status: open
post_name: summertime-and-lazy-days
status: publish
post_type: post
---

# Summertime and Lazy Days

After publishing this week's [FindDay](https://jjg.2soc.net/category/findday/) post I realized that the last three posts on this blog have been FindDay's, which means it's been almost a month since I've written about any of our own projects.  This might give you the impression that not much has been going on around the laboratory. Actually, there's a lot going on, but I haven't had much time to write about any of it.  In fact I've barely found time to get the FindDay posts out.  This is a drag because I'm really looking forward to writing about some of the things we've been working on. A few of the things that are in the backlog: 

  * Utility trailer build
  * [DONOR-1](https://jjg.2soc.net/category/charity-arcade/) update
  * [Pinebook](https://www.pine64.org/?page_id=3707) review & getting started guide
  * [Storyphone](https://github.com/jjg/storyphone) revival
  * Quick & easy little free library build
  * A new 3D printer project
  * ...everything else I can't think of right now
I'm going to try to set-aside some time to get these written-up in the next few weeks.  In the meantime enjoy the weather and happy hacking!---
tags:
  - gullicksonlaboratories
title: Sweating Net Neutrality
link: http://jjg.2soc.net/2017/07/12/sweating-net-neutrality/
author: jgullickson
description: 
post_id: 3713
date: 2017-07-12 11:36:48
created_gmt: 2017-07-12 16:36:48
comment_status: open
post_name: sweating-net-neutrality
status: publish
post_type: post
---

# Sweating Net Neutrality

Just to be clear, **I support net neutrality**. I think it's essential to the function of the Internet, and I think anything else is contrary to the design of the network. That said, I think our time would be better spent engineering a solution to the problem that net neutrality aims to solve, which itself is rooted in the fact that most individuals rely on private ISP's to access the Internet. I could launch into a history lesson about why this is the case, and why it's contrary to how the Internet was designed to work but the fact of the matter is that we're currently stuck with this until we come up with something better that we can implement without the help of the government or existing telecommunications companies who have nothing to gain by solving this problem. Off the top of my head I see three paths forward: 

## Bring back peering

Before the days of the [ISP](https://en.wikipedia.org/wiki/Internet_service_provider), if you wanted to access the Internet you would enter a [peering agreement](https://en.wikipedia.org/wiki/Peering). In simplest terms, this meant that you would attach your network to the Internet with the agreement to allow network traffic from the Internet to flow across your network free of charge. In return you could access the other networks attached to the Internet free of charge. This is all the Internet is: _a network of networks where data can travel freely_, and what makes it _better than anything else which came before it_. Peering requires physical access to an existing Internet-connected network, which can be inconvenient and expensive. This is why ISPs emerged in the first place. ISP's would peer with the Internet, and in turn would sell dial-up access to their network to individuals. At the time (the late 1980's) this was a matter of practicality; the only pervasive network which reached every residence was the telephone network, but things have changed since then. One solution to the net neutrality problem is simply to allow individual networks to peer with the Internet and eliminate ISPs*. We have several network technologies available to us now that could make this possible, so the barrier is more an administrative one than a technical one at this point. I don't know the current state of peering, or what is involved, but I believe this to be the best and most durable solution to the problem. 

## Subvert your ISP

If we're stuck with private corporate ISP's then the only option is to subvert their ability to alter our traffic. The first technology that comes to mind that is capable of this is the Virtual Private Network or [VPN](https://en.wikipedia.org/wiki/Virtual_private_network). VPN's are commonly used to send traffic across the Internet in encrypted form to enable secure, private connections between networks over a public network. They are also used by many consumers as a way to make their traffic appear to be coming from somewhere other than the physical point of origin (useful for getting around country-based restrictions on video services, for example). It's much harder (I'm not going to say impossible) for an ISP to inspect and modify your network traffic if it's running over a VPN that originates inside your home network. If you think of a VPN like a tunnel, the entrance of the tunnel could be your router, and the exit somewhere out on the public Internet, out of the reach of your ISP. If you ISP wants to prioritize your traffic, all they see is the outside of the tunnel and they can't do much about what's inside**. This effectively renders the ISP's attempts to subvert net neutrality impotent. There are downsides to the VPN approach. First it's wasteful because it consumes more bandwidth than direct connections and it uses more energy to encrypt/decrypt network traffic. It also requires that you have systems in place on your network to provide the VPN connection, and another system somewhere on the public Internet to act as the other end of the tunnel. There is also a speed penalty due to delays of encrypting/decrypting traffic which can increase the latency (or "lag") you feel when accessing websites, playing games, using video chat, etc. 

## Build a new Internet

This might be the strongest solution of all, but it's also the most complex. In some ways it's simple, building a network like the Internet is well-covered territory. On the other hand the physical infrastructure is expensive, and without the ability to leverage things like "[eminent domain](https://en.wikipedia.org/wiki/Eminent_domain)", it's likely you'd have to resort to wireless technologies. I think that it would be possible to construct a global network using only wireless links so long as the applications running on the network are designed properly. The only thing holding back these type of applications are legal constraints mostly related to copyright and intellectual property. If these could be set aside, a network with the level of functionality we currently enjoy from the Internet could be constructed with far less expense, utilizing far less power and other natural resources and with far greater levels of reliability. Here again we are not limited by technology or the laws of nature, just the made-up boundaries we've setup for ourselves. 

## What now?

I've deliberately oversimplified the above because at the moment I'm not sure what is the best route forward. I think the VPN approach is the most realistic in the short-term, but it's not without problems and the implementations I've seen are not something that is accessible to everyone. That said there may be a manifestation that could be made user-friendly, which could serve as a stop-gap until of of the more robust, effective and permanent solutions can be implemented. What's clear is that relying on the government or corporations to solve this problem is pointless, and if we want to continue to enjoy the current and potential value of a free and open Internet, we need to take matters into our own hands. 

* * *

*I believe that in the long run, this will be the result if ISP's get their way and net neutrality is eliminated. I'd consider this a solution, but it will mean an extended period of degradation (a decade or more) which means a lot of harm to a lot of people. **Of course, the ISP could just constrict all traffic that looks like a VPN tunnel, but there are ways to avoid that as well.---
tags:
  - gullicksonlaboratories
title: Tiny Project - Lego Treats
link: http://jjg.2soc.net/2017/10/23/tiny-project-lego-treats/
author: jgullickson
description: 
post_id: 4893
date: 2017-10-23 09:51:10
created_gmt: 2017-10-23 14:51:10
comment_status: open
post_name: tiny-project-lego-treats
status: publish
post_type: post
---

# Tiny Project: Lego Treats

Jamie came up with an idea to give-out tiny packages of LEGO in place of candy for Halloween to the kids at her office and for the packaging chose a classic of DIY projects, the [Altoids tin](https://en.wikipedia.org/wiki/Altoids). We happened to have a few of these in the warehouse, and she was able to garner enough additional donations to get good number of tins in usable shape. ![image1](/wp/2017/10/image1.jpeg) My part of the project was creating "baseplates" that matched the size of the tin. We have a surplus of other LEGO parts to donate to the project but we don't have a lot of plates, and the ones we have are not quite the right size to fit the tins properly. Additionally, these parts are surprisingly expensive and would have blown the budget for a project that we were donating everything for. If only there was a way to get a perfectly-sized part that wasn't too expensive... (you know where this is going). Since I first got into 3D printing almost a decade ago, people have been printing LEGO-compatible parts. In the early days these almost never worked because the quality of the printers and the materials meant the parts just were not precise enough. However it had been a long time since my first attempt and I thought it would be worth a shot, so I found a [customizable model](https://www.thingiverse.com/thing:597959) of the plate we wanted and tweaked it to fit nicely inside the Altoids tin. ![IMG_6418](/wp/2017/10/img_6418-e1508771360414.jpg) I printed a test piece and while it didn't turn out perfect, it actually worked! With that test behind us I switched into "mass"-production mode and started churning-out three boards at a time. ![IMG_6419](/wp/2017/10/img_6419-e1508771391474.jpg) The printer ran like this for almost three days straight, almost certainly the longest amount of time it's run nonstop without a breakdown. The quality of the parts varied but almost all of them were usable and we completed the project by test-driving each set to make sure we could build something fun with them. ![IMG_6425](/wp/2017/10/img_6425.jpg) Jamie did most of the work on this project but it was fun to do something both challenging (in terms of keeping the printer working) and beneficial with 3D printing. These are the sort of applications I'd love to see more of from both personal and especially community-operated printers.
---
tags:
  - gullicksonlaboratories
title: Tiny Project - Record Crate
link: http://jjg.2soc.net/2017/07/08/tiny-project-record-crate/
author: jgullickson
description: 
post_id: 3633
date: 2017-07-08 14:24:10
created_gmt: 2017-07-08 19:24:10
comment_status: open
post_name: tiny-project-record-crate
status: publish
post_type: post
---

# Tiny Project: Record Crate

![](/wp/2017/07/img_1056.jpg) _I'm experimenting with a new sort of project post for the many small projects I do.  _ _I don't usually bother to document and share these projects because it's more work to write them up than to do the work itself, but I'm going to try to develop a more_ _concise style of post that might be more suitable for these projects. _ _This record crate is my first attempt_. Crates have been a staple among record collections for ages because they were cheap, durable and ubiquitous.  However when I went looking for one I was shocked to see them priced upwards of $35! ![](/wp/2017/07/img_1057.jpg) Instead of buying a $35 crate, I bought a [$16 Harbor Freight nail gun](https://m.harborfreight.com/18-gauge-2-in-1-air-nailerstapler-68019.html) (you'll need a coupon) and a $3 board. This left me with enough to add another record to the collection. ![](/wp/2017/07/img_1042.jpg) ![](/wp/2017/07/img_1044.jpg) ![](/wp/2017/07/img_1047-2.jpg) ![](/wp/2017/07/img_1049-1.jpg) ![](/wp/2017/07/img_1050-1.jpg) ![](/wp/2017/07/img_1054-1.jpg) ![](/wp/2017/07/img_1056-1.jpg) To be fair, if you want to replicate this project it might cost you a little more.  I had some scrap 1x2" boards I had salvaged from demolishing a boxspring so add another $5 if you don't have some scrap you can use. It needs some finishing but all-told it was about an hour or so of work (most of which was digging out and putting away tools).   I considered some refinements like cutting handles, beveling corners, etc. but I wanted to see what the least amount of time was necessary to make something that could work.  With some sanding and a coat of varnish I think it will do the job nicely.
---
tags:
  - gullicksonlaboratories
title: Why Personal Supercomputers?
link: http://jjg.2soc.net/2017/12/13/why-personal-supercomputers/
author: jgullickson
description: 
post_id: 5038
date: 2017-12-13 08:36:09
created_gmt: 2017-12-13 14:36:09
comment_status: open
post_name: why-personal-supercomputers
status: publish
post_type: post
---

# Why Personal Supercomputers?

I've known for a long time that I was made to build fast computers. This passion for speed covers several areas of my interests but it is in computers where I have the most ability to contribute to creating the fastest machines. However what I've learned in the last few years is that the value of fast computers is limited unless they are accessible to a wide-range of people. This is why making the computers I build for the [Raiden](https://jjg.2soc.net/category/raiden/) project something any programmer could own is a keystone of the work. I've written before about how having a computer of my very own at a young age had a fundamental effect on my life. The reason this was even possible was due to the personal computer revolution which began not with companies like Apple, Commodore, TI, etc. but with curious and tenacious hackers building machines in their basements and garages. They saw the potential in the overgrown calculator chips (which we call "microprocessors") that the contemporary computer industry regarded as toys and they cobbled together machines out of the materials they had available. They didn't do this because they had a grand vision of how these "microcomputers" would eventually push the industry leaders out of the market and herald unimaginable new applications, they did it because it was cool, and the rest flowed naturally. I want Raiden to follow a similar path. I'm not building Raiden for a specific application, I don't have something particular in mind as I go along. Personally, there are a number of things I want a computer like this for myself, but I want to make sure I'm designing the system (both the hardware and the software) in a way that is open and flexible, making it something others can hack into whatever form suits their yet-to-be-imagined applications. I want Raiden to be something you can _own_, but I also want it to be something that can scale beyond the kind of power that would be practical to own yourself. This dynamic scalability is a promise "The Cloud" has made but for many reasons has failed to provide, and of course the cloud has no provisions for actual ownership. How I plan to address this seeming paradox is to make collaboration a core component of Raiden's operating system. The scale and cost of an individual Raiden computer is designed to be something almost any programmer can afford, and its built-in participation in a global network of Raiden computers is what allows an individual programmer to scale their application far beyond the capability of their own hardware. Participation in this network is not mandatory, however it will be rewarded by providing network access "tokens" to owners who allow their systems unused cycles to become part of the global pool. This means that when you're not putting your Raiden computer to work, you can be earning credit on the global network to use when your own programs are ready to do more work than your personal Raiden system can handle. Since this is intrinsic to the operating system Raiden's development tools can tell you when you've reached this point and accurately estimate how much network credit a given job will consume (how this works and why it is critical is something I'll save for a future post). In addition to earning credit through lending unused computing power, Raiden may also generate credit when you run your own jobs on your own system. The purpose of this is to encourage programmers to invest time learning to use the system and explore ways to maximize the potential of the hardware. I believe this balance of both unfettered access to one's own personal supercomputer and uncomplicated access to a globally distributed, federated cluster allows Raiden to provide the advantages to programmers which the first personal computers lent to my generation, while at the same time providing a scalable platform to run the applications these programmers develop at scale. The incentive mechanism provides benefits to programmers who actively work on applying Raiden to problems or passively participate in supporting other programmers work without providing unfair advantages based on a social, political or economic basis. A side-effect of this approach is that it may also encourage "enterprise" participation in the Raiden network. The reason for this is that the ability for idle hardware to earn network credit might offset costs in a way that competing systems may not. _There are numerous advantages to distributed, federated systems which apply to this architecture but which I won't enumerate here (they are thoroughly treated elsewhere). What is important to note is that these advantages are intrinsic to the system described above and serve to extend the list of advantages to the approach described._---
title: Goals
date: Wed, 02 Sep 2015 09:45:28 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I ran across this passage in the review of The Beastie Boy's "Hot Sauce
Committee Pt. 2" on [ Murfie ](https://www.murfie.com/albums/beastie-boys-hot-
sauce-committee-part-two--2) :

"  All this hurly-burly camouflages the essential truth of  The Hot Sauce
Committee  : that  the Beasties  could sit on an album for two years to no ill
effect to their reputation or the record’s quality. This doesn’t suggest
they’re out of step so much as they’re out of time, existing in a world of
their own making, beholden to no other standard but their own.  "

That paragraph so precisely describes my personal vision of nirvana (not
Nirvana) that reading it gave me chills.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Goodbye Facebook
date: 2019-01-10
author: Jason J. Gullickson
draft: false
tags:
  - facebook 
  - privacy
---

This is a note I originally posted to Facebook in response to their "suicide AI" system described in this [TechCrunch article](https://techcrunch.com/2017/11/27/facebook-ai-suicide-prevention/?fbclid=IwAR0aZHNnWyBTM0J4vrTL_NwIBZ_W6iD1sevm4vPSaXUlaxYEp5gnwt3el1A):

Below is the original, unedited content:

___

So the first thing that came to mind when I read this is that it’s going to result in a lot of false-positives (anyone familiar with AI knows this).  That means Facebook will automatically be sending cops to peoples doors, and there’s a lot of problems with that.  Here’s a couple examples:

https://www.thedailybeast.com/man-threatens-suicide-police-kill-him

https://blogs.findlaw.com/law_and_life/2018/09/apple-watchs-auto-911-call-feature-could-cause-legal-trouble.html

The more I thought about it, the more potentially sinister it became.  If Facebook AI is reading everything you post, and they are willing to alert the authorities based on this to prevent suicide, what else are they alerting the authorities to?

https://web.archive.org/web/20130801184832/https://medium.com/something-like-falling/2e7d13e54724

Then I realized the *real* motivation for this technology: marketing.

Facebook is an advertising company, and think for just a moment how vulnerable someone considering suicide is to being sold happiness; in the form of material goods, activities and of course in pill form.

Perhaps even worse, consider how vulnerable surviving friends and relatives are, in the wake of a successful suicide.

This might sound like wild speculation, but to anyone familiar with the way Facebook willingly and repeatedly violates its users trust, it’s hard to imagine why they *wouldnt* do exactly this.

https://www.nytimes.com/2018/12/18/technology/facebook-privacy.html?fbclid=IwAR14-eVuwzWuqAcPX8B2ErCibnViVmSq25g81vlAFjZ5fOlmipA4-758lx8

Considering all this, and the fact that there’s no way to "opt out" of this system (it’s already been reading your posts) I can no longer voluntarily interact with Facebook and I’ll be leaving the system permanently soon.

Unfortunately (like the mafia), you can’t just "quit" Facebook.  But it’s a good first step.  I urge you to read all the linked articles in this post & comments and seriously consider if you should do the same.

https://medium.com/@SpiderOak/facebook-shadow-profiles-a-profile-of-you-that-you-never-created-302f99f20930

https://www.privacyinternational.org/

I would like to stay I touch with you and to that end I will be reaching-out to all my Facebook friends individually so I can update my private contact book.

___---
title: Good FPGA dev board
date: Mon, 16 Dec 2013 23:08:27 +0000 (GMT)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
http://papilio.cc/index.php?n=Papilio.Hardware

  

  

Sent from Evernote  
---

---
title: Good Programmers
date: Thu, 06 Aug 2015 09:09:56 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I've been thinking a lot about good programmers lately.

I think the only real requirement to become a good programmer is a genuine
love of the work (which is no different from any other trade I imagine).  If
you don't actually love programming you can still achieve a level of
proficiency that will allow you to be a professional and make a living, but
it's bad for you, and it's bad for the world.

It's important to note that even if you love to program now, you might not
always love it (and the converse is of course true as well).  I've been
programming since childhood, and while most of my life I've loved it, there
have been several long stretches (years) where I wrote no code at all (or if I
did it was out of obligation).

During these times I thought that I had reached a point where I was no longer
going to be a programmer. I thought that I, or the world, had changed and it
was no longer something I was going to do.  Each time this passed, and each
time I understood afterwards why I stopped programming.  These experiences
have made me a better programmer.

Tools come and go, as do interest, motivation and resources.   If you love to
code, do it.  Don't let anyone else tell you you can't (or shouldn't),
especially because your work doesn't fit in with the current fashion or trend
(remember that what's practiced today will be obsolete tomorrow).

Learn a number of languages, systems and other tools; not just because each
will teach you something new but more importantly you'll start to see what
they have in common, and what all programming tools have in common, and those
are the things that great programmers carry with them through time.

\--

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Hack It
date: Sun, 4 May 2014 09:44:03 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
You may have noticed I had a brief flirtation with [ svbtle
](http://www.svbtle.com/) , in fact the three posts I made are still over
there:

  

[ http://jjg.svbtle.com/ ](http://jjg.svbtle.com/)  

  

It's a really cool platform, simple, attractive and incorporates some of the
very ideas I've had over the years to support the cultivation of ideas.

  

However I was using it for the wrong reasons, and as nice as it is, it's still
an old-world monolithic system based on centralized servers.  This isn't a
direct criticism, it will serve many people well, but it's not right for me,
and using it does nothing to improve Preposterous.

  

So I painstakingly moved my posts back to Preposterous, and committed myself
to figuring out why my custom domain suddenly broke.  In the process I
remembered some of the subtle reasons I fell in love with the Posterious model
in the first place, why I was compelled to re-create a part of it and learned
a little more about nginx and ipv6 along the way.

  

So for better or worse, [ gullicksonlaboratories.com
](http://gullicksonlaboratories.com) is once again powered by Preposterous
(avaliable on both ipv4 and ipv6!), warts and all.  This will force me to
either improve it, or stop posting altogether.

  

talk about a rock and a hard place...

  

  

\- Jason

  

---
title: Happy Birthday Preposterous!
date: Fri, 12 Dec 2014 10:38:32 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
It was one year ago today that Preposterous went on-line.  I christened the
event with this well-crafted post:  
  
[ http://jason.gullickson.preposter.us/pirst-fost.html
](http://jason.gullickson.preposter.us/pirst-fost.html)  
  

Over the course of the last year Preposterous has grown in irregular and
unpredictable spurts, but I'm happy to say that after spending considerable
time elsewhere I have returned to working on Preposterous in earnest and have
some fairly ambitious plans for the next year.  
  

In my experience if you can keep anything alive for two years there is a very
strong chance that it will continue on and grow, and if you can't then the
chances of the project going anywhere are significantly diminished.  Here's to
hoping that what I have in mind doesn't kill us before this time next year.  
  
  

\- Jason  

---
title: hard delete test 0
date: Fri, 10 Jan 2014 22:33:31 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
  

---
title: Hard delete test 1
date: Sat, 14 Dec 2013 00:00:07 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---


---
title: "Has It All Been Leading to This?"
date: 2019-09-06T13:55:52Z
draft: false 
---

Last week I had lunch with a friend to talk about the projects we're working on (and how I might be able to pursue one of these professionally) and one of the things we talked about was developing a line of devices that provide most of the features of things like connected thermostats, light switches, doorbells, etc. without requiring you to send data to "the cloud".  I talked about my plans to start by building a thermostat along these lines, but he suggested I start with something simpler.  At the moment I didn't understand that he was talking about the big picture, so I focused on rebutting his arguments on a technocal level.

It wasn't until a few hours later that it dawned on me what he was getting at, but when it did, my brain kind of exploded and I saw how some of my best projects in the past may have been leading-up to this one.  

When I realized that what my friend was getting at was that I start with the simplest device that could then serve as a building-block for consequitive designs, I stopped thinking about each device individually and started seeing them as a progression where each new device would build on the foundation of the last.  This perspective change made me think a lot more about the *software* side of the devices and think about how it could be composed in layers that can be built-up or peeled-away to match the complexity and capabilities of each device.  I then realized that I had already created and tested these layers long ago without even realizing that they were related.

Now it seems obvious, but isn't that how hindsight always is?

The first project was [RESTduino](https://github.com/jjg/RESTduino), which provides an HTTP interface to the [GPIO](https://en.wikipedia.org/wiki/General-purpose_input/output) of a networked microcontroller.  This approach is somewhat commonplace now, but it was novel (dare I say, *original*?) when I first implemented it almost a decade ago.  The second project was [JSFS](https://github.com/jjg/jsfs/), an HTTP interface for storage (and more, but we'll get into that later).  I developed JSFS to address the specific problem of storing data for client-side Javascript applications.  Again there are now many technologies like this, but when I started JSFS the options were slim, and the ones that existed were proprietary and certainly not portable.  JSFS evolved into much more than a simple storage system, but at the heart of it was simply an HTTP interface to hardware resources, not completely unlike RESTduino.

Why I didn't see the relationship between these two project before I'll never know, but when I started thinking about creating a line of "smart" devices it clicked, and the more I thought about it I realized that a combination of these two systems could provide the "operating system" for everything from a microcontroller driving a lightswitch to a high-performance compute cluster.

I should elaborate a little on that last part.  One of the features that I designed for JSFS but never implemented was the ability to *execute* code (JSFSX, if you will).  In basic terms any file containing sourcecode (Javascript, Python, C Rust, etc.) could be executed by issuing a special HTTP request.  In addition to executing this code, JSFS could use its federation features to distribute execution across a group of JSFS servers.  Since JSFS already stores and distributes data, this parallel execution model would works not unlike [Apache Spark](https://spark.apache.org/), but simpler and with less programming language restrictions.  I didn't pursue this feature while JSFS was under active development because at the time the demand by its users was focused on storage, and the industry didn't seem very interested in high-performance computing (things have changed in the last few years...)

So where is all this going?  Well, as the programming model for RESTDuino/JSFS(X) fits with the needs of the devices I plan to design, I'm going to combine the two and re-implement it in Python, because Python is avaliable on all the processing architectures I'm targetting, from embedded microcontrollers to full-blown general-purpose computers.  A microcontroller running the software I'm describing on top of [MicroPython](http://micropython.org/) will have the same programming interface as a supercomputer running the same.  Different devices will of course have different features, but these will all share the same API structure, and will all provide the same basic level of functionality.

Going back to the conversation that started this all, my next step is to write a version of this software (which doesn't even have a name yet) that will run on hardware designed for the simplest of all these devices: a switch.  This could be as simple asn a [ESP8266](https://en.wikipedia.org/wiki/ESP8266) and a relay, but I want to make sure that what I'm building has the potential to be re-used as a building block for more sophisticated devices an as such I'm planning on building a switch that is a little more feature-rich than it needs to be.  If I learned anything from watching the rise of devices like the [iPhone](https://en.wikipedia.org/wiki/IPhone), it's that there's value in providing hardware that you might not need immediately, especially if you're designing something that will allow other programmers to devise new and unforseen applications for.


---
title: Hat Trick
date: Thu, 12 Dec 2013 23:12:39 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is the third post, to debug some code that is only supposed to process
new messages.  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: Hello Friends (and goodbye)
date: Wed, 12 Feb 2014 22:57:50 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Hello Friends,  
  
This is a bit overdue.  I’ve been feeling this way for a long time but
couldn’t really put my finger on it until tonight.  I’ve caught myself
engaging in negative behavior more and more frequently, most often taking
place online but even occasionally in person, and I don’t like it.  
  
I remember the joy of tech, of a time before every conversation about
programming had to turn into a language debate, when the only thing that
mattered about a tool was that you could build something cool with it.  Now it
seems like almost every conversation turns into some sort of competition or
confrontation, political or ideological debate and it sucks all the joy out of
the work.  
  
There’s so much good work to be done, and a lot of it going on.  I think that
social networks and media are very powerful tools for good, but in the last
few years they seem to be more effective at propagating negative broadcast
messages, and like a flat ethernet network, these storms can drown out
important data.  
  
To stretch this analogy into the absurd, I considered finding a way to
firewall or VLAN these networks to improve the signal-to-noise ratio, but it
quickly became obvious that this is a futile effort.  The topology doesn’t
make it easy, and the nature of the nodes in the network make it nearly
impossible.  
  
So as usual the most direct approach is best, and I’m planning to bow-out of
these systems for the foreseable future.  Instead I plan to concentrate on the
mountains of work avaliable which clearly contributes to a brighter future,
and do what I can to avoid incuring drag.  There will always be negative
influences, but it’s become clear to me that none are as extremely lopsided
than the modern-day AOL’s we’ve constructed at the cost of the robust,
independent, syndicated media we had before the dawn of Myspace.  
  
I’ll be posting updates on my progress here, which is likely to lead to
improvements in this site (eating your own dogfood usually has that effect).
If you’d like to stay in touch, perhaps you’d consider joining up (just send
your first post to [ preposterous1984@gmail.com
](mailto:preposterous1984@gmail.com) ).  I’d love to hear what you have to
say, and I’ll have time to do more in-depth reading now that I’ll be off the
140 character feedbag.  
  
  
\- Jason

---
title: "Hello World"
date: 2018-11-27T12:23:25-06:00
draft: false 
featuredImg: ""
tags: 
  - programming 
---

## C Beautiful C 

{{< highlight c >}}
#include <stdio.h>

int main(void) {
  printf("Hello, World!\n");
  return 0;
}

{{< /highlight >}}
---
title: Hippo
date: Fri, 10 Jan 2014 19:32:42 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/185-image.jpeg) ](assets/185-image.jpeg)

---
title: Homing in on 1.0
date: Fri, 13 Dec 2013 23:57:41 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
In just over an hour tonight a lot of the critical bugs in Preposterous fell.
A handful remain, but the most mysterious ones are behind us for now.

  

We'll see if I can wrangle the rest tonight.  Would be awesome to wake up
tomorrow with the first release behind me.

  

  

\- Jason

---
title: "How to get there from here?"
date: 2018-11-06
draft: true
tags:
  - philosophy
---

As an anarchist I'm interested in autonomy.  This means eliminating the means of control used by authority.  For me that's primarily indirect violence via capitalism, so my primary objective is to remove the things I need to survive from being mediated via capitalism.

Ultimately in order for this to work on any human scale it requires collaboration, but it's easier to get people on-board when you can show them something that works.  So start small.

Start with your own home, or even a single room in your home.  Study the space and identify what's required to support it (heat, electricity, etc.).  Examine how these requirements are bound by capitalism and find ways to meet these needs without that dependency.  Invite others into these spaces and encourage them to enjoy the autonomy they provide, in the spirit of Hakim Bay's Temporary Autonomous Zones (TAZ).

This sounds kind of abstract so maybe a concrete example will help:

(insert example here)

Once you've achieved autonomy in one space (no matter how small), the next step is to expand the area of that space by applying the same process to an adjacent space.  If you've liberated one room, liberate the one next to it; if you've liberated a household, work on your neighbor next.  Once you have a square of autonomy to anchor yourself to, the next square should be easier, then the next, and so on.

There will undoubtedly be setbacks, and exponential growth should not be expected.  Nor should unchecked growth be desirable; if a space resists autonomy forcing it to become autonomous is just another form of tyranny.  Instead grow around these spaces.

## References/Ideas

* Conway's Game of Life (could be used to simulate growth of autonomous spaces?)
* The Golden Ratio
---
title: How to use Preposterous 1.0
date: Wed, 18 Dec 2013 13:40:12 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
It occurred to me that the welcome email could use some better instructions on
using Preposterous, but I've promised myself to stay out of the code for at
least a week after the 1.0 release was posted.  So, until I can update the
welcome email, here's some basic instructions, _in blog form!_

_  
_

**To create a new blog**

Send an email containing your first post ("hello world" is sufficient) to [
preposterous1984@gmail.com ](mailto:preposterous1984@gmail.com) .  You'll get
an email back letting you know the blog was created and you'll get a link to
the new blog as well.

  

**To add a new post**

Send another email to [ preposterous1984@gmail.com
](mailto:preposterous1984@gmail.com) !  Preposterous currently supports text,
HTML, photos/images and most audio and video formats, so send anything  you
like to create a new post.  Once the new post has been... posted, you'll get
an email back from Preposterous letting you know it's ready with a link that
goes directly to the new post.

  

**To edit a post**

Send the updated post _using the exact same title_ to [
preposterous1984@gmail.com ](mailto:preposterous1984@gmail.com) .
Preposterous will replace the old post with the new one, and send you an email
when the updated post is ready.

  

**To delete a post**

Deleting a post is like updating a post, except instead of sending an email
with updated information, you send one that is empty.  Currently, this doesn't
remove the old post entirely (it will still be listed in the index), but it
will remove the contents of the post.  Removing the post from the index is on
the todo list...

  

**To see who else is using Preposterous**

Visit [ http://preposterousme.com ](http://preposterousme.com) to see a list
of everyone who's posting to the beta site.

  

This should be enough to get you started.  If you think of something else you
should be able to do, take a look at what's being worked on here:

  

[ https://github.com/jjg/preposterous/issues?state=open
](https://github.com/jjg/preposterous/issues?state=open)

  

If something you need isn't on that list, post a new Issue and we'll see what
we can do about it.  Also if something doesn't work the way it's supposed to,
you can use this Issues list to let us know about that too and we'll work on
fixing it up in the next release.

  

I hope you enjoy this version of Preposterous, let me know when you find bugs
and I look forward to seeing what you do with it!

  

  

\- Jason

---
title: HTML Weird A test
date: Mon, 8 Dec 2014 00:14:12 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Looks like plain text-ish posts are _improved_ but I see a lot of old posts
with weird questionmark-in-a-diamond things now, which **isn't encouraging** .

  

I'm hoping that that doesn't happen with this post.  Maybe it's just some
weird old-fashioned thing?

  

Here's to hoping.  Another sentence to test why not?

  

  

\- Jason

---
title: Human-readable blog names
date: Wed, 25 Dec 2013 01:41:53 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
For those of you who find names like "098dbd12ffcc1e87bf166d68e02cb51e" hard
to remember, we've added a little feature that will give your blog a slightly
easier to memorize name.  These names cannot be changed (if you'd like a
technical explanation as to why, read on below), but at some point in the
future we'll offer personalized blog URL's via support for custom domains.

  

In the meantime, every Preposterous blog gets an awesome machine-generated
human-readable name, enjoy!

  

The reason this name can't be changed is that it is actually derived from the
email address  associated with the blog (don't worry, it can't be reverse-
engineered in any practical sense).

  

As you know, Preposterous blogs are tied to the email address that you use to
submit posts.  Since displaying your email address to the world is not the
nicest thing to do, we generate a [ hash
](http://en.wikipedia.org/wiki/Cryptographic_hash_function) of your email
address and use that as the publicly-visible address for your blog.  This
works great, but these hashes are hardly memorable, so we use [ another
algorithm ](https://github.com/zacharyvoase/humanhash) that takes the hash for
input and generates (more) human-readable names.

  

Since these names are ultimately derived from your email address, they can't
be altered "at will", and the only practical way to change them is to change
the email address used for the blog.  As mentioned above, down the road we
plan to offer a "custom domain" option that will let users point their own
domain name at a Preposterous blog which will bypass these generated names
altogether.

---
title: Humans Love-Hate Relationship With Automation
date: Mon, 03 Aug 2015 15:32:23 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Back in the 1950's and 60's people said _"In the future, nobody will have to
work!"_ now they complain _"In the future, there will be no jobs!"_ .

I have a very simple solution to this: If you automate your job, you keep
getting paid for it. This would need to be a law, because obviously few
companies are going to comply voluntarily, but they should, and here's why:

Robots don't sue you for getting hurt/killed on the job. They rarely show up
drunk and get sick far less often than humans. They are massively more
productive and don't complain about office chairs or lack of natural light.
They don't form unions or demand health insurance. Long story short, it's
still a net-gain to automate even if it doesn't reduce salary expenses.

But it gets better. With this kind of motivation, workers won't only welcome
automation, but they will help design it. Not only does this make the
transition less painful, it also means the results will be superior because
people who know the job will be working cooperatively to automate it. Hell,
some people might find out they like it so much they go on to another job and
repeat the process.

FWIW this is a normal mode of thought for programmers, going into a new task
and thinking about how you're going to automate yourself out of it. Imagine if
the entire workforce approached problems this way, without fear that doing so
would result in financial ruin?

So in the long run we have more robots doing work while people continue to get
paid, and employers enjoy lower expenses and smarter workers. I know that win-
win scenarios are not fashionable, but I think if we were willing to give up
on our precious fear, it might just work.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Image link test
date: Fri, 10 Jan 2014 08:29:19 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/181-photo.jpg) ](assets/181-photo.jpg)

---
title: Image plus notification production test
date: Fri, 13 Dec 2013 23:26:27 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/42-3dprintingwithreprap01_01.jpg)
](assets/42-3dprintingwithreprap01_01.jpg)

---
title: Inline images working correctly
date: Wed, 25 Dec 2013 00:10:44 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
One of the main things holding me back from migrating my existing Wordpress
blog at [ http://www.gullicksonlaboratories.com
](http://www.gullicksonlaboratories.com) to Preposterous was an issue with the
way Preposterous rendered images (essentially ignoring the fact that they are
presented in-line with text).  I'm happy to announce that this bug has been
fixed, as you can see below:

  

![Inline image 1](/preposterous/assets/143-tesla-roadster-01.jpg)  

  

I'm in the process of determining if there are any other barriers to migrating
my WP blog.  I'm very excited about the possibility of making the move, so any
other showstoppers I can come up with will be documented soon and with any
luck, dispatched with before the new year.

  

  

\- Jason

---
title: iOS 3
date: Mon, 8 Dec 2014 00:19:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This email will have some _style_ .  Hopefully if there is any problems left
this will **invoke** them.

  

We can only hope.  Right!  
  
Sent from my iPhone

---
title: iOS bug test 1
date: Sun, 15 Dec 2013 14:39:20 -0800 (PST)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Testing to see if the iOS bug impacts non-ios mail clients on iOS

—  
Sent from [ Mailbox ](https://www.dropbox.com/mailbox) for iPad

---
title: iOS bug test 2
date: Sun, 15 Dec 2013 16:40:05 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Now testing from iOS built in mail app. Sent with my thumbs

---
title: iOS image 10
date: Sun, 15 Dec 2013 23:57:46 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Sent with my thumbs

[ ![](/preposterous/assets/96-photo.jpg) ](assets/96-photo.jpg)

---
title: iOS image 11
date: Mon, 16 Dec 2013 00:01:59 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Sent with my thumbs

[ ![](/preposterous/assets/97-photo.png) ](assets/97-photo.png)

---
title: iOS image and text test 1
date: Tue, 24 Dec 2013 23:02:03 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a test to debug a problem with posts from iOS that include both images
and text. This is test 1 Sent with my thumbs

[ ![](/preposterous/assets/140-photo.jpg) ](assets/140-photo.jpg)

---
title: iOS image and text test 2
date: Tue, 24 Dec 2013 23:22:08 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is some **html** text, _let's see what happens..._  
  
![](/preposterous/assets/141-photo.jpg)

  
  
Sent with my thumbs

---
title: iOS image plus text test 3
date: Tue, 24 Dec 2013 23:52:28 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a test to make sure in-line image support for iOS didn't break email
from gmail

  

![Inline image 1](/preposterous/assets/142-cobra_commander.jpg)  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: iOS mail image test 20
date: Tue, 17 Dec 2013 20:27:55 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a test to see if Jamie's issue is consistemtn. Sent with my thumbs

[ ![](/preposterous/assets/119-image.png) ](assets/119-image.png)

---
title: iOS test 5
date: Sun, 15 Dec 2013 15:27:42 -0800 (PST)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Another test from iOS via mailbox app

—  
Sent from [ Mailbox ](https://www.dropbox.com/mailbox) for iPad

---
title: iOS video test
date: Wed, 25 Dec 2013 09:47:37 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Trying some video of my #reprap straight from the ipad.

---
title: iPhone Case
date: Fri, 10 Jan 2014 22:17:07 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
![Inline image 1](/preposterous/assets/186-img_0041.jpg)  

  

source: [ http://www.thingiverse.com/thing:167177
](http://www.thingiverse.com/thing:167177)

---
title: Iron Butterfly
date: Mon, 16 Dec 2013 21:36:02 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/109-photo.jpg) ](assets/109-photo.jpg)

---
title: Is the web worth =?UTF-8?Q?it=3F?=
date: Thu, 13 Aug 2015 12:20:56 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I had myself a little [ Tweetstorm
](https://twitter.com/jasonbot2000/status/631870873209712641) this morning
about the subtle tyranny enjoyed by the companies that make web browsers.
It's hard for me to explain in a way that feels accurate and complete to me,
but the tweets do an OK job.

It's thinking about interlocking dependencies like this that make me think
about giving up web development all together and focusing on writing code for
something that isn't so entangled in the socio-economic-political structure
that makes writing good software on the web such a quagmire.

The problem is that unless what I'm doing is going to replace the web, it's
just escapism, which isn't a way to live your life (even if it's necessary now
and again).  So it's not a decision about working on the web or working on
something else, it's a question of working on the web or replacing it with
something else.

My frustrations with the web are mostly related to it in terms of being an
application platform, and there are certainly better ones out there.  The
problem is that the web is more than just an application platform, and so when
considering replacing it, so what it comes down to is this: even if it's a
frustrating application platform, do the other things the web provides make it
worth the frustration?

I'm not sure.  I used to think the answer was obviously yes, but as the web
moves closer and closer to a small group of content farms owned by an even
smaller group of companies, and away from the constellation of independent,
interesting and creative websites I feel less and less like it's worth
preserving, and more and more like it's devolving into an interesting old tool
that served us well but has become obsolete.  Like when a counterculture
becomes a product.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Jedi
date: Sat, 12 Jul 2014 02:17:16 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a screenshot of the [ jsfs ](https://github.com/jjg/jsfs) -hosted
editor "jedi", editing its own source code:  

  

![](/preposterous/assets/257-screenshot 2014-07-12 at 2.00.22 am.png)  

  

Jedi is the first application developed specifically to run on jsfs.  By
beginning with an editor, it's possible to bootstrap the developent of any
other application and develop them from within jsfs itself.  There is a lot
more to do, obviously, but this is a significant development, and a major step
toward making jsfs behave like a true general-purpose computing platform.

  

Jedi source code is available on [ Github ](https://github.com/jjg/jedi) now,
future versions will be available directly via jsfs.

  

  

\- Jason

​

  

  

---
title: JSFS Freeloading
date: Sun, 08 Feb 2015 08:50:55 -0600
author: jjg
draft: false
tags:
  - preposterous
---
I've been thinking more about the conversations [ Preston
](https://twitter.com/gl33p) and I have had about freeloading (mostly due to
having conversations with people who are against it) and in particular the
idea that freeloading is necessary to achieve reasonable levels of efficiency
in robust or redundant systems. This morning a very concrete, practical
application of this principle came to mind.

**Background**

[ JSFS ](https://github.com/jjg/jsfs) is a REST/HTTP-based file storage system
I've been working on.  JSFS provides block-level deduplication of the files it
stores, which means that each file gets cut up into pieces and only unique
pieces consume disk space.  I plan to write a lot more about JSFS soon, but
I'm holding off until the next release is ready.

**JSFS Freeloading**

As the number of unique blocks stored increases, so does the likelihood that
the next stored file will contain a block that has already been stored.  Based
on this, it is at least _conceivable_ that some new files will be composed
exclusively of existing blocks and therefore consume no storage at all* in
JSFS.  What I propose, is an extension to JSFS that makes determining this
state user-accessible and that any commercial JSFS store allow files like this
to be stored at no cost.

It might seem unlikely that this could actually occur (a file composed
completely of pre-existing blocks) but in practice it is not uncommon
depending on the nature of what is being stored.  If JSFS is used to store a
lot of small original works from say, a blog author, then this scenario is
less likely.  If it is used to store large media files, and in particular
things like collections of music or video for many different users, it is
actually quite likely that that duplicate blocks exist, and in some cases
complete identical files (differing only in their name or namespace) are
stored as well, in which case it's obvious that additional copies of these
files will require zero additional storage.

There are other aspects about how JSFS works that will further increase the
likelihood of a no-unique-blocks file scenario, but I'll leave elaborating on
that that for a later post.

Considering this, it's not hard to imagine a large commercial JSFS system that
would potentially "generate" a large amount of this "free" storage that would
potentially go unused.  Like waste heat from a power plant, if this can be
captured and used for useful work it pushes the system to otherwise
unobtainable levels of efficiency.  If the SLA's for this "free" storage are
established and managed correctly, any impact on the "primary" applications
can be minimized or eliminated while empowering users who might otherwise be
unable to afford un-subsidized storage (or avoid the exploitation that comes
with subsidized storage).

* some resources are consumed to relate the existing blocks to the new file, and of course some resources are required to retrieve the stored file, but these are orders of magnitude less costly than the disk storage that would be required to store the data in a traditional filesystem. 

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: JSFS-
date: Thu, 24 Sep 2015 01:55:13 -0500
author: jjg
draft: false
tags:
  - preposterous
---
JSFS's scalability limits center around two things:

  

1\.  The memory-resident superblock

2\.  Maximum memory available to a single Node.js process

  

The memory-resident superblock means that memory utilization grows in-step
with storage pool size.  This design choice was selected to focus on
performance as opposed to absolute scalability.  JSFS outperforms other
deduplicating file systems in terms of memory efficiency (RAM-to-storage
ratio) but this still limits maximum storage based on available system memory.

  

The first problem wouldn't be quite so bad if not for the second one.  If the
maximum JSFS storage pool size was limited to the amount of memory in a 64 bit
address space this would seldom be a showstopper, however Node.js imposes a
per-process memory limit that is significantly lower than what a 64 bit system
can address.  It's hard to get an authoritative answer about the limit but
from what I've read it's at most 4GB and possibly as low as **2GB** .

  

When I settled on the memory-resident superblock I wasn't aware of this
limitation of Node.js.  Even so I did consider other implementations that not
only eliminate the memory-resident superblock (and associated scalability
limits) but also make it possible to leverage multithreading more, which can
help work-around Node.js's per-process memory limits.

  

The reason I didn't pursue these options, aside from not knowing about
Node.js's memory limits was that they would make some of the features I wanted
to include in JSFS either more complex or potentially impossible.

  

Now that I'm aware of the memory issues I must adopt one of these other
designs in order to scale JSFS for one of my core use cases.  That said I
don't want to throw away functionality for systems that don't have the
scalability requirements I'm facing with this one case.

  

So my plan is to fork the project, and since the forked version will have
fewer features, I'm calling it JSFS- (as opposed to JSFS+).  This might seem
counterintuitive as this fork will be capable of handling much more storage
(and potentially more load as well), but I want to make it clear that it is
not a "superior" version of JSFS, but simply a more specialized one.

  

I do want to maintain API-level compatibility between JSFS and JSFS-, as well
as federation support to the degree that it makes sense.  There will however
be a few features that will have to go, and there may also be a performance
penalty when compared to JSFS, at least when storage pool sizes are below a
certain threshold.

  

I haven't completed the design for the new version yet, but right now it's
clear that the "directory" functions (returning an index of files stored
within a namespace or path) will probably have to go.  It's also likely that
versioning will be eliminated as well (although this is less certain).  Write
performance is likely to take a hit as the inode data previously stored in-
memory will be written out to disk (although with this new distributed
superblock design, synchronous writes may not be required).  Read times are
likely to be slower too although there may be some ways to work around this.

  

Since there will be no more monolithic superblock it will be possible to split
reads and writes off into separate processes which will help improve
performance on multi-core systems (as well as reduce the impact of the Node.js
memory limit).  It remains to be seen exactly how far I can go with this
before the overhead of each process become inefficient, but it's at least more
parallelizable in theory compared to the memory-resident superblock.

  

I hope to begin working on JSFS- within the next week.  I don't expect it to
take more than a few days to complete the coding, but testing is another
matter (especially at the scales in targeting.

  

I'll post updates here as the project progresses

  

  

  
// jjg

---
title: json index test 10
date: Fri, 27 Dec 2013 10:14:14 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
10th test of json index generation

---
title: json index test 2
date: Wed, 25 Dec 2013 11:15:35 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 2 of the json post index generator.

---
title: json index test 3
date: Wed, 25 Dec 2013 11:19:09 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 3 of json indexing

---
title: json index test 4
date: Wed, 25 Dec 2013 11:20:30 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 4 of json indexing.

---
title: json post index test 13
date: Fri, 27 Dec 2013 10:30:39 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Lucky #13!

---
title: json post index test 1
date: Wed, 25 Dec 2013 11:12:10 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is test 1 of the code that generates json post index files.

---
title: json post index test 6
date: Wed, 25 Dec 2013 11:23:33 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 6 of json post index generation

---
title: json post index test 7
date: Wed, 25 Dec 2013 11:24:38 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 7 of json post index generation

---
title: json post index test 8
date: Wed, 25 Dec 2013 11:27:21 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 8 of json post index generation.

---
title: json posts index test 11
date: Fri, 27 Dec 2013 10:26:33 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is test 11 of the posts json index code.

---
title: json posts index test 12
date: Fri, 27 Dec 2013 10:27:43 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 12 of json posts index

---
title: Kick it
date: Wed, 19 Aug 2015 23:07:34 -0500
author: jjg
draft: false
tags:
  - preposterous
---
For awhile I've been kicking around the idea of doing a crowdfunding campaign
to cover the cost of the components I need to build a working prototype of my
OffGRiD project ( [ http://wiki.2soc.net/doku.php?id=offgrid
](http://wiki.2soc.net/doku.php?id=offgrid) ).

  

I don't need to raise a lot, in fact $1000.00 should do it.  I have most of
what I need already, but there are a few monolithic costs (the Pixel Qi
screen, the solar panel, etc.) that are just a bit beyond what I can swing for
something experimental.

  

It _would_ be a bit of a "throwback" in that I don't plan to offer any sort of
"pre-order" rewards in the typical sense.  The goal of the project is to
produce a design that can be produced locally after all.  What I _could_ offer
is early access to the design files and documentation (it will be open-source
once it's refined of course) but I'm not sure what other "perks" would make
any sense.  I suppose I could offer a prototype unit if someone pledges
$1000.00 and foots the entire budget.

  

I have thought a little about how to make getting your hands on an OffGRiD
easier, and perhaps that could be part of the campaign.  I'm imagining a
website where you could customize the machine by selecting hardware options
(different processor boards, storage, batteries, interfaces, etc.) as well as
design attributes and have the output of the process be anything from
printable files and a BOM to automatically ordering components and printed
parts online.  Another feature could be connecting customers with makers in
their area who they could work with to complete their machine.

  

If the site described above could be designed in a somewhat generic way, it
could be used as a model for any sort of "manufacturing 2.0"-type product.  In
fact that might be the real project here, and OffGRiD might just be a sort of
test run (while still being valuable itself of course :)

  
  
//  jjg

---
title: Kinetta
date: Sat, 22 Nov 2014 16:10:07 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
The **Kinetta Cinema Camera** , all that seems to remain is this PDF:  
  
[ http://www.kinetta.com/download/files/kinetta-camera-brochure.pdf
](http://www.kinetta.com/download/files/kinetta-camera-brochure.pdf)  
  
I remember reading excitedly about the Kinetta back in 2004.  Today the
general specs seem fairly run-of-the-mill, but as an expressive tool I know of
nothing contemporary that comes close.  
  
Too bad.  
  
I wonder if I could talk the company into release the design documents if they
don't intend to resume work on the project.  
  
  

\- Jason

---
title: "Lasertime"
date: 2019-02-17T14:14:24Z
draft: false
tags:
  - k40
  - lasercutter
---

Last weekend I spent most of my birthday setting-up the "K40" laser I purchased on eBay.  I was excited to get the machine online, but after reading a *lot* about the machine I wanted to dedicate some time to setting it up carefully.


# Make room

The first step was to clear some space.  I needed a stand for the machine and unfortunately it was slightly too big to fit on the desk I had hoped to use.  If the weather was more cooperative I would have just cut a larger top for the desk but as those tools are located in the garage (and it's January), that will have to wait until things thaw out a bit.

![Empty folding table](/k40_empty_table.jpg)

In the meantime I found a sturdy folding table to use.  It's almost twice as large as it needs to be, but it will do until I can get a new top cut for the intended desk.


# Power

I want to make sure the laser has enough power to function properly and minimize the chances of interruption due to something else on the same circuit tripping a breaker. I found an outlet on a relatively unused 15A circuit, and although this is the same circuit that the (gas) dryer uses, the dryer is in the same room I can make sure that the two machines are not running at the same time.

In the long run I'd like to have a dedicated circuit for the laser, which is one of the reasons I located it near the breaker box.


# Cooling

![Bottles of distilled water](/k40_coolant.jpg)

The laser is liquid cooled and comes with a small "garden pond"-style pump to circulate water through the machine.  There's a lot of discussion about what the best mix of liquids is but for now I'm starting with simple distilled water and a few drops of dishwasher soap.  I may add something to prevent alge/etc. growth but as long as I keep the water moving I don't expect even that to be a problem.

![Coolant pump](/k40_pump.jpg)

As the pump and hoses were included with the machine the only additional costs are distilled water, a storage tank (5 gallon bucket & lid) and a flowmeter to make it easy to tell when the coolant is flowing.

![bucket and mechanical flow meter](/k40_flowmeter.jpg)

At some point I might invest in a more sophisticated flow measurement device (something that could shut-down the laser automatically if flow is interrupted) but for now this is enough to keep an eye on things.

Aside from making sure the coolant is flowing (and not leaking) the most important thing to do at this stage is eliminate any air bubbles in the cooling system.  I had a couple of stubborn bubbles in the laser tube, but I was able to work these free by (carefully!) tipping the machine with the coolant pump running.


# Ventilation

This turns out to be the most difficult part of my installation.  I chose a location near a window to make things easier, but after testing the exaust fan included with the machine I decided that I'd be more comfortable with something more robust.  Toxic fumes are one of the biggest dangers of these machines and I didn't want to take any chances.

After a *lot* of reading and shopping around I was able to locate a few additional parts to complete the ventilation system:

* Exaust manifold/adapter
* Fan
* Hose
* Exterior exaust port

![Saw and board](/k40_cut_board.jpg)

Aside from assembling these components the bulk of the work is getting the end of the exaust system outside of the house.  I want to avoid making this any more permanent than it has to be (just in case I decide to relocate the laser later), so I removed the inside and outside panes from a nearby window and cut a piece of plywood to fit the space.  

![Hole in board](/k40_cut_hole.jpg)

I then cut a hole in the plywood for the exaust port and mounted the whole thing in the window frame.  Once this was secure I mounted the fan near the exaust port (this lets the fan *pull* the exaust through the hose instead of *pushing* it up from the back of the laser) and connected the manifold, fan and exaust port with hose.

![Exhaust fan](/k40_fan.jpg)

I'm not certain that this fan will be enough, but it was inexpensive and I couldn't find one that I felt better about for less than 10x the price, so I'll experiment with this and upgrade if necissary.


# Test fire

At this point it's finally time to test the laser!  I was really anxious about this because there's a lot of ways to break one of these, and I was especially nervous about something happening to the giant piece of glassware that is the heart of the operation.  It's also the most expensive part and not something I can pick-up locally, so I was looking forward to finding out if it worked or not.

![K40 control panel](/k40_controls.jpg)

While a computer is needed to do anything useful with the machine, you can fire the laser using only the control panel. First double-check power, cooling & ventilation and then execute the following steps:

1. Place a test target in the clamp
2. Carefully move the laser head axis to the center of the target
3. Connect the main power
4. Disengage the e-stop button (twist in the direction of the arrows and it will pop-up)
5. Turn the "machine switch" to the right
6. Flip the "power switch"
7. Flip the "lighting switch"
8. Reduce laser power to 10%
9. Watch the target and press "laser test switch"

![Laser burning wood](/k40_test_fire.jpg)

A puff of smoke and a dark spot on the target (along with no terrible sounds or smoke elsewhere) confirm that the laser is operational!


# Control

Now that the machine itself checks-out, it's time to control it using a computer.  For this I'm using [K40 Whisperer](http://www.scorchworks.com/K40whisperer/k40whisperer.html), a great piece of free software designed to replace the Windows-only proprietary tools that come with the machine.

![Computer running K40 Whisperer](/k40_whisperer.jpg)

Other than the usual Linux permissions problems, getting K40 Whisperer working is a piece of cake.  After [following the directions](http://www.scorchworks.com/K40whisperer/k40w_manual.html) I was able to manually move the laser's axis around using the on-screen controls.  The next (and final step) is to make the laser do some real work.


# Burn baby burn!

I used [Inkscape](https://inkscape.org/) to create a simple drawing in [SVG](https://en.m.wikipedia.org/wiki/Scalable_Vector_Graphics) format (this is one of the formats suppored by K40 Whisperer).  I could have made this easier by drawing a shape instead of using text, but I didn't realize that until I had already done it so I had to add one additional step to the process.

Once the file was properly formatted I opened it up in K40 Whisperer and positioned the laser on my test piece.  I dialed the laser power up to 30%, re-checked cooling & ventilation and then clicked "vector engrave".

![Letter J engraved in wood](/k40_test_j.jpg)

The machine leapt into action and within a second or two the job was complete!  


# What's next?

There's a lot more to do. The stock bed doesn't allow you to use the whole capacity of the laser and doesn't provide much ability to focus the laser, so that might be the next thing I change.

Another modification is to add "air assist" to help make cuts cleaner, keep the optics from getting smoked-out and prevent material from starting on fire.

I also want to add an ammeter to measure the current going to the laser tube to get a more accurate idea of how hard I'm pushing it.

Finally I'm planning to upgrade the exhaust system as it works OK, but I don't trust the stock fan and I think a more powerful system will reduce the amount of cleaning & maintenance needed to keep the machine running.

There's also lots of work to do to determine the best parameters to use for various materials.  I expect this to be an ongoing effort.


# References

* https://k40laser.se
* https://wayofwood.com
* https://lasergods.com



---
title: "Learning Spark"
date: 2019-01-24T08:48:29-06:00
draft: false
tags:
  - spark
  - rain
---

I'm learning [Apache Spark](https://spark.apache.org/) for a work project and of the "big data" systems I've looked at, it's my favorite so far.

I was pretty disappointed with how little assistance it provides the developer in terms of making parallel computing transparent (vs. HPC tools & languages) but it has its charms.

In any event the biggest barrier for me so far has been getting my head around the [MapReduce](https://en.wikipedia.org/wiki/MapReduce) programming model.  My first professional experience in working with data was SQL/RDBMS-oriented and a few years back I started working with key-value stores, but I never really embraced MapReduce, so now I'm paying for that.

It's frustrating to be held-back from reaching a goal or solving a problem that you could solve quickly using what you know because you have to learn something new, but on the flip-side I *love* learning new things, so I'm trying really hard to frame this process with a focus on learning how to adopt the *philosophy* of Spark & MapReduce and not trying to simply force the models I'm comfortable with onto this new environment (having been on the receiving end of that kind of force, I can appreciate why it's a bad idea).

Another exciting aspect about this is that it gives me a very practical application for the [RAIN](/tags/rain) project.  Spark is probably not a perfect fit for a machine like RAIN since Spark is really oriented to data-bound processing and RAIN is more constrained in the storage/memory resources than a more traditional server farm, but if nothing else it will give me a more practical workload to experiment vs. benchmarks.

(it also gives me an excuse to wrench on RAIN during "business hours")


---
title: Lenovo Thinkpad Yoga 11e Chromebook - First Impression
date: Thu, 12 Feb 2015 19:41:37 -0600
author: jjg
draft: false
tags:
  - preposterous
---
As you may know a little more than a year ago I traded in my Apple Macbook Air
11 for a $300 (now $200) [ HP Chromebook
](https://play.google.com/store/devices/details/HP_Chromebook_11_Black_Black_Wi_Fi?id=chromebook_hp_11_blackblack_wifi)
.  That machine demonstrated (for the lowest possible cash outlay) that I
could not only get by, but thrive using an inexpensive Linux-based laptop as
my "daily driver".  There were a few challenges making the jump, and while
significantly less powerful than the MBA, performance rarely held back my
ability to get work done.

However over the course of a year the inexpensiveness of the machine began to
show.  While somewhat durable along the way it picked up a display issue that
caused the screen to "wink out" after coming in from the cold or under other
seemingly random conditions, and even after disassembling the device
(something the designers made no affordances for) I was unable to identify and
resolve the cause of the problem.  It was around this time that I came across
the [ Thinkpad Yoga 11e Chromebook
](http://shop.lenovo.com/us/en/laptops/thinkpad/11e-series/11e-yoga-chrome/) .

![tp11e1.jpg](/preposterous/assets/30-tp11e1.jpg)

There were a few key ingredients that I found appealing about the Thinkpad.
It was designed to be tough (targeting the education market) and having owned
several Thinkpads in my consulting days I believed this claim more than I
would have from say Dell or HP.  It had a build-in SD card reader, which
anyone who works with embedded hardware knows comes in handy (I've also wanted
to compare performance of running chroots on SD vs. USB flash storage).  The
Thinkpad also sports a fold-around design that allows the laptop to be turned
inside-out into something like a tablet, and this coupled with a touchscreen
opens up the possibility of using it as sort of an incredibly over-built
e-reader or kiosk interface (folded like a tent it could be great for running
[ Octoprint ](http://octoprint.org/) next to a [ Reprap ](http://reprap.org/)
).  Interestingly enough it also featured a wired Ethernet port, which is
something that I still need often enough that it was an attractive addition
(and something rarely found on machines in this class anymore).  This coupled
with a sub-$500 price tag made the machine very attractive, and as I was
finding myself increasingly on the lookout for a successor to my HP
Chromebook, it seemed like the right way to go.

Now that I've had the device in my hands for a few hours I thought I'd capture
my first impressions.  I'll start with the good stuff:

It is built like a tank.  There is zero flex whether you hold it by the
keyboard or the screen.  The hinge feels so strong that you're afraid you
forgot to unlock the lid the first time you open it (there's no lock).  When
placed on a solid surface there is no bounce, rock or flex while typing.  I
can't say enough good things about how solid this thing is, it feels like you
could drop it two meters and all it would do is bounce

Speaking of typing, the keyboard is excellent, easily the best keyboard I've
used on a laptop for as long as I can remember.  It's not quite as epic as my
[ Das Keyboard Model S Ultimate
](http://www.daskeyboard.com/model-s-ultimate/) , but it's as close to it as
anything else I've used, and while audible, it's not as likely to get me
dragged out of a coffeeshop and beaten for typing at full speed for hours.
The trackpad is solid and feels good as well, the HP's trackpad always felt a
bit twitchy and almost like it "stuck" when released.  The palmrests, along
with the entire case seems to have a texture to it that isn't as slick as the
HP, and so doesn't seem to be as tacky against your skin (and will probably
resist fingerprints better as well).

The screen is very sharp, colors vibrant and bright.  The resolution is the
same as the HP's so there's not a lot to report there.  I haven't spent a lot
of time using the touchscreen, but it's definitely multi-touch capable and
there seems to be some support in ChromeOS (soft keyboard shows up when in
tablet mode, two-finger zoom, etc.) but I'm unsure how it will behave under a
Linux window manager.

Performance should be a considerable step-up from the HP, with twice the RAM
and a faster Intel processor, but here again I haven't spent enough time with
the machine to really notice a difference, which I suppose is a testament to
just how optimized for typical web tasks the HP machine was.

Now on to the areas where things could be improved.  The first thing I noticed
is that the tab key is unimaginably sensitive, so much so that at first I
thought it was telekinetic.  Just laying a finger on the tab key issues a tab,
you don't even have to depress it.  This is a significant concern since tab
plays a big role in my terminal-and-vim world, so I'm a little worried about
this.  Once I understood that it wasn't necessary to depress the key I've
adapted fairly quickly, but it's absurd to have a problem like this on a new
machine, and if not for the fact that it would take a month to get a
replacement, I'd probably return it based on this alone.

That's right, it takes a month to get one of these.  I'm sure it's a function
of supply and demand, or perhaps because Lenovo expects these to be ordered in
big batches for schools, but whatever the case it took a good four weeks for
the machine to show up after ordering it direct from Lenovo.  It might be
possible to get one faster through another distributor, but I was unable to
find another source for this specific model (Amazon carries similar ones that
ship with Windows but since ChromeOS is a prerequisite for the way I run Linux
that was a non-starter).  So to get this specific model, you probably have to
order direct, and Lenovo's website is now stating a five week wait for this
model.

That leads to the next disappointment.  The model I received isn't _exactly_
the model I ordered, and it's not clear to me the one I wanted is still
available at all.  The key difference between the two is that in the one I
received the Ethernet jack has been deleted, replaced with one of those
plastic "you didn't get the luxury package" plugs found in so many 1980's
American cars.  This was particularly frustrating to me since it was such a
differentiator compared to other Chromebooks, and furthermore the no-Ethernet
model lists for almost $100 less than the one I ordered on the Lenovo website.
I attempted to address this issue by contacting Lenovo which started with a
web chat where I was directed to contact the sales department via telephone,
who then directed me to tech support, whom I was eventually able to convince
that the [ two ](http://shop.lenovo.com/us/en/laptops/thinkpad/11e-series/11e-
chrome/) [ different ](http://shop.lenovo.com/us/en/laptops/thinkpad/11e-
series/11e-yoga-chrome/) models exist, who then directed me to post-sales who
was supposed to remedy the situation, but instead instructed me that they
could help me return the machine, issue a refund and then re-order the correct
model.  Since that would involve waiting at least another month I asked if
they could simply cross-ship me the replacement and they were completely
baffled by the concept.  After almost two hours of this I gave up, wrote down
my case number and hung up the phone.

More than anything else about the experience, this made me second-guess
selecting this machine, and I was very close to throwing in the towel and
returning it.  After taking a few hours to chill out I decided instead to
write this review and spend some more time with the machine before deciding to
give up on it just because it had bad parents.

Also it has those ugly, uncomfortable "Intel inside", etc. stickers, yuck.

After tonight I should have my regular development tools up-and-running and
I'll be able to give the machine a proper work-out.  After that I'll decide if
it's going to be worth keeping around, warts and all, or if it's worth trying
to keep the HP alive long enough to find a suitable replacement.  If it stays
I'll write a follow-up once I've had a chance to explore some of the areas
glossed-over in this review and decide if I can recommend it to others in
spite of the shortcomings described above.

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Lenovo Thinkpad Yoga 11e Chromebook - Part 2
date: Mon, 16 Feb 2015 11:49:06 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Having spent a few days using the Thinkpad for real work I wanted to follow-up
on my first impressions review and illustrate the reasons why I think this
machine is a keeper.

![IMG_1097.JPG](/preposterous/assets/31-img_1097.jpg)

First off all of the positive points from my first impressions post hold true.
The keyboard is really, really good (with one exception), and the more I use
it the more I like it.  The keys require more travel than most laptops but the
feedback is excellent (both tactile and auditory) so as you become accustom to
it typing speed increases dramatically.

It might seem weird that I make such a big deal out of the quality of the
keyboard, but as a programmer it's the part of the computer I use the most,
and with laptops you're pretty much stuck with whatever they come with, so
it's a serious deciding factor for me.  I can say that after having used the
Thinkpad for a few days worth of work that's it's the best laptop keyboard
I've used in as long as I can remember.

I've also noticed more the extra care that was taken to make the machine
physically tough.  I haven't tested this toughness yet, but as you handle the
machine you notice small touches like reinforced corners or rubberized edges
that make it easy to hold on to and provide protection in areas most likely to
experience impacts.  As I said before there is zero flex in the frame whether
you hold it by keyboard or screen, and when closed (or flipped around in
tablet mode) it feels more like a toolbox than a computer.

I didn't have a lot to say about the display in my first review other than it
looks nice, and I have to admit when I first opened it up it felt like there
was an excessive amount of bezel around the screen, as if the case was
designed for a much larger LCD.  I imagine to some degree that this
contributes to the durability of the machine, providing ample "crush zones"
around the otherwise fragile glass of the display.  Having used the machine
for a few days I've stopped noticing this altogether and there is an
interesting side-effect that I wouldn't have otherwise considered.  Since the
screen is surrounded by a good couple of centimeters all the way around the
bottom of the screen doesn't run all the way down to the hinge like on most
modern laptops.

This makes the bottom edge of the screen stop well above the tops of your
hands, and requires less of a downward angle to look at.  Since most of my
typing happens at the bottom of the screen (editors, terminals, etc.) I spend
a lot of time looking at this area and while perhaps not as aesthetically
pleasing as a display that fills the entire "lid" of the laptop, this
arrangement is much more comfortable during long coding sessions.  The whole
thing makes me reconsider the ergonomics of laptop screens and wonder if an
arrangement that places the bottom edge of the screen closer to where it would
be on a properly setup desktop workstation is worth exploring.

Speaking of the display, the touchscreen capabilities seemed to me to be
something of a gimmick when I was researching the machine but having used it
more now I can say that it actually comes in handy, even for the perhaps
unusual things that make up most of my work.  I found that when browsing
documentation it's easier to lay a thumb on the side of the screen and push
the page up to scroll vs. reaching down to use two fingers on the trackpad.

I've also found that the tablet mode is great for reading e-books as you get
something closer to an actual printed page worth of content compared to
regular-sized tablets.  This is especially nice when reading PDFs whose
pagination doesn't reflow very well on traditional e-readers.  Diagrams and
illustrations are also easier to read and adding annotations using a fat
forefinger are more precise, naturally.

![IMG_1095_turned.jpg](/preposterous/assets/31-img_1095_turned.jpg)

I'm interested in exploring the use of the touchscreen for drawing or diagram
work but I haven't found an application that I like for this purpose yet so
that will have to wait.  As of late I have been using [ A Web Whiteboard
](https://awwapp.com/) quite a bit and this might be an excellent use of the
touchscreen function.

I also expect the "tent" mode to come in handy in the laboratory as a way to
view reference material while my hands are otherwise occupied.

![IMG_1096.JPG](/preposterous/assets/31-img_1096.jpg)

Having used the system for some "real work" I can now say definitively that
performance is orders of magnitude above what the HP Chromebook 11 was
capable.  This comes as no surprise as the specifications alone belie this,
and higher performance should be expected given greater price of the machine.
That said this has less impact on my typical workflows as they are not very
demanding beyond the occasional need to compile something but in those cases
the time it takes is roughly 1/4 of what it took on the dual-core ARM-based
HP.

Where the increased performance and memory headroom (4GB vs. 2GB) can be felt
more is in the browser, where page render times are noticeably faster than on
the HP and it's possible to keep a lot more tabs open before feeling the
memory crunch.

I'm looking forward to throwing some more compute-bound loads at the Thinkpad
once I get my 3D modeling tools installed, but that may have to wait until I
sort out a storage issue that I'll get into in detail later.

One other thing I'll touch on is battery life.  During the time I've used the
Thinkpad I haven't seen the charge drop below 60%.  I have been working with a
mix of battery time and mains power so that might not be anything to get
excited about, but I can say that I spent a few hours coding, downloading
Debian package, compiling Node.js, Redis, etc. and other regular activities on
battery power without experiencing significant drain.  I was pretty satisfied
with the battery life of my HP Chromebook and was hoping to get similar
performance out of the Thinkpad.  So far that seems on-track, but a few days
on the road will be a better indicator than the tests I've done so far.

Another note about power.  While clever in theory, the HP's micro-usb power
connector was a bit of a hassle.  The idea of being able to charge the laptop
using any micro-usb power source was attractive but in practice if you didn't
use the HP-supplied charger you could barely charge the battery while using
the laptop.  Additionally the connector (like all USB connectors) has the
extraordinary power of always being wrong-side-up when you try to insert it,
and its diminutive size always made me afraid that I'd someday bump it and
tear the socket off the board.

The Thinkpads power supply, while having all the charm of any other power
brick does have a very robust connector and the nice feature of being
insertable with either side up.

There are other pleasant surprises, little details like the pulsing dot over
the "i" in Thinkpad when the lid is closed are fun and something of a
testament to the higher level of detail that goes into this machine vs. most
low-cost laptops.

So the good points are still good, what about the bad?  For the most part I've
been able to get over the problems I reported in the first review.  The lack
of a wired Ethernet port is still frustrating but not enough to give up the
good qualities of the machine for (that might be a different story if there
was a compelling alternative that possessed an Ethernet port).  Additionally
the Thinkpad's USB ports are USB3, so it might be possible to get a USB-based
Ethernet adapter that isn't limited to 100mbps.  Also the stickers came of
amazingly well with no residue, so the only real issue remaining is the tab
key.

The psychic tab key is a problem.  Having looked closely at it I think it's a
problem unique to this specific machine, and I suspect that it's mechanical.
If you look closely you can see that the tab key appears to be ever so
slightly depressed compared to the keys around it, so I wonder if what's
really happening is that it's always closed, and touching it releases it
momentarily.

I want to pull the cap off the key and see if perhaps something is just stuck
under there, but I can't find a definitive source that indicates the correct
way to do this (and more importantly, how to put it back together).  I assume
it's just a regular scissor mechanism but I don't want to pry the key off and
find out that I was wrong about that.

I did go through the motions of removing the entire keyboard and this is
another area where I'm impressed with the machine's design.  Much of the
device is field-serviceable and it's held together with quality screws that go
in and out with ease, unlike many other laptops which rely on tabs and
adhesives to hold things together (and never go back together quite right).

I'm pretty sure that a warranty claim could be made to get the keyboard
replaced and that this would take care of the tab key problem, but after my
first experience with Lenovo support I'm very hesitant to jump back into that
process.  I'd prefer to replace the keyboard myself if I could get them to
send me a replacement unit, rather than send the whole machine back to them
for service and get it back some indeterminate amount of time later.  I'm also
looking to see if they have a service provider locally that I could visit and
discuss this with and resolve the issue in a way that minimizes disruption.

Overall I recommend this machine to anyone looking for a durable, inexpensive
laptop who can live within the limitations of a ChromeOS machine.  I think it
also has great potential as a Crouton-based Linux laptop, but there are some
issues not specifically-related to this Thinkpad that I need to resolve before
I can recommend someone go out and buy one of these for that specific
application.  The good news is that the price has dropped as well, so if you
can tolerate the wait you can get a lot of laptop for the money.

If I can find a solution to the [ suspend-related issues of running Crouton
from removable storage ](https://github.com/dnschneid/crouton/issues/288) that
will make this a no-brainer for anyone looking for a low-maintenance, durable,
inexpensive and reasonably powerful Linux machine.  Until then however the
only easy way to run Linux is from the built-in storage which is in short
supply and not upgradeable by any reasonable means (at least not that I've
found yet).

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Local build test 1
date: Sun, 7 Dec 2014 23:38:25 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Just testing the local debug stuff for the first time in a long time.

---
title: Log
date: Sat, 9 Aug 2014 07:32:22 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/260-photo.jpg) ](assets/260-photo.jpg)

---
title: Mailserver Woes
date: Mon, 29 Dec 2014 13:27:28 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Running an Internet mailserver isn't what it used to be.

I found out today that Google is rejecting emails from Preposter.us.  Not
flagging them as spam, flat-out rejecting them at the gate.

Through digging into this I learned a lot about all of the crazy contortions
that email goes through today in order to make it from one keyboard to another
screen.  It's pretty wild, certainly a proper organism and I'd say that if
intelligence is going to become an emergent phenomenon on the Internet, the
seed crystal is going to be in the mail system.

Wonder and awe aside, I think I got to the bottom of the problem, an outdated
reverse DNS record, but I fear that anyone who may have attempted to try out
Preposter.us since the departure from Gmail may not be receiving notifications
for their posts.  If this is you, please try again and if you continue to have
trouble reach out on twitter to @jasonbot2000.

\- Jason

---
title: Might Drain Guard!
date: Sat, 1 Feb 2014 12:31:40 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ https://github.com/jjg/drainguard ](https://github.com/jjg/drainguard) Ok,
not the most glamorous thing I've made with my Reprap, but another practical
piece that took about 30 minutes to design and an 1.5 hours to print.

  

![Inline image 1](/preposterous/assets/203-jason's _img_0229.jpg)  

  

[ OpenSCAD ](http://www.openscad.org/) source & printable STL files are [
available on Github ](https://github.com/jjg/drainguard) : [
https://github.com/jjg/drainguard ](https://github.com/jjg/drainguard)

  

  

\- Jason

---
title: Migration Test 1
date: Wed, 18 Nov 2015 15:33:00 -0600
author: jjg
draft: false
tags:
  - preposterous
---
We're moving things around. This is just a test to see how things are going so
far.. // jjg

---
title: Migration test 2
date: Wed, 18 Nov 2015 15:41:17 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Something not quite right so this is another test of the migration. // jjg

---
title: Migration test 3
date: Wed, 18 Nov 2015 15:47:18 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Still having issues so firing off another test. // jjg

---
title: Mildred Pearson (RHaC Part 3)
date: Thu, 29 Oct 2015 14:40:19 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Proof-of-concept is working.

As predicted, it is slow, but I'm able to store & retrieve data without
actually storing the data anywhere.  It's tempting to immediately jump into
attacking the performance problem, but I'm doing my best to hold-off until I
understand every inch of what works and why.

After a somewhat long search for a hash function I settled on [ Pearson
hashing ](https://en.m.wikipedia.org/wiki/Pearson_hashing) .  It seemed to
have the essential properties while being fairly simple to understand and not
deliberately cryptographic.  I'm using a "textbook" implementation which can
probably be improved on, and I'm still studying it so I can effectively
improve it's performance in my test system.

This brings up the next interesting observation.  I have a hunch that it's not
the speed of the hashing function that is holding me back at this point but
the speed of the "combinational" code that generates all possible input data.
At first I thought this was a "permutation" problem and found a number of
discussions on optimizing permutation however my problem is a bit more complex
because the range of values isn't limited to some initial set, just the size
of the input.

I suspect I'm spending at least as much time in the combinational code as I am
hashing, if not considerably more.

So other than documenting current progress, next steps include making sure I
understand all of the parts of the working POC completely, and then inserting
timing measurements to establish optimization priorities.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Milestone
date: Tue, 17 Jun 2014 07:16:06 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
![](/preposterous/assets/246-screenshot 2014-06-17 at 7.10.03 am.png)  

​  

  

It may not look like much now, but it represents great progress...

---
title: Minneapolis 1
date: Fri, 9 Oct 2015 08:50:07 -0500
author: jjg
draft: false
tags:
  - preposterous
---
// jjg

[ ![](/preposterous/assets/80-img_0236.jpg) ](assets/80-img_0236.jpg)

---
title: Mixtile Gena First Impression
date: Mon, 10 Aug 2015 13:44:29 -0500
author: jjg
draft: false
tags:
  - preposterous
---
On a whim I ordered a [ Mixtile ](http://www.mixtile.com/) Gena from [ Tindie
](https://www.tindie.com/products/Mixtile/mixtile-gena/) because it looked
like a more open version of the guts inside my Pebble watch, and I'd love to
do things with my watch that it was never designed for.  It's also a third of
the price of even the entry-level Pebble, so I thought it was worth checking
out, even though no documentation was available at the time (I figured when it
showed up there would have to be some by then).

![IMG_2174.jpeg](/preposterous/assets/47-img_2174.jpeg)

I figured wrong :)

The box showed up surprisingly quick, but contained the device wrapped in
bubblewrap and nothing more.  I did a quick search to see if any more info had
shown up on the web since I ordered but no luck, other that a couple forum
entries from [ similar lost souls ](http://www.mixtile.com/forum/questions-2)
.

Who needs instructions anyway?

I plugged it in and turned it on (guessing that you hold the single button on
the left, since the controls are almost identical to the Pebble) and it
started right up.  Here's a terribly-shot video:

After a second or two a clock is displayed, and no surprise the top and bottom
buttons on the right of the unit scroll through a number of pre-installed
watchfaces.  Pressing the center button brings up a menu of features and of
course the one I'm most interested in, "Settings".

There's a lot of built-in "Pebblesque" features that look like they would just
work once the device is paired with a phone, but unfortunately there's nothing
in the settings indicating that this can be done.  There is a Bluetooth on/off
switch, and when you turn it on and scan for Bluetooth devices with your phone
something shows up that you can pair with, but it never reaches "Connected"
state, and the device continues to instruct you to "Connect the phone" to use
the phone-related features.

So at the moment there's not a lot I can do with it, but I'll keep poking at
it and also see if I can get something out of the support community.  To be
clear the device has potential, especially if the company comes through on
their promise to provide the device firmware and API in an open-source way.
It appears to be fairly well made and the screen is very nice in ambient light
(the back lighting seems a little weak but I haven't tested that extensively),
and works well in sunlight as well as regular artificial light.  Performance
is snappy with the built-in firmware and although I can't do a lot with it
yet, it doesn't have that sluggishness you get with so many cheap gadgets.

Even if developing software for the device doesn't materialize (or requires
some cumbersome toolchain) if the Gena does what it looks like it can do (once
properly paired with a phone) will do most of what I use my Pebble for, at a
price that will make it accessible to a much wider audience.  With a handmade
or 3D printed case & band the whole investment could easily come in under $40,
which is getting into Casio Databank range...

...and if Mixtile does come through with an open-source development kit, it
could not only open up previously imagined applications but it could also
become a watch that could be used with devices other than smartphones.  Other
phones (feature phones or even basic flips) could theoretically use the Gena
as a display for incoming texts, etc. and in the reverse as a controller input
for music playback, call management, etc.  This would significantly lower the
cost-of-entry in using a "smartwatch" and dramatically increase the world
audience for these devices, so it's an exciting proposition.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: More crossbow parts
date: Mon, 7 Sep 2015 15:53:15 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Working through some iterations of a quiver design.

  

![image1.JPG](/preposterous/assets/70-image1.jpg)

  

Work in progress on **Github** : [ https://github.com/jjg/xbow-accessories
](https://github.com/jjg/xbow-accessories)  
  
  
// jjg

---
title: More mail debu
date: Wed, 31 Dec 2014 12:06:38 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Just another test to work out the kinks in the mailserver.

  

  

\- Jason

---
title: MTA hardening test 1
date: Sun, 21 Dec 2014 11:49:36 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Let's see if this gets through. \- Jason

---
title: Multimedia no go
date: Fri, 13 Dec 2013 00:28:37 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
OK looks like that photo confused the code.  Need to learn more about decoding
emails.  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: My Preposter.us Plan
date: Mon, 15 Dec 2014 01:21:38 -0600
author: jjg
draft: false
tags:
  - preposterous
---
About a year ago, I began writing [ Preposter.us ](http://Preposter.us) (known
then as "Preposterous") to precisely fill my personal needs for a blogging
platform.  Now I'd like to refine and polish it into it's purest form, and
create a publishing system that is **perfect for 1000 users** .  I also want
to use it as a test-bed for theories I have on building better software.

  

The first steps of this plan are already underway. [ http://preposter.us
](http://preposter.us/) is now operational, serving [ Preposter.us
](http://Preposter.us) blogs on independent servers. [ Preposter.us
](http://Preposter.us) is no longer dependent upon Google for accepting email,
and eventually will manage it's own DNS as well, making its only dependency a
public Internet address.

  

As they say, "freedom isn't free", and with this independence comes the cost
of operating, monitoring and maintaining these systems.  However the real cost
of this plan lie in software development which, unlike servers and network
access does not tend to decrease in cost over time.

  

I intend to cover these costs by applying a variation of the **1000 True
Fans** ( [ http://kk.org/thetechnium/2008/03/1000-true-fans/)
](http://kk.org/thetechnium/2008/03/1000-true-fans/\)) approach.  Instead of
setting out to make [ Preposter.us ](http://Preposter.us) grow exponentially,
I plan to intentionally limit its membership to no more than 1000 users.  This
serves two purposes:  The first is that it allows the operational cost to be
predictable and decreasing over time.  The second is that it allows the
software to preserve its narrow, specialized feature-set; effectively
selecting only users for whom it is the right fit.

  

This second purpose is key because it is this deliberate, conscious limiting
of product scope that allows not only the operational expense to decrease over
time but also the software development expense, traditionally the more
difficult of the two to predict.  By focusing on a narrow set of features and
selecting only users who desire just these features the software development
work can be focused on improving these features to the point of perfection.

  

Based on a year's worth of observation of the [ Preposter.us
](http://Preposter.us) software in action, and my familiarity Internet
application development and operation, I have selected $10.00USD per user per
month as the initial subscription rate for [ Preposter.us
](http://Preposter.us) .  This provides a theoretical maximum revenue for the
second year of [ Preposter.us ](http://Preposter.us) operation and development
of $120,000.00USD; a modest amount in the current market.

  

After the second year, operational costs and projected development costs will
be reviewed and calculated and if the peak subscription count is reached, the
price per user will be reduced to reflect the reduction of costs.  As
operational and development costs are constrained, over time it's possible
that the overall cost of the system could become near or effectively zero.

  

**To be crystal clear,** [ Preposter.us ](http://Preposter.us) will continue
to be an open-source project, freely available to anyone who would like to
download the code and host their own server.  A subscription will only be
required to utilize the platform hosted at [ http://preposter.us
](http://preposter.us/) .

  

In addition to this subscription model, I intend to use [ Preposter.us
](http://Preposter.us) to experiment with the idea of users-as-investors.
It's all too common to see the time and effort users have invested in a
platform discarded by software companies when the time comes that they are
acquired by another company or otherwise sold or dismantled.  I propose that
using a piece of software, even if it is free, entitles you to some portion of
ownership.  For [ Preposter.us ](http://Preposter.us) , this ownership is
expressed in the ability for users to shape future product direction, and have
a significant ability to influence choices that will impact the future of the
platform such as sales, acquisitions, etc.

  

Simply put, paying users of [ Preposter.us ](http://Preposter.us) will have
the ability to vote on feature-level changes to the software and even on what
bugs or defects are repaired first.  Should there ever come a time when an
offer to acquire [ Preposter.us ](http://Preposter.us) is made, the body of
users will have a controlling stake in the product capable of blocking the
sale or alternatively purchasing [ Preposter.us ](http://Preposter.us)
themselves.

  

The exact details of implementing these proposed models is under development,
and it is the intention that these will continue to be developed as part of
the development roadmap for the second year of [ Preposter.us
](http://Preposter.us) 's operation.  As these are not the most common
approach to operating and developing a system there is work to do in
discovering the best way to carry them out, but to be clear the intent is to
design, develop and operate a system that is deliberately limited to an
audience size that allows it to be financially viable while constraining
software and product development scope sufficiently to generate the highest-
quality software imaginable with a predictable, decreasing software
development cost.

  

If successful, [ Preposter.us ](http://Preposter.us) could pave the way for
new software development business models that produce quality software in a
sustainable fashion without exploitation.

  

For the moment, there is existing work to complete before [ Preposter.us
](http://Preposter.us) will be ready to accept paid subscriptions (this is
expected to begin during the first quarter of 2015).  In the meantime, [
http://preposter.us ](http://preposter.us/) will continue to operate free-of-
charge for curious or adventurous users.  As a reward for their efforts during
this primordial stage of the platform, users who create blogs on [
Preposter.us ](http://Preposter.us) during this time will be grandfathered in
to the first year subscription program at no cost, so long as they maintain a
posting rate of at least one post per month.

  

To get started, email your first post to: [ post@preposter.us
](mailto:post@preposter.us)

  

I look forward to testing these theories together,

  

  

\- Jason

---
title: Negative action
date: Thu, 03 Sep 2015 07:52:50 -0500
author: jjg
draft: false
tags:
  - preposterous
---
This is an important one to me so expect to hear more about it later.

Something occurred to me awhile back when we were fighting against [ SOPA
](https://en.wikipedia.org/wiki/Stop_Online_Piracy_Act) , and it applies to
any policy or politics that are affect the Internet, or any technology in
general.

The frustrating thing about these policies is that they are often not defined
or ratified by the people most affected by them.  It's important to remember
that these people can't implement these policies themselves.

Past generations of my brothers and sisters have laid the tracks on which run
the most powerful engines of imagination in history.  They carry the
intellectual cargo of our past to the hungry minds of future generations.  The
name of this railway is _"The Internet"_ .

We information technology workers are the only ones who can actually make
these policies real.  Congresspeople can't configure a router, the CEO of the
phone company doesn't write code, and the President of the United States
doesn't haul cable.  I understand that not all of us may agree about the
sociopolitical aspects of how the network is used, but as engineers and
scientists we can agree on how the network itself should run.  By refusing to
make changes to the Internet that make it a worse _network_ , we can eliminate
the most sinister of these policies.  Moreover, we will be able to maintain
our integrity as professionals as well.

It is key that we do this together.  If everyone who possesses the skills to
manipulate the workings of the network holds themselves to this professional
standard, and refuses to pass these skills on to others unless they too uphold
this commitment, the future of the network is secure.  If not we are in for an
uphill battle, and energy will that could be used to make things better will
be wasted pushing against those willing to compromise the integrity of the
network for personal gain.

This is something we can do without force, without violence and without
bloodshed but instead by holding our power in restraint.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Nerdcon Day Two
date: Sat, 10 Oct 2015 10:57:23 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Learning a lot about authenticity.  I never looked at writing fiction as a
process of interacting with the characters in your story as _people_ .  This
is probably why I've never been able to write more than a few pages of
fiction.

  

Looking forward to applying what I've learned (hopefully I can remember it) to
some existing story idea as well as a few new ones that have come up during
the con.

  

The even has also helped me understand more about what works for me in regard
to interacting with groups of people and events like this.

  

Overall I've learned a lot so far.

  

  
// jjg

---
title: New attachment handling test 1
date: Sat, 14 Dec 2013 13:31:04 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a test to make sure we didn't break something by expanding the
attachment handling code.

  

This code avoids storing attachments we don't know what to do with, and makes
it easier to generate markup to support attachments of various sorts.

  

  

\- Jason

---
title: New attachment handling test 2
date: Sat, 14 Dec 2013 13:32:13 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Here's a post with an image attachment.

  

  

[ ![](/preposterous/assets/57-3dprintingwithreprap01_03.jpg)
](assets/57-3dprintingwithreprap01_03.jpg)

---
title: New Jack Slug test 1
date: Fri, 13 Dec 2013 23:39:30 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Just a normalish post to make sure we didn't break existing slugification.

---
title: New mailbox photo test 1
date: Sun, 14 Dec 2014 03:48:17 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/2-image1.jpg) ](assets/2-image1.jpg)

---
title: New slug's test 5
date: Fri, 13 Dec 2013 23:46:19 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
now let's try a troublesome title...

---
title: New slug test 2
date: Fri, 13 Dec 2013 23:42:26 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
second test of new slug code

---
title: New slug test 4
date: Fri, 13 Dec 2013 23:45:07 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 4 of new slug code

---
title: New way, what's this about a new way?
date: Sun, 14 Dec 2014 03:42:07 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
When the Droogs come out at night they speak a unique language that is fun to
hear but often horrendous to watch. \- Jason

---
title: Next Steps
date: Mon, 16 Dec 2013 05:28:46 -0800 (PST)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Now that 1.0 is complete, what I have in mind next for Preposterous is trying
to put it to use as-is on a daily basis.  This means treating it like my
primary blog, and looking for ways to migrate old blog content over and setup
a custom domain, etc.

  

None of this should be showstopper stuff, but I'm going to try to make a
conscious effort to keep my hands off the code for a bit (maybe up to a week)
and really feel what should be done next by using it and gathering feedback
from other users.

  

In the meantime if there's something you think belongs in Preposterous or you
find a bug, be sure to post it to the Issue tracker on the Github page (
https://github.com/jjg/preposterous ).

  

  

\- Jason

—  
Sent from [ Mailbox ](https://www.dropbox.com/mailbox) for iPad

---
title: Notification Test 1
date: Fri, 13 Dec 2013 08:30:37 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is the first test of the post notification feature

---
title: Notification Test 2
date: Fri, 13 Dec 2013 08:32:14 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Let's try this again...

---
title: Notification Test 3
date: Fri, 13 Dec 2013 08:33:09 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Third time's the charm?

---
title: Notification Test 4
date: Fri, 13 Dec 2013 08:34:10 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Fourth class?

---
title: Notification Test 5
date: Fri, 13 Dec 2013 08:37:14 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I think we're on 5 now...

---
title: Notification Test 6
date: Fri, 13 Dec 2013 08:38:25 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
and again!

---
title: Notification Test 7
date: Fri, 13 Dec 2013 08:39:28 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Is seven a lucky number?

---
title: Notification Test 8
date: Fri, 13 Dec 2013 08:43:54 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Testing a slightly more useful notification.

---
title: Notification test 9
date: Fri, 13 Dec 2013 08:44:49 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
In less than 10, I think we're ready.

---
title: Not quite
date: Tue, 01 Sep 2015 08:00:28 -0500
author: jjg
draft: false
tags:
  - preposterous
---
It was close but I didn't quite hit one-post-per-day last month (29 out of
31).

It became clear that one per day wasn't realistic, it's just too easy to
forget and forcing a post just to hit the number is kind of dumb.

That said one post per day on _average_ seems to be worthwhile, so I'll try
that this month.  I think having the arbitrary goal has helped me publish
ideas that have been banging around for awhile but I've held-back writing up
because they were incomplete or I felt I couldn't explain them well.  That's
OK I guess, but I think a blog is a good way to get those ideas out there even
if they're not perfect yet, and nothing stops you from re-publishing them once
they are in better shape.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Not slacking 
date: Tue, 15 Sep 2015 07:17:07 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Just busy printing. // jjg

[ ![](/preposterous/assets/72-img_0111.jpg) ](assets/72-img_0111.jpg)

---
title: OctoPrint - Control your 3d printer from anywhere using a web
 browser
date: Wed, 25 Dec 2013 06:29:52 +0000 (GMT)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ OctoPrint ](https://github.com/foosel/PrinterWebUI) (available on [ github
here ](https://github.com/foosel/PrinterWebUI) ) is an alternative "host"
program (the program that controls the 3d printer, typically a desktop
application running on a desktop or laptop computer) which is controlled
entirely via a web interface.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Main.png)

_OctoPrint Homescreen displaying status and temperature graph_

  
This means that instead of having to be physically nearby the printer to start
and monitor print jobs, you can submit them from anywhere* and keep an eye on
the progress of the print from any device with an internet connection and a
web browser.  
  
I've been looking for something like this ever since I started using [ my
Reprap ](http://www.gullicksonlaboratories.com/projects/reprap/ "RepRap") , as
the machine is located in the laboratory and I'm not always able to hang out
for hours to keep an eye on things.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Controls.png)

_Manual printer controls_

  
In addition to basic host features like loading files, monitoring print
progress and manually operating the various axis of the printer, OctoPrint
includes a few additional features to address the fact that you're not in the
same room with the printer to keep an eye on it.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Webcam.png)

_Monitoring a print job remotely via live video feed_

  
The first is the ability to display a live video feed inside the OctoPrint web
interface.  Essentially this passes-through a feed from an external program
that operates the web cam, but having it right in the same page makes it very
convenient to monitor all of the critical elements of the print job while it's
running and intervene immediately if you see something going wrong.  OctoPrint
uses an http request (just a web link) to get the images from the camera,
which has the added benefit of working with dedicated stand-alone web cameras
as well as cameras connected to other computers.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Timelapse.png)

_OctoPrint can automatically create timelapse movies of your print jobs_

  
In addition to displaying a live video feed, OctoPrint has a timelapse feature
which captures still images from the camera feed at regular intervals (time-
based or triggered when the printer begins a new layer).  OctoPrint then
collects these stills and assembles them into a video file automatically.  
  
Aside from being cool to watch, these videos can be extremely valuable when
tuning and troubleshooting the printer (or seeing what went wrong with a
failed print in the middle of the night).  
  
http://www.youtube.com/watch?v=8EbIO71Bi-M  
  
OctoPrint also keeps a copy of each file it's printed so you can easily re-
print something without having to upload the file again (important since
devices like the iPad can't upload local files via web page), and since the
design of the web interface is "responsive", it works well on any browser
regardless of screen size (at least every one I could test).  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Files.png)

_OctoPrint keeps a copy of the files you print for easy reproduction later_

  
Another nice thing about OctoPrint is that it was designed from the beginning
to work well on the [ Raspberry Pi ](http://www.raspberrypi.org/) , which can
be easily attached to the printer and make the whole setup self-contained.  In
my case I'm using WiFi on the Raspberry Pi as well, so my entire printer can
be operated with nothing but a power cable (and perhaps in the future, on
batteries alone).  
  
I've been using OctoPrint in its various incarnations since I first heard
about the project in a [ post to the Google+ 3D Printing Group
](https://plus.google.com/u/0/106003970953341660077/posts/GQmn9tSgfGP) by it's
author [ Gina Häußge
](https://plus.google.com/u/0/106003970953341660077/posts) around Christmas
2012.  I've been using it as my exclusive printer host for the last week or so
without incident.  I've used it for prints lasting more than 7 hours, and I'd
say at this point it's stable enough to be used as a replacement for my old
standby [ Pronterface ](https://github.com/kliment/Printrun) .  
  
As with anything, there are [ room for improvements
](https://github.com/foosel/PrinterWebUI/issues) , and while I can do things
like load filament by entering the extruder commands directly into the app's
terminal, it would be nicer if these functions were exposed via buttons on the
control tab.  
  

[ ](http://www.gullicksonlaboratories.com/wp-content/uploads/2013/01
/OctoPrint-Terminal.png)

_ When all else fails, you can always get it done in the terminal _

  
I'd also like to see the ajax hooks documented (so I could have other devices
interact with OctoPrint via http) and have the ability to have "callbacks" in
the form of URL's that are called when certain interesting events occur like
the completion of a file parsing, target temperature reached, end of print,
etc.  The good news here is that the developer has been extremely responsive
to both bug and feature requests while maintaining the discipline to keep the
program focused on its key objectives (i.e., avoiding "bloat").  
  
OctoPrint is a great example of the advantages of [ Open Source
](http://en.wikipedia.org/wiki/Open_source) software development and in a
later post I plan to outline some of the non-technical aspects of its
development to illustrate why I think OctoPrint could have only existed in an
Open Source environment and why it's important that we keep the future of 3d
printing (and all other technology) open.  
  
  
  
  
  
_*to access OctoPrint outside of the network your printer is attached to
you'll have to open up your firewall/router to let those requests in.
Currently OctoPrint doesn't provide a means to authenticate requests, so until
it does, this would probably be a "bad idea"._  
  

Sent from Evernote  
---

---
title: Octowatch
date: Sat, 8 Feb 2014 10:12:47 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Use your Pebble watch to monitor (and eventually, control) 3D printers that
use the excellent  [ Octoprint ](https://github.com/foosel/OctoPrint) host
software.

  

Octowatch is currently Alpha software.  For current status on the project, or
to submit comments, suggestions or bugs, visit the Github repository located
here:

  

[ https://github.com/jjg/octowatch ](https://github.com/jjg/octowatch)

  

  

\- Jason

---
title: Octowatch mentioned in Make Magazine
date: Wed, 17 Dec 2014 07:14:50 -0600
author: jjg
draft: false
tags:
  - preposterous
---
My open-source Pebble watch project Octowatch ( [
https://github.com/jjg/octowatch ](https://github.com/jjg/octowatch) ) was
mentioned in an interview with  Gina  Häußge,  creator of Octoprint ( [
http://octoprint.org ](http://octoprint.org) ) in volume 42 of Make Magazine.

  
![](/preposterous/assets/5-img_0654.jpg)

  

_Interview available online here_ : [ http://makezine.com/magazine/make-42
/interview-with-octoprints-gina-hausge/ ](http://makezine.com/magazine/make-42
/interview-with-octoprints-gina-hausge/)

  

Octoprint is web-based "host" software for controlling a 3D printer designed
to run on small Linux computers like the Raspberry Pi and let's you control
the printer using any device on the network with a web browser.

  

Octowatch uses Octoprint's API to extend a portion of this monitoring and
control to the Pebble smartwatch, allowing you to monitor (and soon, control)
the printer from your wrist.

  

For me it's very exciting to have my work show up in Make and especially when
it's a project that I feel came about the way I think all products should in
the future.  Octowatch came about to meet a personal need of mine, and is only
possible because of the open nature of Octoprint and the freely-available
Pebble development tools.

  

The whole experience has re-invigorated me to continue work on Octowatch and
to avoid hesitation when inspiration strikes for projects like this in the
future.

  

  

\- Jason

---
title: "ODROID GO"
date: 2019-02-07T18:32:29Z
draft: false 
tags:
    - odroid
    - odroid-go
    - education
    - hardware
    - arduino
    - esp
---

A [friend on Mastodon](https://fosstodon.org/@kelbot) turned me on to the The [ODROID GO](https://www.hardkernel.com/shop/odroid-go/).  It reminded me of another DIY gaming/hacking kit I'd seen except that it was 1/4 the price.  After reading about it I was really surprised that I hadn't heard of it before (I have some ODROID hardware and try to stay on top of this sort of gear).  I described it to Jamie and she told me to order one.  Now a week or so ago I said I wasn't going to start anything new [until the laser is on-line](/posts/thrashing/), but who am I to argue?

Since I didn't know what to expect, I ordered just the device itself (there are [many accessories](https://ameridroid.com/products/odroid-go-game-kit)).  I was pleasantly surprised by how quickly it arrived.

![ODROID GO front view](/odroid_front.jpg)

One of the coolest things about this device is how much documentation ODROID has provided.  It's a bit rough in places, but they've even included a "[coding camp](https://wiki.odroid.com/odroid_go/arduino/01_arduino_setup)" that walks you through several lessons in working with the hardware.  This combination of low-cost and decent documentation make it a good fit for teaching, and as Berty is homeschooled, we thought we'd give this a shot.

The device comes as a kit but there's no soldering or other technical tools or skills required to assemble it other than a small screwdriver.  Everything you need to test the device is included, but if you want to use the game emulator (part of the pre-installed software) you'll need a MicroSD card (at least 8GB according to the docs) and you'll have to hunt-down some ROMS.

![ODROID GO back view](/odroid_back.jpg)

However none of that is required to begin programming the device itself, and in fact you can get through the first 7 of the coding camp lessons without additional hardware.

What you *will* need is a computer; most anything with a USB port will do (ODROID includes documentation for Mac, Windows and Linux).

Programming is done using the [Arduino IDE](https://www.arduino.cc/en/Main/Software), and the documentation provides the information necessary to get this working with the ODROID GO hardware.  The heart of the device is an [ESP-32](https://en.wikipedia.org/wiki/ESP32) module so for the most part it's just like working with any other ESP-32.  ODROID provides a library for the GO which makes accessing the GO-specific hardware (LCD, buttons, etc.) easy.  As you might expect the hardest part is getting the Arduino environment setup correctly.

One the IDE is setup and talking to the device, the coding camp is a breeze.  The examples are straightforward and ease you into working with the device.  With a little guidance even someone with no programming experience could learn learn to program the ODROID GO.

![Berty writing code](/odroid_hacking.jpg)

What was slightly harder was reverting the device to the stock game-emulator software.  This gets into using command-line tools and burning firmware images and requires particular versions of Python, which can be tricky on some computers.  Since I do this everyday it was simple enough for me to run through the process on my Linux laptop, but after looking over the recommended procedure (which involved installing Brew, etc.), we decided not to go about this on Bert's Mac.

This device has a lot of potential.  Aside from being inexpensive and fun, the fact that the device is based on the ubiquitous ESP-32 means that programming skills learned on the ODROID GO can translate directly to a wide-range of applications.  Unlike other "learn to code" environments, almost everything you learn programming the ODROID GO can be re-used to program a wide-range of other ESP-32 based hardware as well as new devices of your own design.

The only way I would improve on this would be to make it possible to program the ODROID GO without requiring another computer.  Given the flexibility of the device I wouldn't be surprised if someone hasn't already found a way to do this, but if not its certainly something that could be done.

Aside from the educational potential, it's a great little game emulator in a compact, comfortable form-factor and available at a very reasonable price.  This makes it a great "gateway drug" for aspiring hackers.

---
title: Off-by-one errors
date: Fri, 13 Dec 2013 00:12:01 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
They are hard, but I guess if that just means that the system won't process a
single post during a cycle, there's worse things that have happened.

  

Also, would be cool if we could teach it to ignore signatures...  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: "OffGRiD Iteration One"
date: 2019-19-04
draft: true
tags:
  - hardware
  - offgrid
  - solar
---

The objective of iteration one is to add solar battery charging to iteration zero, facilitating extended off-grid operation.

Sticking with "the simplest thing that can possibly work" approach, the first step toward adding solar charging to OffGRiD is to replace the AC power supply with a solar panel that can provide a charging voltage that matches the voltage provided by the AC supply.

However most common solar panels of this size have a rated output voltage of 5 or 12 volts, and the S10-2 needs 20 volts DC to charge the battery.  I could build a custom panel to get closer to the required voltage, but that would stray from the "simplest" criteria for this iteration of the project.

Even with a custom panel the voltage won't be exactly what the netbook needs, and due to the nature of solar panels the basic panel voltage varies a *lot*, so some sort of regulation is required regardless of cell configuration.

Since 12 volt panels are cheap and easy to come by, I started by looking for a way to get 20VDC out of a 12VDC panel.  I found some inexpensive boost converters based on the [XL6009E1]() module that are able to convert the voltages generated by a typical 12 volt panel and boost them to 20VDC. This additional voltage doesn't come for free of course, and the cost of increased voltage is decreased current, dropping the alreay small amount of power avaliable from a panel of reasonable size.  In addition to this trade-off, more power is lost due to the imperfect efficiency of the boost converter (around 94%).

## Iteration One Cost
| Qty. | Price | Desc. | Notes |
|------|-------|-------|-------|
| 1 | $12.95 | Solar Panel | 5w, 12v, polycrystalline, Amazon |
| 1 | $2.73 | Boost Converter | 4 amp, 3-30VDC input, 5-23VDC output, Amazon |
| 1 | $0.84 | DC Power Connector | 5.5mm x 2.5mm, 2 amp, Amazon |
|   | $196.64 | Iteration Zero | |
|   | **213.16** | **Total** | | 


---
title: "OffGRiD Iteration Zero"
date: 2019-04-15T16:31:51Z 
draft: false
tags:
  - hardware
  - offgrid
  - solar
---


The goal of *OffGRiD Iteration Zero* is to quickly build an outdoor-usable computer suitable for writing (in English or Python or whatever).

![](/oi0_outside.jpg)

## Iteration Zero hardware 

* Lenovo IdeaPad S10-2
* Pixel Qi display
* 120 GB SSD 
* 2GB RAM Upgrade

The build centers around the 10.1" [Pixel Qi](https://en.wikipedia.org/wiki/Pixel_Qi) display.  Its dual-mode nature makes it perfect for this application, and as such the Pixel Qi has been my display of choice for every OffGRiD design I've made.  I thought the display was no longer available (which is part of why I stopped working on OffGRiD), but I was able to find some [NOS](https://en.wikipedia.org/wiki/New_old_stock) on eBay, so I picked one up for this project.


### Lenovo IdeaPad S10-2

![](/oi0_s10-2.jpg)

I chose the IdeaPad S10-2 primarily because it is [known to work with the Pixel Qi display](https://www.engadget.com/2010/07/19/how-to-install-pixel-qis-3qi-display-on-your-netbook-and-why/).  As it turns out the netbook has a number of additional qualities which make it well-suited for this application.  In particular, these include a decent keyboard, a fairly rugged chassis, radio killswitch and huge (for a [netbook](https://en.wikipedia.org/wiki/Netbook)) battery.  It also turns out to be a surprisingly easy machine to work on with user-serviceable access to the RAM and hard disk.

There were a few things that surprised me about the machine (not necessarily bad, just unexpected).  The first is the weight, it's really heavy for its size.  Most of that is probably the battery, but some of it might be attributed to thermal management (I don't know what the Intel Atom processor's power dissipation numbers are but the machine often feels warm).  I also forgot how machines with mechanical disks vibrate; not just when the disk is "seeking" but just the constant "whir" of the spinning drive.  I never noticed this before solid-state storage was common, but now it really stands out.

Speaking of the hard disk, I had also forgotten just how slow mechanical drives are.  To be fair, this disk is probably one of the worst even for its time (designed to be a low-power, low-speed laptop drive) and it's particularly bad when the disk has gone "asleep" and then has to spin-up to read a file.

Memory-wise 1GB is pretty small even for an older version of Ubuntu, and if you run more than on or two programs at a time, the O/S quickly resorts to swapping which reduces performance.  This performance drop is magnified by the slow mechanical disk.

The good news is that most of these problems don't really interfere with the primary goals of this project, and in the case of slow disk and "snug" RAM, both will be addressed by upgrades that are part of this iteration.


### Pixel Qi installation

Replacing the stock LCD with the Pixel Qi display was as simple as any other laptop LCD replacement, which is to say that it's a bit spooky and involves both the sensation of breaking things as well as a little bit of actually breaking things.

The first step is to shut the machine down and remove the battery.  This is not only good for safety reasons, but necessary to let the lid fold back enough to remove the display.

![](/oi0_battery_out.jpg)

The most stressful part is removing the bezel.  There are only two screws which means there are countless fragile plastic clips holding the rest of the assembly together.  There may be better ways to do this, but how I went about it is to carefully slide something thin, strong and ideally softer than the plastic along the gap between the front and the back of the lid.  Of course when this doesn't work a large flat screwdriver will do.

![](/oi0_bezel.jpg)

Once you've broken all the tabs on the left, right and top of the bezel there's two more at the bottom which fortunately don't need to be forced.  There's two slots for a flathead screwdriver to be placed turned carefully to release the remaining clips.  At this point the bezel should be free.

![](/oi0_frame_screws.jpg)

Next there's four screws which secure the back of the lid to the frame.  These don't put up much of a fight but be careful removing the last screw (there's a cable for the webcam that runs to the back of the lid, you don't want to put any unnecessary stress on that guy).  At this point you can lay the whole thing out like an open-face sandwich.

![](/oi0_lcd_tape.jpg)

OK I lied, the next part is the most stressful.  Before removing the original LCD from the frame, Remove its cable to prevent damaging it when the LCD is freed from the frame.  These connectors are delicate, and there's a piece of non-obvious (clear) piece of tape holding the connectors together.  I didn't trust myself to pull this tape completely off, so I *eased* it off the screen side of the connectors and left it on the cable side so I could re-use it on the replacement display.

![](/oi0_frame_screw.jpg)

Once the cable is, free removing the original LCD is just a matter of removing four screws on the edge of the frame.  Don't be fooled into removing unnecessary screws (I almost did this).

![](/oi0_hold_panel.jpg)

Mounting the new display is just the reverse of removing the original.  Hold the new panel in one hand while fastening the bottom two screws loosely, then pivot the screen into place for the last two screws.  This way you can snug everything up without worrying about twisting the frame or putting any harmful forces on the new display.

Remember when I said removing the connector was stressful?  I was wrong about that being the worst, because reconnecting it means putting some pressure on the most expensive component of the build (the new display), and LCD's are not fans of having too much pressure applied to them.  There's also so many fine pins in this connector, getting it lined-up right is as difficult as it is important.

![](/oi0_new_lcd_connection.jpg)

Very carefully, slide the cable side connector on to the Pixel Qi.  It puts up a little resistance, so very, very carefully apply force to the connector while holding the rest of the display as gently as possible (try to spread the load out to the edges).  I wasn't able to get the connector on as completely as I would have liked, but it felt secure enough and I taped it back in as-is (I'd rather have to re-do it than break the new display).

With the wiring connected I was anxious to see if the connection was good enough, so I *carefully* re-installed the battery and turned the machine on just long enough to see the BIOS screen.  Once this appeared successfully, I powered everything back down, removed the battery and finished buttoning it up properly.

![](/oi0_screeen_works.jpg)

Overall the process took about an hour.  Now that I know my way around it I could probably go a bit faster, but I wouldn't want to rush it if I didn't have to.  At the least I can say with some confidence that I could do the job again without much fear of causing unrecoverable damage to the machine.


### SSD and RAM Upgrades

Originally I had only planned to upgrade the display during this iteration of the project but while researching the S10-2 I came across a very inexpensive SSD drive to replace the stock hard disk.  Given the power saving and performance benefits of SSD over mechanical hard drives (as well as the user-serviceability of the S10-2) I decided to include this upgrade in the first iteration.

I also going to upgraded the machine's RAM from 1GB to 2GB (the maximum supported).  I *thought* the machine would come with 2GB but I must have mixed up the specs. with another eBay listing.

Replacing the disk and upgrading the RAM was pretty uneventful.  Both are easily accessible by unscrewing screws in their respective access covers on the bottom of the machine.

![](/oi0_hdd.jpg)

The only difficulty was in fastening the new SSD to the existing hard disk's tray.  The holes in the tray were slightly out of alignment with the thread holes in the SSD, so the screws used for the factory disk didn't fit right in the SSD.  However, with some "convincing", I was able to get two screws installed and reassemble the machine.

A quick check of the BIOS to make sure everything is recognized and it's time to reboot and install the operating system on the new drive.

These inexpensive upgrades make the machine feel much more responsive than before.  Getting rid of the hard disk removes some of the weight, heat and noise of the stock machine and I expect there will be measurable improvements in battery life as well.


## Iteration Zero Software

To keep things simple I found the latest 32-bit image on the Ubuntu website (16.04.6 LTS xenial) and performed a default install on the machine.  I could have found a more appropriate distro for older hardware like this, but I wanted to quickly determine if there were any problems and figure out how well the hardware was supported by Linux.  In the long run I'll put together an O/S build that makes better use of the S10-2's meager hardware.

That said I was surprised at just how well a "modern" stock Ubuntu ran on the little netbook.  As far as I can tell all of the hardware works (WiFi FTW!) and performance isn't bad as long as you're not hitting the disk too hard.  With only 1GB memory is tight, and I imagine if you were to run a few memory-hungry applications the O/S would start reaching for swap and things would go downhill quickly, but even with the stock hardware I'm able to comfortably write this post, listen to some FLAC files and have Firefox open to pull-up reference material.


## Iteration Zero Cost

| Qty. | Price | Desc. | Notes |
|------|-------|-------|-------|
| 1 | $57.70 | Lenovo IdeaPad S10-2 | Used, eBay, $35.00 + $22.70 shipping |
| 1 | $110.00 | Pixel Qi LCD Display | NOS, eBay, $110.00 |
| 1 | $18.99 | Dierya K1 120GB SSD | 2.5" SATA II, Amazon |
| 1 | $9.95 | 2GB DDR SDRAM | generic PC2-5300 667MHZ, Amazon |
|   | **$196.64** | **Total** | |


## Results

Initial testing shows everything to be working correctly.

Over the next few weeks I'll be spending a lot of time working with this machine and gathering statistics on battery life as well as taking notes on usability, versatility and durability.  The objective will be to establish the performance envelope of this iteration and experiment with the software side of things to find an optimal configuration that provides the most utility while maximizing the primary goal of outdoor, off-grid computing.

---
title: "OffGRiD Iteration One Software"
date: 2019-07-01
draft: false 
tags:
  - OffGRiD
---

So far this post is a very unorganized collection of notes taken while experimenting with different operating systems, window managers and desktop environments on OffGRiD Iteration Zero.  The objective is to find a configuration which makes the most out of the hardware for its intended applications without requiring an obscene amount of configuration and tweaking.

## O/S Testing Summary

| O/S | Version | Results | Notes |
| --- | ------- | ------- | ----- |
| Ubuntu | 16.04.6 | Everything works and works well |
| Ubuntu | xxx | Everything works, but slow | | 
| Hauku | xxx | Locks up at boot screen | |
| ElemantaryOS | 5.0-stable | won't boot | 64-bit only |
| Debian | 9.8.0 | WiFi not recognized even using "nonfree"? | live, nonfree, Cinnamon desktop |
| Arch | 2018.07.01-i686 | N/A | untested |

Given the amount of pixels avaliable on the display, I think a full-screen-oriented "desktop" is more suitable than one which is oriented around a number of small windows.  To this end I've found a handful of Linux GUI's which might make a good choice.  Of course any "desktop" can be configured to work this way, but an environment designed intentionally to run things full-screen is likely to do it better and with less up-front effort.

Another key consideration for the software is minimizing resource consumption that may relate to battery usage.  Given the "Off" in OffGRiD, anything I can do to reduce power consumption extends the machine's usefulness, and makes it easier to power the machine from off-grid sources like solar.  The extreme approach to this would be to hand-assemble the entire software "stack", optimizing everything from the kernel-up to use only as much power as necissary to run the intended applications.  At the other end of the spectrum is using something that "just works" like Ubuntu, and settle for the energy efficiency that comes out-of-the-box.  After a lot of consideration I've decided to pick something in-between these two extremes for OI0 and use an existing Linux distro but customize the installation to reduce the amount of unecissary resource consumption.

Something else I'd like to explore along the way is how I might leverage spare resources (such as extra RAM) to further reduce power consumption.  For example, could RAM be configured as additional disk cache to reduce SSD power consumption (and if so, would this be worthwhile)?  Or perhaps the use of precomputation when the machine is connected to a power source to reduce off-grid computational electrical consumption?

Based on the above I though the right fit for this stage of the project was Arch Linux 32.  However after spending a solid hour fighting chicken-and-egg problems trying to get the WiFi to work, I started to rethink my priorities...
 

## References

* https://www.linux.com/news/best-lightweight-linux-distros
* https://cdimage.debian.org/images/unofficial/non-free/images-including-firmware/current-live/i386/bt-hybrid/
* https://wiki.debian.org/bcm43xx
* https://www.archlinux.org/
* https://i3wm.org/docs/
* http://openbox.org/wiki/Help:Getting_started
* https://en.wikipedia.org/wiki/Cinnamon_(software)
* http://www.windowmaker.org/
* https://wiki.archlinux.org/index.php/Installation_guide
* https://wiki.archlinux.org/index.php/Wireless_network_configuration#Troubleshooting_drivers_and_firmware
* https://wiki.archlinux.org/index.php/Broadcom_wireless
* https://wireless.wiki.kernel.org/en/users/Drivers/b43#list_of_hardware
* https://bbs.archlinux.org/viewtopic.php?id=245145

---
title: ok then
date: Fri, 13 Dec 2013 00:26:41 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
seems like some of my messages are not getting through...  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: On cloud computing
date: Mon, 6 Oct 2014 14:07:54 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I've been making an analogy lately between IBM's predicted world need for
about 5 computers and the current cloud computing situation.  Turns out I'm
not the only one looking at it this way, in fact this article takes the same
position, interestingly _before_ the rise of the clouds:

  

[ http://www.theguardian.com/technology/2008/feb/21/computing.supercomputers
](http://www.theguardian.com/technology/2008/feb/21/computing.supercomputers)  

  

There certainly are a lot of Utopian ways to view this model, but I think
seeing this from the 1940's IBM perspective sheds important light on the
subject.

  

IBM didn't see a world market for 5 different computers, IBM saw a world
market for 5 IBM computers.  This might sound obvious or the meaning subtle,
but if you're familiar with how companies used to own IBM mainframes (which is
to say, they didn't) you'll appreciate why the distinction matters.

  

Consider the personal computer revolution.  Why was it a revolution?  Before
personal computers the average person was unlikely to interact directly with a
computer on a regular basis.  If they did, it was likely that they were a
data-entry professional or possibly an "operator".  It was very unlikely that
they were a programmer, as the number of programmers worldwide at this time
would be unlikely to fill a reasonably-sized conference hall.  The computers
that exists were largely leased from IBM or a handful of smaller manufacturers
or were custom purpose-built machines by universities or private research
institutions.  The range of applications for these computers amounted mostly
to accounting systems or other business information management like tracking
insurance policies and the like.  There were some sites where scientific
computing was happening but these were mostly at universities and only
graduate students were allowed any sort of creative access to the machines.

  

In 1971 Intel created the first mass-produced microprocessor, which bundled
the most interesting parts of a stored program computer into a single chip.
For some reason they released this at a price-point that was accessible to the
typical electronics enthusiast (most of which were building HAM radios at the
time) and a few of them began building their own computers.  Given the
scarcity of computer resources at the time, there was enormous motivation for
anyone who wanted to do something new with a computer to make these projects
successful.  It's hard to imagine what that would have felt like, the ability
to suddenly have unfettered access to something that otherwise was locked-up
in universities or enslaved to the most boring and monotonous tasks.

  

For the most part the established computer industry ignored the whole thing,
as it was obvious that these toy computers would never become capable of
competing with the "Big Iron" that was IBM's bread-and-butter at the time.
However there were a few companies paying attention (some growing right inside
the garages and basements of these early microcomputer hackers) and it was
these companies that would have the vision to bring the microcomputer to the
masses in the early 1980's.

  

Suddenly machines that just five years earlier were only available to the
richest private companies and most advanced university students were being
placed in the hands of laymen and even children.  The number of programmers
exploded, and along with it the breadth and depth of computer applications.
Over time these toy computers gained strength and began to penetrate the
professional world as well as a generation of hackers who had grown up with
entire computers at their disposal were not going to suddenly go back to
sharing a single machine with the entire company.

  

My friend Alan summed this up nicely during a word processing class in tech
school.  He was assigned a workstation that shared a single dot-matrix printer
with two other stations and was told the reason was that he kept picking the
station with a dedicated laser printer.

  

The instructor explained, "What if you work for a company that only has one
dot-matrix printer?  You'll be glad you know how to work with a shared printer
then!"

  

to which Alan replied: "I'm not going to work for some poor-ass company that
can't afford to buy me a laser printer".

  

This might sound quaint (especially in an era where a printer can be purchased
for less than the price of the ink it consumes) but the point it illustrates
is clear, once someone has experienced the power of dedicated resources they
resist going back, and the experience they have gained gives them a choice
about where they spend their time and talent.

  

But what does all this have to do with cloud computing?  As we consume more of
the conveniences of the shared computer we simultaneously loose the control,
autonomy and expressiveness of the personal computer.  The needs of the many
outweigh the needs of the few or one, and increased security and stability are
required of the cloud that compromise the openness and flexibility that
allowed new uses and applications to emerge from the personal computer that
simply were not possible in the pre-microcomputer revolution world.  A more
sinister aspect of this consolidation is that is is happening under the
corporate umbrella of a very small but very rich collection of companies,
companies whose bread-and-butter is just the kind of technology that emerges
from the creative use of computers.  Can it be proven that these companies
will use this to their advantage in order to squelch competition that might
arise from their own clouds?  Perhaps not, but history indicates that any
other outcome is unlikely.  Furthermore malice aside, large monolithic systems
such as these are know to develop systemic vulnerabilities due to their lack
of internal diversity, and as we become ever more dependent on these services
(either directly or by using services built on top of them) we begin to drive
the statistical probability of a catastrophic failure to 100%.

  

Fortunately there are alternative architectures that can provide the key
conveniences of cloud computing while eliminating the costs and risks outlined
above.  However the first step in pursuing these is to raise awareness of the
true cost of cloud computing in its current form, until then the viability of
other approaches will be artificially deflated compared to the "blinders on"
view of cloud computing as it exists today.

  

  

\- Jason

  

---
title: One is the lonliest number
date: Fri, 13 Dec 2013 00:31:46 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Lets see if we fixed that "one message won't go through alone" bug...  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: one more test of automated processing (3)
date: Sun, 15 Dec 2013 23:37:40 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is test 3 of the automated processing bug.

---
title: One more test tonight
date: Sun, 14 Dec 2014 04:00:15 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/3-image1.gif) ](assets/3-image1.gif)

---
title: Palm Pre vs. ZTE Open
date: Thu, 9 Jan 2014 22:55:56 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
My friend Matt asked me what I thought of the Palm Pre 2 compared to my
Firefox phone (a ZTE Open).  It was more than I could fit in a Twitter
message, so thought I’d turn it into a blog post.

  

In a way the two phones are very similar in that they both provide a platform
which treats HTML5 web apps like first-class native applications.  On the
other hand, one (the Pre), represents the end of the line, the most mature
version of a product with this vision.  The Firefox phone represents the
beginning of a similar vision.

  

The Pre 2 (and WebOS) is polished and sophisticated, but frozen in 2010.  It
has clever, well-thought-out interactions and beautifully designed graphic
elements.  Performance is good, on par with other devices of it’s time and the
hardware fits together well and is solid and robust.  The Pre 2 was a premium
device and it shows, and includes features like inductive charging that are
still considered exotic in most phones.

  

The ZTE on the other hand is a primitive device, designed to be inexpensive.
It’s well-made compared to other phones in its price range, but that said it’s
far from luxurious.  The processor is slow, the camera is feeble and the
touchscreen is limited to one touch at a time.  Even so, the device performs
admirably given its humble components and is surprisingly useable for a device
that can be had for less than $100 USD without a contract.  Hardware aside,
the real star of the show is Firefox OS.  Like WebOS, FFOS gives web apps
access to device features that other platforms reserve for native applications
only.  This gives web developers much more creative latitude than other
platforms, and opens the door for mobile software that is truly device
independent.

  

So which is better?  Well, if nothing else you could say that the ZTE Open w/
Firefox OS is the right choice because the Pre is a dead-end product.  While
WebOS does continue on as an open-source project, and there are some companies
intending to release products based on it, they are things like television
sets and at the moment it looks like there’s no plans for another WebOS phone
from anyone.  Additionally, many of the existing WebOS apps no longer function
because they are built against older versions of service API’s that have been
depreciated, so for example while there are several great Twitter clients for
the Pre, none of them work with the current version of Twitter’s API.  This
doesn’t mean that the Pre is no longer useful, but unless you plan to maintain
the code, there’s no guarantee that the apps you use will continue to work as
time goes on.

  

On the flip side, there _is_ an active open-source community producing new
code for WebOS, and even a package management system (“ [ Preware
](http://preware.org/) ”) that makes finding and installing software from
these efforts easy.  So for the intrepid hacker, the Pre can be a useful
platform even if mainstream developer support has waned.

  

Conversely, the ZTE Open provides a basic platform for learning how to build
the mobile web apps of the future.  It’s clear that the Mozilla foundation
intends to back the Firefox OS effort for the foreseeable future, and as the
constraints of less-open platforms like Android and iOS become visible to more
and more users, adoption of FirefoxOS will grow.  This, coupled with the low
cost-of-entry for these devices, mean that time spent learning how to build
apps for FirefoxOS will likely be rewarded down the road.  If not in the hands
of well-off high-end smartphone consumers, then in the hands of those who are
first entering the smartphone market and others who see the platform as a
tonic to the aches and pains of other more established platforms.

  

  

\- Jason

  

  

  

  

---
title: Pancake bot
date: Sat, 28 Jun 2014 07:33:53 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/255-photo.jpg) ](assets/255-photo.jpg)

---
title: Parts
date: Wed, 16 Sep 2015 08:29:54 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Bet you can guess what this is going to be... // jjg

[ ![](/preposterous/assets/73-image1.jpg) ](assets/73-image1.jpg)

---
title: "PAX - Personal Assistant X"
date: 2018-09-24
draft: true
tags:
  - ideas
---

Notes from an early morning:

* Conversational email interface ("do you have this file?"/"Here's some matches"/"Send me the third one")
* Video annotated using computer vision on each frame to identify objects, people, text, etc. and count frequency, proximity, etc.
* Personal Assistant X, consumes and indexes all content you create, proxies account access, etc.
* Creates a composite timeline across all activiteies (apps, devices, etc.)
* Parts: archive, search engine, journal, classifiers, account interfaces, analytics
* Book reader (or interface into existing reader provided sufficient API) adds books to library, logs reading time in the journal, captures highlights, bookmarks, etc.
* Capture media in -> mood out relationships (reactions to posts, etc.)
* Intervention prompts ("how are you doing?", "are you OK?", etc.)
* A desklamp (or just bulb?) as an interface to physical media (image capture, video/audio recording, other physical data)
* vector capture pen/marker
---
title: Permission issue test
date: Fri, 13 Dec 2013 05:24:11 -0800 (PST)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Now testing to see if updates no longer create duplicate index entries...

---
title: Pipeline
date: Mon, 9 Jun 2014 08:13:51 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
**TL;DR**

Pipeline tells you exactly and only what you need to do next.

  

**Background**

I've been working on systems to keep track of what I'm doing and what I've
done since the time it was important to others that I do anything (before that
I just blundered around doing whatever came to mind).  Over the years I've
developed a number of manual, automatic, physical and electronic systems, each
one a refinement of the previous.  During this time I've learned a lot of
about what works and what doesn't (at least for me) and based on my experience
working with other people and systems, I think that I'm not alone in this
quest for such a system.

  

The latest incarnation of this is something I call "Pipeline", and it's
currently in the design phase.  Simply put, at any given time Pipeline
presents you with the thing you need to work on next, allowing you to focus on
only two things at a time:

  

  1. What you are doing now   

  2. What you are doing next   

  

There's two specific advantages to this myopic approach.  The first is that
there is zero ambiguity about what to do at any given time; as soon as you
finish what you're doing, you consult Pipeline and it tells you exactly what
to do next.

  

The second is that there is almost no cognitive overhead associated with
thinking about anything other than the task at hand.  Worst-case you're
distracted by thinking about the next task (if this becomes a problem Pipeline
could even hide that task until you've committed completion of the current
one, nut my experience leads me to exposing the next task whenever the system
is consulted).  What this prevents is meta-thought about the prioritization of
all tasks, and a form of premature optimization that can be crippling (and is
rarely beneficial).

  

**Where do tasks come from?**

The "next" task is drawn from a (if you're like me, bottomless) pool of tasks
that are populated ad-hoc by you or anyone else you trust.  Pipeline's job is
to handle prioritizing these tasks based on any source of information it can
draw from.  I'm not ready to go into detail about this part yet, but suffice
to say it's intentionally simple, and leans heavily on a negative feedback
loop, like all good cybernetic systems.

  

**What if I can't do the next task?**

Sometimes Pipeline will pick a task you can't (or don't want to) do right now.
In those cases you simply "decline" the task, and it's returned to the pool.
Declines are recorded, along with any information about the context of the
decline that are available and this negative feedback is used to influence the
prioritization of all tasks.  Pipeline doesn't allow of infinite
procrastination though, and at some point you'll be forced to accept a task or
it will just keep coming back*.

  

Status

As mentioned earlier, Pipeline is the latest incarnation of a system I've been
developing and building for decades, and will continue to evolve over time.  I
do have an implementation in mind for this specific iteration of the idea, but
not a specific implementation timeline.  That said, if you're interested in
helping out [ tweet me ](http://twitter.com/jasonatmurfie) and we can talk
about some of the implementation-level details I have in mind and maybe write
a little code.

  

  

\- Jason

  

---
title: pirst fost
date: Thu, 12 Dec 2013 22:10:41 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is the first post to the preposterous blogging platform.  

  

  

  

\- Jason

---
title: "Please, email"
date: 2018-12-04
draft: false
tags:
  - communications
  - technology 
---

At a time when there are dozens of ways to communicate over the Internet, email may seem quaint or antiquated.  It's often the go-to example of weak privacy ("never put anything you wouldn't want your mother to read in an email!"), but in light of modern threats to online privacy, email may be the only method of communicating on the Internet you should trust.

# but email isn't encrypted!

By default no, Internet email is not encrypted.  This might lead you to believe it is less private than other services which provide "strong end-to-end encryption by default", but it's important to understand exactly what this means and how it can be subverted.

When you send a message to a friend the message is necessarily not encrypted as you author it (otherwise you couldn't read it on the screen).  When you send an encrypted message, this "cleartext" message is then turned into "cyphertext" using a "key" and transmitted to your friend who then decrypts the message using a key back into cleartext.

This sounds straightforward but there's a lot of ways it can go wrong.  First if the system you're using can read or store the cleartext message before you send it, anyone with access to that system can read your message.  If this system is is a website running on a server you have no control over, that could be a lot of people.

The same problem exists on the receiving end.  If your friend reads the message on a system you don't control, the decrypted message could be read by anyone else with access to the system.

The bottom line is this: whoever holds the encryption keys can read your messages, and in most "secure" messaging systems, the system holds the keys.

What this comes down to is that if you are to have secure, private communications you need to ensure that the only people who have the keys needed to decrypt a message are the people who you want to read it, and that the decrypted messages only exist on systems that you or your trusted recipient have control over.  Email using public key encryption is the only system I know of that can provide this level of privacy assurance.

But what about day-to-day communications?  Surely encrypting every message and only communication with people you have exchange keys with isn't always practical?  Here again email provides a good solution.

A trusted and properly-configured email server along with properly-configured clients can ensure communication between users of the server are encrypted in transit with no specific effort on the part of the users.  Furthermore, communications between such servers enjoy the same level of protection in transit.

The key here is "trusted and properly-configured".  If you are sending email through a server or service you can't trust, then email'a automatic encryption is fairly meaningless.  For example if you use Gmail, every email you send (and receive) is read by Google AI and used for advertising purposes.  On the other hand if you use your own email server (which is a lot easier to run than most people think), you can get a reasonable level of privacy without resorting to public key encryption.

## What if I don't have anything to hide?

Beyond privacy, email allows participants to choose a method of communicating that suits them.  Email is a *protocol* not a product, and as such there are a wide-range of clients and servers designed to suit a variety of users.  Using an email client of your choosing allows you to write in a comfortable environment free of distractions.  Email doesn't even require an always-on Internet connection so you can compose your messages off-line, and only receive messages when you feel like it.

There are many other reasons to choose email when communicating on the Internet (perhaps I'll do a series that drills deeper into each area?) but for these any many other reasons, when you want to have a meaningful, private electronic conversation with me, **[please, email](mailto:mr@jasongullickson.com)**.---
title: "Portastudio"
date: 2018-12-09T08:00:00Z
draft: true
tags: 
  - music
---

I came across the [TASCAM DP-006](https://tascam.com/us/product/dp-006/feature) the other day and it got me really excited.  It might seem dumb to spend $150 on something you could do with a laptop and some free software, but I've never had much luck using computers when it comes to making music.

This might be surprising given the fact that I'm a programmer, and I understand the practicality or using general-purpose computers to replace expensive and bulky physical devices, but for me software tools like that get in the way of the creative process when it comes to music.

"Back in the day", I was charged with wrangling the electronics as well as my duties as rhythm guitarist.  In addition to live sound I also managed recording and managed to cobble-together a usable "studio" out of thrift-store finds, scavenged home stereo components and the occasional piece of "legit" music equipment (the jewel of my setup was a six-channel RadioShack mixer).  This worked and got the job done within our non-existent budget, but it pretty much limited us to recording everything at once, live, with little ability to fix mistakes without starting over.

Studios solve this using multitrack recording, where each instrument is recorded alone.  This way if someone makes a mistake, or comes up with a better way to perform a part, only that single "track" needs to be re-recorded.  It also allows you to mix the tracks together in various ways without requiring the band to repeat the performance over and over. As the person doing this mixing, this multitrack capability was very attractive.

However time in a studio is expensive and completely out-of-reach for us at the time.  Enter the "Portastudio": everything you need to do multitrack recording anywhere for less than $1000.00.

For less than the cost of a few hours in the studio, you could purchase a Portastudio and have access to multitrack recording capabilities 27x7!  Not only this, you could bring the studio to the band, which saves a lot of time and money packing, hauling, setting-up, etc.  It also means you can take time to experiment and play with the equipment in a way that would be hard to justify when the meter is running.

I was pretty sure that if I could get my hands on a Portastudio all of my technical problems would be solved and we'd be producing an endless stream of amazing albums.  As I've since learned there's plenty of technical problems with these machines but that never came to my attention because even the least expensive of these devices was out of my teenage dishwasher metalhead reach.

I've since learned that cassette-based multitrack are not the panacea I imagined them to be, which is part of the reason they have been replaced by digital recording and in many cases replaced by software running on general-purpose computers.

So why get excited about a device that is essentially a throwback to the bad-old-days of limited numbers of tracks, no effects and more hardware to carry-around?  
---
title: Post ajax post index test 2
date: Sun, 15 Dec 2013 12:57:06 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
A second test of what happens when we do an update using the ajax indexes.

---
title: Post attract test 1
date: Sun, 21 Dec 2014 13:15:58 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/8-img_0687.jpg) ](assets/8-img_0687.jpg)

---
title: Post Host Upgrade Test
date: Fri, 23 Oct 2015 08:45:57 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Just checking to make sure everything made it thought the O/S upgrade.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Post index not getting updated when it should
date: Fri, 13 Dec 2013 08:59:22 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
For some reason it seems that the post index isn't getting updated when a new
post goes in.  This is a test of that.

---
title: Post index not updated bugfix test post
date: Fri, 13 Dec 2013 09:03:04 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Looks like this bug is fixed.

---
title: Post-migration autonomous processing test 1
date: Wed, 18 Nov 2015 15:49:47 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Another migration test. // jjg

[ ![](/preposterous/assets/5-image1.png) ](assets/5-image1.png)

---
title: post timestamp test 2
date: Fri, 10 Jan 2014 09:02:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 2 of timestamp display

---
title: Post timestamp test
date: Fri, 10 Jan 2014 08:50:44 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Just testing the new post timestamping feature. \- Jason

---
title: Preposterous 1.0
date: Mon, 16 Dec 2013 00:46:44 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I'm happy to announce that Preposterous 1.0 is now available!

  

A number of features have been implemented and many bugs have been put to bed.
There's also been some minor architectural changes that allow Preposterous to
be a little more sophisticated (like, rendering non-incomplete HTML) but also
retain the spartan-by-design spirit.

  

Notably, the index pages (the server index as well as the post indexes for
individual blogs) use an ajax-style request to load the contents of their
lists.  If this sounds too fancy, don't panic, the static lists are available
in the files **/blogs.html** and **/ <blog id>/posts.html ** respectively.
This lets the index pages be written in complete HTML without having to parse
and update them each time a new blog or post is written.  This also opens the
door for customization of these indexes by clever hackers.

  

Additional points of interest include support for text, html, image (jpg, png,
gif), audio (mp3, ogg, wav) and video (mp4, mov) posts, and a "reload" flag
for the script that allows server operators to update all static content on
the site from the original IMAP source.

  

To see what's being considered for the next release, visit the [ Issues list
](https://github.com/jjg/preposterous/issues?page=1&state=open) of the [
Github repository ](https://github.com/jjg/preposterous) and stay tuned to [
@preposterous_me ](https://twitter.com/preposterous_me) on twitter for
updates.

  

Also I'd like to thank everyone who's shown interest in the project and
provided encouragement to see this through.  I'm excited to see what you do
with it!

  

  

  

\- Jason

  

  

_p.s. if you'd like to give Preposterous a try without setting up your own
server, you can join the beta site by sending a post to[
preposterous1984@gmail.com ](mailto:preposterous1984@gmail.com) to get
started. _

---
title: "Preposterous Archive"
date: 2018-12-11T15:00:00-06:00
draft: false 
tags:
  - archives
  - preposterous
---

I've sucessfully [imported the archives](/tags/preposterous) from my longest-running [Preposterous](https://gitlab.com/jgullickson/preposter.us) blog!

There's still some kinks to work out (in particular, a bunch of posts are convinced they are from the year **0001**), but the script works well enough that I didn't want to leave the content offline until I worked out the last few kinks.

Most of this content hasn't seen the light-of-day for years and it's very exciting for me to see some of it again.  A lot of it is weird debugging posts I made as I was developing Preposterous so it may be of little interest to others, but for me it's kind of cool to see the project evolve.

Even though I stopped pursing it, Preposterous is a project I'm very fond of.  It's a great example of what you can do with very little resources when sufficiently enthusiastic.

Hopefully these posts will be of some interest to others as well.  They are listed chronologically alongside [the other posts](/posts/) on this site, or you can get an index of the imported content only by viewing the [Preposterous tag](/tags/preposterous/).
---
title: Preposterous, not just for humans!
date: Thu, 12 Jun 2014 07:37:36 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
A novel use of [ Preposterous ](https://github.com/jjg/preposterous) came to
mind this morning.

  

Since all you need to do to create a Preposterous blog is [ send an email
](http://preposterousme.com/oregon-king-uranus-autumn/how-to-use-
preposterous-1-0.html) , any system capable of sending email can create a
Preposterous blog.

  

A number of systems I'm responsible for are capable of sending notifications
and logging events via email.  These emails are sent to groups of individuals
which works OK, but it can flood the ol' inbox, and it's also not great if you
need to grant someone access to the event/log history who hasn't already
received all the messages.

  

By adding [ preposterous1984@gmail.com ](mailto:preposterous1984@gmail.com) to
the destination for these emails, you get a nice little chronological record
of these messages and events that anyone can review.

  

Of course care must be taken not to send anything sensitive to these blogs (as
they are public), but that's good advice for both humans and machines.  If
this application becomes popular I'd consider implementing measures to hide
these posts from the blog index but I don't know that I'd ever add support for
truly private posts on [ preposterousme.com ](http://preposterousme.com) (for
that I'd recommend running your own server, which is a [ pretty low hurdle
](http://preposterousme.com/oregon-king-uranus-autumn/preposterous-on-the-
raspberry-pi.html) itself).

  

If your machines create a Preposterous blog I'd love to hear about it!

  

  

\- Jason

---
title: Preposterous on the Raspberry Pi
date: Thu, 19 Dec 2013 01:15:04 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Not that it should be a surprise, but [ Preposterous
](https://github.com/jjg/preposterous) runs great on the [ Raspberry Pi
](http://www.raspberrypi.org) .  Since it doesn't need a database or web
application server, the memory-constrained environment of the Raspberry Pi
doesn't hold back Preposterous's performance at all.

  

At some scale, the throughput of the USB-based network interface would
probably begin to slow things down, or perhaps the speed at which files on the
SD card can be accessed, but "seat of the pants" performance seems almost on-
par with remote servers (i.e., across a Internet link), even for audio or
video.

  

If you want to try this yourself, the setup is very straightforward and the
standard Raspian distro includes everything you'll need to get started using
the steps below:

  

**Create email box to receive posts**

This can be a simple free gmail account, or any other service that provides
IMAP access.

  

**Clone the github repo**  

Issue the command below anywhere you like (probably easiest to do in your home
directory if you don't have something else in mind):

  

git clone [ https://github.com/jjg/preposterous.git
](https://github.com/jjg/preposterous.git)

  

**Create the web root**

This is a folder where the blog files will be stored.  If you don't already
have a webserver installed, this can go anywhere (the easiest place to get
started is to create a directory inside the Preposterous directory you made in
the previous step):

  

cd preposterous

mkdir blogs

  

**Copy the config**

Copy the preposterous.cfg.template file to preposterous.cfg:

  

cp preposterous.cfg.template preposterous.cfg

  

**Edit the config**

Open preposterous.cfg in your favorite editor and fill in the blanks.  Here's
a brief description of each parameter:

  

**imap_server:** _address of imap server, for gmail it's[ imap.gmail.com
](http://imap.gmail.com) _

**smtp_server:** _address of smtp server, for gmail use[ smtp.gmail.com
](http://smtp.gmail.com) _

**smtp_port:** _smtp server port, for gmail use 587_

**email_address:** _email address the server will use to receive posts_

**email_password:** _password for above email address_

**web_hostname:** _name of this device, either a DNS name or IP address_

**web_filesystem_root:** _path to where blog files should be stored_

  

For example, here's what the config looks like on my test Pi:

  

[mailserver]

imap_server: [ imap.gmail.com ](http://imap.gmail.com)

smtp_server: [ smtp.gmail.com ](http://smtp.gmail.com)

smtp_port: 587

email_address: [ preposterous1984@gmail.com
](mailto:preposterous1984@gmail.com)

email_password: mysupersecretpassword

  

[webserver]

web_hostname: [ 10.0.1.85:5000 ](http://10.0.1.85:5000)

web_filesystem_root: ./blogs

  

**Initialize the site**

Send a test post to the email address you created above and then run the
following command inside the Preposterous directory:

  

./preposterous.py

  

After a few minutes you should get back your prompt and you can move on to the
next step.  If you receive any errors, double-check your configuration file,
and if things still don't work, let us know.  You should also receive an email
back from the server that includes a link to your new blog's index, and a
second message linking to the post itself.  Of course these links won't work
until you have a running webserver...

  

**Run the web server**

For testing, we can use Python's built-in web server (and for most things Pi-
related, it's probably sufficient).  Run the following command inside the "web
root" folder you specified in the config:

  

python -m SimpleHTTPServer

  

Now point a browser at the IP address of your Raspberry Pi and include the
port of the SimpleHTTPServer (by default this is port 8000).  The URL should
look something like this:

  

[ http://10.0.1.25:8000/ ](http://10.0.1.25:8000/)

  

With any luck you'll be greeted with the Preposterous server index page and a
link to the blog you created by sending that initial email.

  

To make the server operate in an ongoing fashion, you'll need to create a cron
task to run [ preposterous.py ](http://preposterous.py/) on a scheduled basis.
To do this issue the following command:

  

crontab -e

  

This will open the crontab file for the current user in the system's default
editor.  At the bottom of the file, add a line like this to check for new
posts every five minutes:

  

*/5 * * * * cd /home/jason/preposterous; /usr/bin/python /home/jason/preposterous/preposterous.py 

  

_Note: be sure to replace "jason" with the name of your home directory (or
replace the entire path if you choose to install Preposterous elsewhere)._

  

You'll probably also want to configure the web server to run all the time (or
select another web server that runs automatically).  To run the built-in
Python server at boot, edit the file /etc/rc.local and add a line like the one
below to the end of the file:

  

cd /home/jason/preposterous/blogs; /usr/bin/python -m SimpleHTTPServer

  

_Note: again, adjust the path accordingly.._

  

This should get you up-and-running with your own Preposterous server on your
Raspberry Pi.  Updating to new versions of Preposterous is very easy using the
git pull  command in the preposterous directory.  After pulling a new build,
it may be necessary to "rebuild" the html files generated by Preposterous to
get the latest features.  To do this, first delete the existing files in your
"web root" directory, then run the following command in the Preposterous
directory:

  

./preposterous.py rebuild

  

This will regenerate all posts using the contents of the server's inbox, and
will suppress notification during the rebuild process (otherwise everyone who
posted messages to the server would receive an email for every post they ever
sent).  Once this is complete the server is upgraded!

  

Have fun with your shiny new Preposterous server and if you do anything cool
(or run into any trouble) let us know by posting an [ Issue to github
](https://github.com/jjg/preposterous/issues?state=open) or dropping us a line
on Twitter [ @preposterous_me ](https://twitter.com/preposterous_me) .

  

  

\- Jason

  

---
title: Preposterous summary on Ello
date: Tue, 30 Sep 2014 08:57:52 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I wrote a bit of an epic post on Ello on the subject of Preposterous:

  

[ https://ello.co/jasonbot/post/hANv-C4JO7bckpKDE8mWhw
](https://ello.co/jasonbot/post/hANv-C4JO7bckpKDE8mWhw)

  

I've been thinking about taking Preposterous to the next stage lately so I'm
fishing around a bit for interest in the idea.  Also writing about it reminds
me of how cool it is and gets me excited about working on it again.

  

  

\- Jason

---
title: Print for an ill friend
date: Sun, 2 Aug 2015 05:19:01 -0500
author: jjg
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/37-img_2113.jpg) ](assets/37-img_2113.jpg)

---
title: Prints from last night
date: Tue, 11 Aug 2015 07:38:54 -0500
author: jjg
draft: false
tags:
  - preposterous
---
// jjg

[ ![](/preposterous/assets/48-img_2171.jpg) ](assets/48-img_2171.jpg)

---
title: Production audio test
date: Sat, 14 Dec 2013 13:52:54 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Production test of audio attachments.

  

  

---
title: Production configuration externalization test 1
date: Sat, 14 Dec 2013 13:09:44 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is a production test of the configuration externalization effort.

---
title: Production Notification Test
date: Fri, 13 Dec 2013 08:50:22 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I don't test notification often, but when I do, it's in production.

---
title: Production Upgrade Test
date: Mon, 8 Dec 2014 00:37:16 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is just a quick test to make sure the production Preposterous is up and
running.  Don't expect too much excitement here.

  

If this gets posted we're all good.

  

  

\- Jason

---
title: Production video test 1
date: Sat, 14 Dec 2013 13:44:30 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Production test of video attachment handling and new "don't save stuff we
don't display" code.

  

  

---
title: Progress on the OffGRiD case
date: Sun, 16 Aug 2015 09:17:21 -0500
author: jjg
draft: false
tags:
  - preposterous
---
![image1.JPG](/preposterous/assets/52-image1.jpg)  
  
Hope to have some printable parts _real soon now..._

  
//  jjg

---
title: Protocols
date: Mon, 31 Aug 2015 11:10:03 -0500
author: jjg
draft: false
tags:
  - preposterous
---
This will evolve into a longer one but I wanted to get the essentials captured
before they floated away.

There's a lot of talk today around "decentralized" software and leveraging
things like public API's and blockchain technology.  This is cool stuff, but
it's surprising to me how many people excited about this fail to realize that
the problems we have today with centralized services on the Internet is a
rather new occurrence.

Basically before The Web, everything on the Internet was decentralized.  The
reason this was possible is because of standard protocols.  This might sound
obvious, but if you look at how we value an open or public API, it's value
pales in comparison to an open protocol.  A public API allows you to interact
with a service as it was designed by someone else, but a protocol allows you
to interact with _any_ service in the same way.  It liberates both the service
consumer and the creator from having to worry about compatibility.  It also
delegates some of the hard questions about how services should interact to
larger groups of experienced engineers concerned with the global scope of the
Internet vs. smaller groups concerned with (or at least only most familiar
with) their own domain.

Let me give you an example.  There are an assortment of services that provide
file storage in the cloud: Amazon S3, Microsoft Azure, Dropbox, etc.  Most of
these provide an open REST-based API.  However to interact with them each one
does things slightly differently, even though they all work over HTTP and
attempt to adhere to REST conventions.  The result is that if you write code
against one and decide to use another (or are forced to switch) you have to
write more code that looks a lot like the code you wrote in the first place,
but not exactly.  Moreover, as each vendor improves their service, new
versions of the API are released.  If you want to support more than one
service, you have a lot of work cut out for you.

Now consider email.  Before most people had access to the Internet a lot of
them used email, but they were private, closed systems that could only
exchange mail within their own network.  Some mail systems engineered
"gateways" that could talk to foreign systems but these were complex and
certainly not universal.  The development of the SMTP protocol allowed these
systems to talk to each-other without having to continually adopt the other's
changing standards.  Not only this, but it allowed mail servers and clients to
no longer be tightly bound to each-other, giving users a lot more choice and
forcing down the cost of email.

Imagine if the same was true for storage.  Imagine if there was a SSTP (Simple
Storage Transport Protocol) that you could code against and know your code
would work against any compliant server.  This doesn't magically force vendors
to adopt the standard, but in the case of email, not having SMTP compatibility
put your server at a disadvantage against systems that did, and users demanded
it as well (they wanted to be able to email their friends on different
systems).

Remember I said this was going to be short?  Whoops.

To wrap it up, if we really care about returning the Internet to a distributed
system we need to turn our attention to developing new open protocols instead
of focusing on implementation, because without these protocols, we are
destined to simply create new and more interesting cages.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Protools
date: Mon, 31 Aug 2015 10:54:06 -0500
author: jjg
draft: false
tags:
  - preposterous
---
There's something very exciting about the idea of developing software geared
toward professional use only.  Professionals are willing to invest in tools
that provide value (it's simple math) and they are willing to learn to use a
tool if doing so will make it more valuable.

The idea of being able to pick up a piece of software and start using it, with
no experience, makes sense for casual users but if you're going to be living
inside a piece of software it should be optimized for that kind of use, not
optimized to make it easy for anyone to use.

There used to be a lot of software like this, but even territories like film
editing and CAD have mostly fallen to the "intuitive" school.  Don't get me
wrong, I think that approachable, novice-intuitive tools should exist (not
everyone is a professional at everything) but I don't think that they should
exist at the cost of expert tools.

Unix is another kind of protool.  A little effort in learning the essential
concepts of Unix (commands, arguments, pipes, etc.) can provide serious
empowerment, but there is essentially zero way to "discover" this on your own
(without reading the docs or something).

I'd like to consider the "protools" perspective as I develop new projects.
Not so much that I need to target existing professional audiences, but the
idea that it's OK to build something that isn't right for everyone, or
requires a little learning to be able to use effectively.  I think the result
will be less up-front design time wasted and a greater level of user
satisfaction, even if it's for a smaller number of users.

I think there's already enough things in the world that are trying to be right
for everyone, and by that nature, perfect for no one.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Public Access IP
date: Sun, 17 Aug 2014 09:29:09 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Could something akin to "Public Access Television" provide a solution to the
Net Neutrality debate?  
  
(If you're not familiar with **Public Access Television** , it's a fascinating
subject.  This is a good place to start learning about it - [
en.m.wikipedia.org/wiki/Public_access_television
](http://en.m.wikipedia.org/wiki/Public_access_television) )  
  
By providing free access to non-commercial services, Internet carriers would
then be free to apply surcharges and penalties to competing commercial traffic
without endangering the use of the Internet as an open means of information
sharing and communication.  
  
Furthermore, this arrangement would encourage the development of non-
commercial services as an alternative to commercial systems and their
undesirable profit models of advertising and sharecropping.  
  
Public Access Television is itself not without controversy, but it's
interesting to look at if only to understand that there is a long history of
hoarding of communication technology by private industry, as well as people
fighting for its use in the public interest.

---
title: Radium
date: Tue, 31 Dec 2013 23:12:47 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/171-photo.jpg) ](assets/171-photo.jpg)

---
title: "Raspberry Pi 3 A+"
date: 2018-12-04T23:00:00-06:00 
draft: false
tags:
  - electronics
---

![Raspberry Pi 3 Model A+](/rpi-3-ap.jpeg)

Original Raspberry Pi Model B (left), new Raspberry Pi 3 Model A+ (right).---
title: Real Progress
date: Mon, 8 Dec 2014 01:05:51 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I can't express how excited I am to finally put [ Issue #47
](https://github.com/jjg/preposterous/issues/47) to bed.  That one really
irked me, it made everything that I wrote in Gmail look like crap, and since
I'm using a Chromebook as my "daily driver" that was pretty much everything of
substance that I posted.

  

The other major source of posts is the camera on my iPhone, which
unfortunately still has an [ open Issue
](https://github.com/jjg/preposterous/issues/48) .  I'm less concerned about
this than I was before tonight however since the problem seems to have more to
do with dubious engineering choices on Apple's part than on how Preposterous
is handling the post data, and while I do plan to address this I don't feel
like it's all my fault anymore.

  

Regardless I feel like I can start using Preposterous in earnest again, and
this is encouraging work to explore taking it to the next level.  I'd love to
be able to devote more time to developing Preposterous as well as return to
using it as the root of my public Internet presence.  I'll be moving
Preposterous to a new more concise domain name soon, and working to make it a
more complete blogging platform without compromising its deliberate
simplicity.  More on that soon.

  

In the meantime expect to see more posts here more regularly and stay tuned
for updates on what's coming down the line.

  

  

\- Jason

---
title: Recyclotron Jr. and Recyclotron City
date: Sat, 24 Jan 2015 15:08:22 -0600
author: jjg
draft: false
tags:
  - preposterous
---
For the last few months Jamie has been adding the occasional non-book-things
to our free little library.  Sometimes these are toys, or art or other
interesting things that find a home with one of the patrons from the
neighborhood.

In a way this is a sort of embryonic version of a more ambitious project we've
been noodling on we call the Recyclotron, but the Recyclotron is a bit more
sophisticated and is designed primarily to find loving homes for things that
might otherwise end up in a landfill.  Someday we'll get around to building
one, but in the meantime this has been an interesting experiment along similar
lines.

Recently Jamie put a few near-foodstuffs in the library (some k-cups), things
we tried and decided we didn't like, or things we received more than we could
use, etc.  This made me wonder what it would be like if every house on our
street had a little box out front used the same way.  The street would become
something of a shop, stocked with otherwise unwanted items, available at no
cost to the neighborhood.

Imagine if you were working on a project around the house and you needed a
part, or if you were working in the kitchen and you needed an ingredient, or
perhaps a painting and you need another brush...you get the idea.  What if
instead of heading out for Wal-Mart or having to wait for something via mail-
order you could take a stroll down the street and perhaps find just what you
need available for free?  Even if you don't find what you're looking for maybe
you find something else interesting, or maybe you just end up going for a nice
little walk, regardless of the outcome the experience is positive, and you can
always run to the store anyway if you don't find what you need.

This seems like such a good and obvious idea that I'm sure someone else has
thought of it, and perhaps even implemented it somewhere, but nothing comes to
mind (if you know of such a thing please share).  There may be potential
downsides but nothing jumps out at me, especially if these boxes are located
on their owner's property (preventing them from becoming dumping grounds,
which might happen if they are centralized on shared land) and if the size is
somewhat dictated they would be naturally self-regulating in terms of contents
and aesthetics (hard to park a Camaro on blocks inside one).  It seems
something like this would be particularly valuable in something like a "tiny
house" neighborhood, where storage is at a premium.

I'm curious what others think of this idea, is it something you'd welcome to
your neighborhood, or is there some downside that isn't apparent?  I'll be
thinking about how to extend our experiment and measure the results, but if
you have thoughts or know of other similar efforts we'd love to hear about
them.

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Release Zero
date: Fri, 13 Dec 2013 01:29:19 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
A public demonstration version of Preposterous is up and running on Gullickson
Laboratories (if you're reading this, you probably already know that).  You
can see a list of current users here:

  

[ http://preposterous.gullicksonlaboratories.com
](http://preposterous.gullicksonlaboratories.com)

  

Additionally a Github repository has been created and code has been posted.
There are many known and unknown bugs; feel free to add to the Issues list as
you please

  

[ https://github.com/jjg/preposterous ](https://github.com/jjg/preposterous)

  

That's all for now

  

  

\- Jason

---
title: RESTduino revisited
date: Wed, 9 Apr 2014 01:35:42 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ RESTduino ](https://github.com/jjg/RESTduino) is a little Arduino sketch I
wrote a few years back for the Arduino Ethernet shield.  The idea was to make
it easy for web developers to talk to the Arduino hardware using a REST-style
interface.

  

The other day when I logged into [ Github ](http://github.com) I saw a big
string of people starring the project, which surprised me because it's a bit
old and I haven't updated it in awhile.  Turns out that someone recently used
RESTduino as an ingredient in a recipe for [ opening and closing their garage
door using Siri ](http://delian.io/siri-open-the-garage) (with a particularity
clever way of working around Siri's lack of a public API).

  

While I was poking around in the Github stats I also ran across a company
selling an integrated Arduino/WiFi board called " [ WiLD FiRE
](http://shop.wickeddevice.com/resources/wildfire/) " that ships with
RESTduino as the stock firmware, how cool!

  

There's just something really awesome about finding your work out there making
it easier for people you don't know to build cool things.  I remember
hesitating publishing RESTduino because I thought it was too simplistic,
something anyone could write if they bothered, or too incomplete/buggy/etc.
Finding examples of it taking on a life of its own like this is an awesome
reminder of the power of open-source, and that you should share as much of
your work as you can regardless of what you think it may be worth.

  

  

\- Jason

---
title: "Revelations 2:18"
date: 2019-02-18T16:26:26Z
draft: true
---

I'm not a computer engineer, I'm a computer scientist.
---
title: Reversible hashing as a form of compression
date: Sun, 25 Oct 2015 11:02:14 -0500
author: jjg
draft: false
tags:
  - preposterous
---
[ JSFS ](https://github.com/jjg/jsfs) stores files as blocks (1MB typically)
named for a hash of their contents. If it were possible to reverse this hash,
it wouldn't be necessary to store the contents on disk, right?

So the idea is to use a hash function that is easily reversible when the a
file is stored, and then reverse the hash when the file is  retrieved, thus
reducing the amount of disk storage consumed.

I don't expect to get this for free, and I anticipate that the trade-off will
be processing time for storage, but I'm interested in exploring that because
processing power increases much faster than storage density, and this problem
has the potential to be " [ embarrassing parallel
](https://en.wikipedia.org/wiki/Embarrassingly_parallel) ".

I figured this was a solved problem but it turns out that most people working
in this space are trying to accomplish the opposite (an unreversable hash) so
it's been hard to find much prior work.

Right now my only thought on implementation is as follows:

  * At write-time, append a known value to the block data before generating the hash 
  * At read-time, feed random values into the hash function until the known value appears in the output 

I'm sure there are more clever ways to do this, but I think this will work,
and it can easily be run in parallel. The key optimization would be to select
(or create) a hashing function that can generate a sufficient non-clashing
"namespace" while being simple and fast, and generating as little output as
possible.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Review - Hakko FX-901 Cordless Soldering Iron
date: Sat, 27 Dec 2014 22:49:51 -0600
author: jjg
draft: false
tags:
  - preposterous
---
I've had a few different cordless soldering irons and for the most part
they've been disappointing.  Don't get me wrong, I don't expect a portable
iron to perform like a nice temperature-regulated bench unit, but most of the
ones I've tried either don't work at all or are difficult-to-dangerous to use.

  

What I need out of a cordless iron is something that I can keep in my toolbox
to use in the field for lightweight soldering jobs (circuit boards, small-
gauge wire, etc.) for either repairs or prototyping in the field.  Something
equivalent to a run-of-the-mill 15-30 watt unregulated iron would do the
trick.  Up until now the best one I've had is an old Black & Decker unit that
uses butane cartridges (which are possibly no longer made) and has a large tip
that makes navigating even large through-hole components a challenge.  I've
tried several times over the last few decades to find a replacement for this
iron, but I have yet to find anything better.

  

So when Sparkfun announced a battery-powered Hakko iron ( [
https://www.sparkfun.com/products/13151
](https://www.sparkfun.com/products/13151) ) it had my interest.  I've never
owned a Hakko before but I've heard from various trusted sources that they are
well-made and they make some bench units that look pretty sweet.  The price
seemed very reasonable, so I thought I should give it a try.

  

The iron arrived in simple packaging and without a lot of documentation, but
the back of the package card covered the essentials.  What was surprising to
me was that the battery life in terms of run-time was expected to be greater
with NiMH cells over alkaline (100min+ vs. 75min).

  

![image1.JPG](/preposterous/assets/10-image1.jpg)  

  

While alkaline cells produce a higher temp, I was happy to give up a little
heat for more run-time and a reusable battery.  I picked up 4 2500mah AA NiMH
cells for the unit (I bought mine locally but you can order them along with
the iron from Sparkfun: [ https://www.sparkfun.com/products/335
](https://www.sparkfun.com/products/335) ) and I haven't used it long enough
to exhaust them yet (about 2 hours of on-off so far) so I'll have to post an
update later to attest to the battery life claims.

  

![image2.JPG](/preposterous/assets/10-image2.jpg)  

  

With batteries charged and in place the unit is well balanced, if a little
heavy compared to my butane iron (and of course compared to a mains-powered
iron).  For the amount of time I used the iron to test it this wasn't a
problem, but I could see how using this device for extended soldering sessions
might cause some strain.  For me this is unlikely to be an issue since if I
were planning to do a lot of soldering I'd do it at the workbench with my
regular iron.

  

The iron heats up surprisingly fast and is ready to solder in 15-20 seconds in
the cool winter laboratory.  It's fast enough that turning it off to set it
down doesn't slow you down too much (which is good because it sits a little
awkwardly on the bench).

  

![image3.JPG](/preposterous/assets/10-image3.jpg)  

  

Once up to temp the tip takes solder well and is very efficient at
transferring heat to the joint.  I was able to rip right along soldering
through-hole components about as fast as I can with my bench iron once I got
the hang of holding the Hakko.  In fact, I got going so fast that I soldered
an entire board before realizing that I was soldering the headers in the wrong
holes (I guess I'll test it's de-soldering capabilities soon).

  

![image4.JPG](/preposterous/assets/10-image4.jpg)  

  

A nice feature of the kit is that it comes with a cover for the iron that is
built in such a way that it doesn't come in contact with the tip and is
ventilated so you can cover the iron while it's hot and put it away safely
(important for something you plan to keep in a toolbox).  The cover also
interferes with the power switch, forcing it to the "off" position when
installed which should prevent it from burning through batteries when you put
it away.

  

![image5.JPG](/preposterous/assets/10-image5.jpg)  

  

Overall I'd say the FX-901 is the iron I've been waiting for.  It does a fine
job for the kind of soldering I need it for, it's portable, safe(r),
convenient and inexpensive.  It also feels like a quality tool that should
hold up to regular use and the ability to use commonly-available alkaline
batteries is a nice feature for a field-oriented tool (especially if you don't
remember to charge your tools...).  At this point I'd definitely recommend it
to anyone doing electronics work, and it may even make a suitable alternative
to a mains-powered iron if you don't do a lot of soldering or don't have a
regular workspace to setup a regulated bench-top unit.

  

  

\- Jason

---
title: RHaC Part 2
date: Wed, 28 Oct 2015 14:49:34 -0500
author: jjg
draft: false
tags:
  - preposterous
---
Need to think up a clever name for this.

So following-up from my [ last post ](http://jjg.preposter.us/reversible-
hashing-as-a-form-of-compression.html) , I did some experiments and realized
that I was making it more complicated than it had to be.

  * There's no need to "seed" the data, if you have the hash, then you already _know_ when you've found the right source data (the hash will match, **duh** ) 
  * Using random input data doesn't make sense because you could theoretically be searching forever, as opposed to having a finate (if very long) search by simply incrementing through all possible input combinations 

Since my original post I've looked into a number of hash functions and learned
more about the field of study.  I still didn't find a function that suits my
purpose, but at least I know what to call one that would.  What I'm looking
for is a hash function that is:

  * [ Collision-resistant ](https://en.wikipedia.org/wiki/Collision_resistance) (or ideally. collision-proof) 
  * Not especially [ Preimage attack ](https://en.wikipedia.org/wiki/Preimage_attack) resistant 
  * Suitably large range 

More broadly, I want a hash function that is fast to hash and not resistant to
being reversed.  Unfortunately most collision-resistant hash functions are
designed for cryptographic applications where reversability is undesirable.
So I'm still on the lookout for a hash function that suits my oddball needs.

On the upside I've seen several conversations that indicate even a brute-force
approach to what I'm trying to do may be feasable due to newer technology that
is becoming more commonplace (GPU's and FPGA).  Also it's worth noting that
what I have in mind is almost [ trivial for a quantum computer
](https://en.wikipedia.org/wiki/Grover's_algorithm) , so I've got that to
fall-back on.

For now I'm going to plow ahead using existing (if not ideal) cryptographic
hash functions and see if I can at least demonstrate results with small input
data.  If that works, then I'll consider it proof-of-concept, and I can look
deeper at accelerating the reversing process as well as selecting/designing a
hash function more suited for the application.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: RHaC Part 3
date: Fri, 06 Nov 2015 08:43:05 -0600
author: jjg
draft: false
tags:
  - preposterous
---
I think I'll be taking a break on the reversible hash as compression project.

The idea is fun and the potential is exciting.  I think the theory is sound
and my initial proof-of-concept demonstrated that it could work.  The next
step was to reach a "break-even" point (to borrow from nuclear reactor
vernacular) and this was achieved, although the results were sobering.

I had originally planned to optimize the code from the proof-of-concept
experiment before trying to reach break-even, but then decided it made more
sense to at least _attempt_ it with the original code.  So I set this loose
and about a day later (27.76 hours) break-even, storing as much data as was
represented by the hash value, was achieved.

The sobering part was the throughput: **1785 seconds per bit** (as opposed to
bits per second).  Now I understand that no effort was put into making the
code faster, and of course no parallelization, etc. was applied.  These
results are from a very crude C program that I wrote in a couple hours (and my
C is pretty rusty), so I didn't expect it to be fast.  But I do think it
throws some light on just how significant of a challenge this project will be.

I still feel confident that this can be done as none of my findings so far
disprove it.  I do however think it is ambitious enough that it will take a
dedicated amount of time to work on and it's not something I'll be able to do
in spare hours here-and-there.  I'll need to dedicate extended periods of
clear-headed mental energy to the work and  I'll also need help, because as
much as I'd like to learn all that is needed to accomplish this goal, doing so
will extend the timeline indefinitely.

I also still believe that it is worth pursing.  Even a modest accomplishment
could yield enormous value.  This is why I'm setting forth a simple and
achievable rubric for success:

**1 megabyte, 320 kilobits, 32 bytes.**

One megabyte represented by a 32 byte hash, stored and retrieved at a rate of
320 kilobits per second.  This goal achieves a outstanding compression ratio
while maintaining a data rate suitable for high-quality audio and moderate-
quality video playback.  Achieving this goal would allow for the creation of a
media player capable of holding hundreds of millions of songs, or tens of
thousands of hours of video, on a $25 SD card.  It could contain all of
Wikipedia and be updated daily over the most meager Internet connection.  If
it can be achieved with reasonable power consumption (which I believe it can
through an FPGA or ASIC implementation), it could fit in your backpack or your
pocket and be recharged by the sun.

I think this is an achievable and worthwhile goal, and warrants the dedication
it will require.  However I'm not sure how to arrange the necessary resources
at this time, so I'm going to have to suspend the project until I'm able to
solve that problem.  As usual the barriers to innovation are not technical or
scientific ones.

In the meantime, I plan to gather my notes and document my thoughts on the
concept (this is part of the effort) so they are not lost and I can make them
available to others who are interested in working on the parts I'm struggling
with.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: RSS Readers
date: Sun, 27 Jul 2014 11:56:24 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
**Warning** , this is a rant.

  

After getting back from a nice Sunday walk with my wonderful wife Jamie I sat
down on the couch and thought _"it would be nice if there was some sort of
disposable reading like the newspaper I could read right now"_ .  This brought
back to mind an old iOS app called Reeder and reading RSS feeds to chill out
with a cup of coffee.

  

I stopped using Reeder awhile back, probably around the time Google Reader
shut down.  It seems like at that point anyone interested in RSS would have
realized that maybe the idea of managing distributed data feeds through a
centralized service under the control of a private corporation wasn't the way
forward.

  

So this morning I just assumed that things had been straightened out by now
and went shopping for a new RSS reader that I could throw a few feeds at.
After a bit of poking around in the iOS App Store I found "Unread", which
seemed like my kind of reading app (fast and simple).  I paid my $4.99 and
waited for the download to complete.

  

Unread is very nice, attractive, good typesetting, fast; I was really looking
forward to using it, then I tried to add a feed.  Turns out there's no way to
add a feed, only "Accounts".  This was common in applications before the
demise of Google Reader because every RSS reader just assumed you'd be using
Google Reader (in fact for many that was the only account you could sign
into).  But I would have thought that after the GR shutdown things would have
changed, apparently not.

  

I was frustrated about this for awhile and lodged a complaint with the
developer, but I really wanted to read something this morning so I decided to
go through the process of setting up one of these accounts temporarily until I
could use the app to read feeds directly.  No surprise, this only elevated my
frustration.

  

  * [ Feedwrangler ](https://feedwrangler.net/welcome.html) : $19/year   

  * [ Feedbin ](https://feedbin.com/) : $3/month   

  * [ Feedly ](http://feedly.com) : Sign in with Google/Facebook/etc.   

  

Here's [ where Cameron goes berserk
](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=video&cd=1&cad=rja&uact=8&ved=0CBwQtwIwAA&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DCOvsCB2DfPc&ei=PyzVU7iXB4KxyASC74GoAw&usg=AFQjCNGz3S_gcojLCqJFSdi6uathdDjQMQ&sig2=boJoyW3SZbVSbHT5d2xOug&bvm=bv.71778758,d.aWw)

  

Let me get this straight: I need to pay a subscription fee or with my soul
(the road to hell is paved with Single Sign-On) for a service in order to
connect an app I purchased to a feed from a website that is unrelated to the
app or the subscription service?  In order to use a public, open syndication
standard.

  

**Simply put:** **W** hat **T** he **F** uck.

  

This is why we can't have nice things.

  

So many of us are concerned about preserving Net Neutrality but we are willing
volunteer for net _neutering_ by [ trading our liberty temporary safety
](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CB8QFjAA&url=http%3A%2F%2Fen.wikiquote.org%2Fwiki%2FBenjamin_Franklin&ei=rCzVU_HVLtCuyAS18oHoCA&usg
=AFQjCNGSgD9WJOJsruCqI-SJ_pRs-
MEpdw&sig2=T-tIEtLRY76XPxN0CR9TIQ&bvm=bv.71778758,d.aWw) or convenience.
After all, what is Facebook or Instagram but the functionality of a blog + RSS
wrapped up in a easy-to-swallow gel cap, whoese only cost to you is having
your every thought and utterance mined for market research and occasionally
having your perception of reality warped by being involuntarily experimented
on.

  

We have had the technology for almost decades to do what we do now with social
networks and other monolithic, privately-owned web publishing platforms (I'm
looking at you, Medium) without the compromise and prostration that comes with
using these "free" services.  We've been convinced that we are too dumb or too
busy or too important to be troubled with the difficulty or tedium of
operating a web server or installing open-source publishing software.  If that
doesn't work then we are intimidated into believing that security is the
reason we should trust these tasks to the professionals, because they will
protect your information from [ evil hackers ](http://www.nsa.gov/) .  We took
the [ blue pill ](http://en.wikipedia.org/wiki/Red_pill_and_blue_pill) .

  

The irony here is that eliminating net neutrality wouldn't be quite such a
threat to the Internet we know and love had we taken the red pill.  It's
harder for ISP's to establish a " [ fast lane
](http://www.nationaljournal.com/tech/on-net-neutrality-verizon-leads-push-
for-fast-lanes-20140718) " for service providers if the writing you're reading
or the music you're hearing or the video you're watching or the game you're
playing is coming from a constellation of independent systems, hosted across a
diverse set of networks, spread around the world (or beyond).  In a
distributed content Internet, it's a lot harder for gangsters to burn down all
the storefronts, and it's a lot easier to build them back up again faster and
better than before.

  

...so I guess I have to write my own RSS reader...

  

  

\- Jason

---
title: rss test 1
date: Sat, 28 Dec 2013 01:31:28 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
first test of rss feed generation

---
title: rss test 2
date: Sat, 28 Dec 2013 01:34:49 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Second test of rss feed generation.

---
title: RSS Test 4
date: Sat, 28 Dec 2013 01:50:06 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is the fourth test of the RSS generation code for Preposterous.

  

  

\- Jason

---
title: RSS Test Five
date: Sat, 28 Dec 2013 02:06:10 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is the fifth test of RSS feed generation.

---
title: Runtime
date: Sat, 08 Aug 2015 19:56:50 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I'm looking into various "container" systems like [ CoreOS
](https://coreos.com/) because I'm on the hunt for the next level of
refinement of the systems I build and maintain.  These systems are an
improvement over previous "virtualization" technologies from my perspective
because they don't require the redundancy and overhead of full-blown
"computers" to run each component.

That said, I don't think containers take this far enough, and to some degree
they are more awkward; not exactly sysadmin territory, but not really
developer turf either.

What I really want is push everything further up, squarely into developer
territory, far enough up to make the term "devops" obsolete.

What I have in mind is just enough O/S to support a [ Node.js
](https://nodejs.org/) runtime.  This could be a custom Linux distro, or
better yet, a custom kernel that boots the hardware and supports the native
system calls supported by Node.js.

This would lay the foundation for the higher-order functions I've described
before as [ JS/OS ](http://jjg.preposter.us/thoughts-on-js-os-1-0.html) ,
implemented in pure Javascript.  Of course this is possible now, on existing
operating systems, but I want something simple, fast and secure, that I can
install on whatever hardware I have available and decide later how I want to
distribute application and service code to these nodes.

JSFS lays much of the foundation for these higher-order functions, and as-is
it's possible to build a wide array of applications on JSFS alone, but I want
to add a few key features that would make this a truly general-purpose
operating system, suitable for almost any imaginable application.

You would think that something like this exists, an OS that "boots to
Node.js", but if there is, it isn't quite what I need, or I haven't found it
yet.  There are many similar things, but the ones I've found have different
goals, and therefore depart from my precise requirements.  That being the case
my current way forward is to go the [ Linux From Scratch
](http://www.linuxfromscratch.org/) route to build the smallest possible
distro that meets these needs, unless something better comes to mind.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Scaled image test 
date: Fri, 10 Jan 2014 01:52:48 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/179-photo.jpg) ](assets/179-photo.jpg)

---
title: "Scale"
date: 2019-07-09T12:10:44Z
draft: true
---

We treat scalability as absolutely good and necissary, which means that anything which doesn't scale is inferior or bad.  The result is that many applications, their developers and users are alienated because of the costs associated with scalability.  Perhaps even worse, the virtue of scalability isn't absolute, and in many cases the scalable system does people more harm than good.

Consider the personal computer.  Compared to mainframes, the personal computer does not scale.  However, almost every piece of software that provides value to people has been created in the last 30 years was created  on or for the personal computer.  Technically speaking most of what people are talking about when they say "the Internet" runs on the same non-scalable personal computer architecture from the 1980's.  Over the years this architecture has grown more like the mainframe, and in turn the diversity of software has decreased while the centralized control of that software has increased.  This has increased the scalability of the personal computer, but it has also decimated the utility of the personal computer to people, essentially dissolving it's original value.

So why are we fixated on scalability?  The primary force is the same force behind all industrialization.  Software is uniquely suited for mass-production, and one of the attractive things to capitalists about mass production is that profit generally goes up in step with the number of units produced.  Since all that is required to produce another unit of a piece of software is to copy some files, mass-production of software is very profitable.  In the pre-public Internet days, this usually meant copying the files to a disk (or disc) and packing it in a box with a manual.  Compared to say, an automobile, or even a Walkman, the cost-per-unit to produce a piece of software for sale is trivial.  So why were we not obsessed with scalability?  Because while the manufacturing process might need to scale, the software was still used by a single person, on a dedicated computer.  Few applications (other than games) were so demanding that they taxed the resources of a single computer supporting a single user, and so application performance "scaled" linearly with the number of people using the software on their own computers.

When software started to migrate to the Web, this equation changed.  The web eliminated even the cost of the disk and box when it came to selling software, but suddenly everyone using the software was sharing the same computer.  The solution?  Throw more computers at it!  

There's two problems with this solution:

1. Since it wouldn't be profitable (or practical) to dedicate one whole computer to each user, the natural scalability of the old model is lost
2. Software written to 
---
title: Second iOS test
date: Mon, 8 Dec 2014 00:18:01 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This test is the default iOS mail app. I hope it works ok. The next test will
be an html email from this same client. Then maybe pictures? Sent from my
iPhone

---
title: Second weird A test
date: Mon, 8 Dec 2014 00:05:36 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Looks like the problem may have to do with the content type on the browser-
side of things, at least in the sense that if we are explicit about the
encoding type the problem may go away.

  

Of course the tricky part is that this is somewhat hard to test scientifically
without rebuilding the entire index.  I'm not planning to do that just yet,
because I think an email with a couple double-spaced sentences should get us
close.  Alternatively I could re-send the previous post since it showed up
wrong.

  

One way or another...

  

  

\- Jason  

---
title: Self-documentation Tools
date: Fri, 23 Jan 2015 08:07:49 -0600
author: jjg
draft: false
tags:
  - preposterous
---
I've had a number of posts in-progress for a few weeks now, but they're all
missing a piece or two that makes them incomplete (and that doesn't include
editing).

I (as I'm sure others) struggle doing a good job documenting my projects.  I
tend to move at a pace that doesn't leave a lot of room for capturing the
process, and I'd really like to do something about that.  I know that I have
benefited immensely by the effort others have put into documenting their work,
and that many of the things I've been able to do would have been impossible if
not for the documentation that others have created.

I think one of the greatest casualties from the difficulty of documenting a
project have been failed projects.  It's hard to motivate yourself to put
effort into documenting and sharing failure, but I will say that I've learned
just as much, if not more by studying projects that didn't turn out the way
they were expected to (this is especially true in 3D printing, where studying
failures of others helps identify patterns that would otherwise be difficult
to detect looking at just your own bad prints).

It would seem that we have the technology to improve this situation, and
perhaps there are already better options than I'm aware of.  In particular the
use of small video cameras, microphones, speech-to-text conversion, etc.
should be able to be combined into something that could record the work-in-
progress and produce a nice pile of input for the editing machine.  This of
course simply transfers some of the work to editing, but fortunately editing
is less chronologically-bound (i.e., you can go back and edit something, but
it's harder to go back in time and take a photo of something mid-assembly).

A more sophisticated system could even assist with the editing stage, sort of
a pre-processor that could take the captured data and "shape" it based on
what's known about the project, what has been documented in the past and any
other contextual information (date/time, weather conditions, season of the
year, biological monitors, etc.).

What do you think, is there a set of tools you use to document your work that
you would recommend?  I'm currently using [ Evernote ](https://evernote.com/)
as sort of the "hub" of this process but if you have any recommendations or
suggestions please share them wherever you found the link to this post ( [
twitter ](https://twitter.com/jasonbot2000) , [ ello
](https://ello.co/jasonbot) , [ facebook
](https://www.facebook.com/jasonjgullickson) , [ whatever
](https://plus.google.com/u/0/+JasonGullickson/posts) you prefer).

\- Jason

---
title: "Sipeed MAIX BiT"
date: 2019-03-07T16:31:51Z
draft: false
tags:
  - ai
  - ml
  - soc
  - riscv
  - python 
---

The Sipeed MAIX BiT is one of a family of devices aimed at the "[AI at the edge](https://en.wikipedia.org/wiki/Edge_computing)" market.  It caught my eye primarily because of its RISC-V CPU, and I've been looking for a low-cost way to dip my toes into working with RISC-V hardware, but I'm also curious to see if I can apply the devices hardware-accelerated [machine learning] capabilities to my work on accessible high-performance computing systems.

![](/smb_case.jpg)

The [Seeed Studio product page](https://www.seeedstudio.com/Sipeed-MAix-BiT-for-RISC-V-AI-IoT-p-2872.html) provides a lot of detail about the device, but it's buried in [TLA](https://en.wikipedia.org/wiki/Three-letter_acronym)'s and lingo which can be hard to decode.  Here's what I found interesting:

# Hardware Details

The heart of the module is the [Kendryte K210 system-on-a-chip](https://s3.cn-north-1.amazonaws.com.cn/dl.kendryte.com/documents/kendryte_datasheet_20181011163248_en.pdf) chip.  This [SoC](https://en.wikipedia.org/wiki/System_on_a_chip) combines a dual-core RISC-V CPU, hardware-accelerated neural network accelerator (KPU) & audio processor (APU) along with SRAM, ROM  and an array of I/O interfaces.

* CPU
  * Dual 64-bit [RISC-V](https://en.wikipedia.org/wiki/RISC-V) cores
  * 400MHz (overclockable to 800MHz)
  * [IMAFDC](https://en.wikipedia.org/wiki/RISC-V#ISA_base_and_extensions) ISA
      * 64-bit Base integer **I**SA (RV64GC)
      * Integer **m**ultiplication and division
      * **A**tomics
      * Single-precision **f**loating-point
      * **D**ouble-precision floating-point
      * 16-bit **C**ompressed instructions 
  * 2 (one per core) IEEE754-2008 FPU
  * 2 (one per core) 32KiB instruction cache
  * 2 (one per core) 32KiB data cache
  * 8MiB SRAM

In addition to the RISC-V CPU, the SOC include hardware designed to accelerate AI applications:

* Neural Network Processor (KPU)
  * 64 KPU
  * 576 bit width
  * 0.25 TOPS([tensor](https://en.wikipedia.org/wiki/Tensor_processing_unit) operations per second?) @ 400MHz (0.5 TOPS when overclocked to 800MHz)
  * Maximum model sizes
      * Fixed-point real-time: 5.9MiB
      * Floating point (pre-quantisation) realtime: 11.8MiB
      * Non-realtime: limited only by flash capacity (fixed or floating point)
* Audio Processor (APU)
  * 8 channels of audio input data
  * 12, 16, 24 and 32-bit input data widths
  * Simultaneous beamforming up to 16 directions
  * Built-in 512-point FFT 

The SOC has a lot of other features (including the typical I/O interfaces, power management, etc.).  Details can be found in the [datasheet](https://s3.cn-north-1.amazonaws.com.cn/dl.kendryte.com/documents/kendryte_datasheet_20181011163248_en.pdf)

# Hello Sipeed!

Right now I'm primarily interested in programming the CPU, so the rest of this post will be focused on getting set-up to run code on the RISC-V processor.  I'll leave diving-in to the AI features of the chip for a future post.

Out-of-the box the Maix BiT is setup to run [Micropython](https://en.wikipedia.org/wiki/MicroPython). This makes getting started easy (especially if you're already a Python programmer!) but I have to be honest, I was a little disappointed that I wasn't going to be writing something RISC-V specific.  I guess that's a good thing if the goal is to make it easy to adopt the RISC-V open architecture, but it kind of makes finally getting my hands on RISC-V hardware kind of anti-climatic.

This is my first Micropython device but my understanding is that developing for Micropython is pretty much the same all most hardware:

1. Get a USB-C cable
2. Connect the board via USB to a computer
3. Identify the serial port: `ls /dev/tty*`
4. Open the serial port with `screen`: `sudo screen /dev/ttyUSB0 115200`
5. Press `Enter` for MicroPython prompt (REPL)

![](/smb_board_power.jpg)

Now you're speaking to (Micro)Python!  Typing `help()` gives you some basic information about what's available:

```
Welcome to MicroPython on the Sipeed Maix One!

For generic online docs please visit http://maixpy.sipeed.com/

Official website : www.sipeed.com

Current installed module:
os modules:Built-in system operation module and file operation method
machine module:Built in some machine related modules
socket modules:Network operation method
app modules   :Provide some applications

Control commands:
  CTRL-A        -- on a blank line, enter raw REPL mode
  CTRL-B        -- on a blank line, enter normal REPL mode
  CTRL-C        -- interrupt a running program
  CTRL-D        -- on a blank line, do a soft reset of the board
  CTRL-E        -- on a blank line, enter paste mode

For further help on a specific object, type help(obj)
For a list of available modules, type help('modules')
```

Use `Ctrl+a` then `k` and then `y` to exit `screen`.

## Update the firmware

This is important.  I tried skipping this step because I wasn't sure which firmware image to use, but the stock firmware was very different from what the documentation describes and I wasn't even able to blink an LED without updating the firmware first.  The good news is that it's fairly simple:

1. Download the firmware image (I used  [v0.1.1 beta](https://github.com/sipeed/MaixPy/releases/tag/v0.1.1))
2. Install the `kflash.py` script: `git clone https://github.com/sipeed/kflash.py.git`
3. Change directories to the `kflash.py` repository: `cd kflash.py`
4. Flash the firmware: `sudo python3 kflash.py -p /dev/ttyUSB0 -b 2000000 -B dan ~/Desktop/maix/maixpy_v0.1.1_beta.bin`

You should see something like this:

```
[INFO] COM Port Selected Manually:  /dev/ttyUSB0 
[INFO] Default baudrate is 115200 , later it may be changed to the value you set. 
[INFO] Trying to Enter the ISP Mode... 
.
[INFO] Greeting Message Detected, Start Downloading ISP 
[WARN] Use built-in ISP_PROG2, If you download firmware to flash  failed, please use -i1
Downloading ISP: |██████████████████████████████████████████████████| 100.0% 
[INFO] Booting From 0x80000000 
[INFO] Wait For 0.3 second for ISP to Boot 
[INFO] Boot to Flashmode Successfully 
[INFO] Selected Baudrate:  2000000 
[INFO] Selected Flash:  On-Board 
Downloading: |██████████████████████████████████████████████████| 100.0% 
[INFO] Rebooting... 

```

I'm not sure what that `[WARN]` means exactly, but it didn't seem to cause any problems so far...


## Write some Python 

Blinking an LED is the [Hello World!](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of microcontrollers, so we start there.

Re-attach to MicroPython: `sudo screen /dev/ttyUSB0 115200`

The code below is entered line-by-line into the the serial terminal we attached above (just like the BASIC days):

```
>>> from Maix import GPIO
>>> fm.register(board_info.LED_R, fm.fpioa.GPIO0)
1
>>> led_r=GPIO(GPIO.GPIO0,GPIO.OUT)
>>> led_r.value(0)
```

...and of course it's not "Hello World!' without some blinking...

```
>>> import time
>>> for i in range(10):
...     led_r.value(not led_r.value())
...     time.sleep(1)

```

If everything went as planned, you should see the larger LED blink 10 times!

![](/smb_blink.jpg)

Writing anything complex this way would get old fast.  Fortunately there's a couple different ways to make it easier.  There is a built-in editor called [pye](https://github.com/robert-hh/Micropython-Editor) that runs directly on the board itself.  Using pye, you can load, edit, save and run programs using nothing other than the Sipeed MAix BiT and a serial terminal.  

If you prefer to use your own editor, there is a command-line tool called [ampy](https://github.com/pycampers/ampy) you can use to transfer files back-and-forth between your computer and the board.  ampy can transfer Python code you write on your computer over to the board to be run.  It can also transfer other files back-and-forth, useful if your programs need images or other media files.

There's lots of other ways to "integrate" your favorite development tools with the MAix BiT that I haven't had a chance to check out yet.  Theoretically any tool that works with Micropython should be compatible, but some experimenation may be necissary.

# What's Next?

In this post I've only scratched the surface of what this device is capable of.  I was most excited about the fact that I was able to get my hands on real RISC-V hardware for about $12.  I expected there to be a steep learning curve putting that open CPU to work, but as it turns out the learning curve is almost non-existant.  Since I can put the CPU to work without much effort, I'm planning to immediately begin diving-in to see what the AI features can do.  It's not clear to me at the moment how useful these features are outside of their prescribed applications, namely "smart" video and audio processing, but from what I've read of the documentation it should be possible to use the off-the-shelf features to analyze media from sources other than attached cameras and microphones.  It might even be possible to add custom "models" to the device to perform completely differernt AI/machine learning workloads.

I've bought a lot of odd development boards in the past and many of them end up in the parts bin after some initial experimenation. Based on my experience so far with the Sipeed MAix BiT, I don't think this board will be gathering dust in the drawer anytime soon.


# References

* https://www.seeedstudio.com/Sipeed-MAix-BiT-for-RISC-V-AI-IoT-p-2872.html
* https://maixpy.sipeed.com/en/
* https://github.com/sipeed/kflash.py
* https://github.com/shaoziyang/micropython_benchmarks/blob/master/README.md
---
title: Skul
date: Thu, 27 Mar 2014 10:08:51 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
![Inline image 1](/preposterous/assets/217-skull.jpg)

---
title: "Skyhole - A Pi-hole in the cloud"
date: 2019-02-21T14:26:53Z
draft: false 
tags:
  - privacy
  - anticapitalism
---

I used to run a great piece of software called [Pi-hole](https://pi-hole.net/) at home on a [Raspberry Pi](https://www.raspberrypi.org), but the hardware wasn't really up to the task, and since it provides an essential service (DNS), I took it down until I could throw better hardware at it.

I never got around to that, but a couple weeks ago I was messing-around with a public DNS server and wondered if I could do the same kind of DNS-based ad blocking using DNS alone?

Turns out I'm not the only person who thought of this, and in fact there's already something called [PyHole](https://github.com/glenpp/py-hole) to automate the process.

The PyHole documentation (and [linked post](https://www.pitt-pladdy.com/blog/_20170407-105402_0100_DNS_Firewall_blackhole_malicious_like_Pi-hole_with_bind9/)) do a good job of explaining the setup, so I won't cover the same territory here, but I've included a references section below with links to other pages I used in the process.

If you're interested in giving it a try, [get in touch](/about) and I'll share the server IP addresses with you.

## References

* https://github.com/glenpp/py-hole
* [DNS Firewall (blackhole malicious, like Pi-hole) with bind9](https://www.pitt-pladdy.com/blog/_20170407-105402_0100_DNS_Firewall_blackhole_malicious_like_Pi-hole_with_bind9/)
* [HOWTO - Configure a DNS firewall with RPZ](http://www.zytrax.com/books/dns/ch9/rpz.html)
* [Bind9 - Debian Wiki](https://wiki.debian.org/Bind9)
* [How To Configure Bind as a Caching or Forwarding DNS Server on Ubuntu 16.04](https://www.digitalocean.com/community/tutorials/how-to-configure-bind-as-a-caching-or-forwarding-dns-server-on-ubuntu-16-04)
* [IANA -- Root Servers](https://www.iana.org/domains/root/servers)
* [Install, Configure, and Maintain Linux DNS Server](https://likegeeks.com/linux-dns-server/#Types-of-DNS-Servers)
---
title: Slacking, Preposterous 2.0 and Quantum Computing
date: Sat, 18 Jan 2014 08:17:28 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Been a little while since I posted an update on Preposterous 2.0. The short
story is that I'm really hung-up on deleting posts.

It seems like such a core piece of functionality and getting it to work the
way I want it to is turning out to be very difficult (or maybe I'm just
looking at it wrong). It seems like an important and obvious thing, and I'd
feel kind of dumb declaring a second major release without it, but on the
other hand there are a lot of cool things ready to ship, so maybe it's dumber
to hold those back just because delete is giving me a hard time?

On an unrelated note, my quantum computing research has resumed, and I'm
considering creating a portal/aggregator for open quantum computing news and
resources. Right now I'm looking to see if something like this already exists
but it seems like most quantum computing information is in academia (and
expensive books and publishing) or behind private corporate doors.

If I decide to proceed, I'll detail the project here first. - Jason  

  

  
  
\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)  

---
title: Slug Test 3
date: Fri, 13 Dec 2013 23:43:33 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
test 3 of new slug code

---
title: "Soft Launch"
date: 2018-12-07T16:14:34Z
draft: false
tags: 
  - news 
---

As discussed on the [about](/about) page, this site will eventually contain every piece of content I've created.  Currently I'm in the process of writing software to translate this content from it's various existing forms into a format my new blog software ([Hugo](https://gohugo.io)) can understand.

In the meantime I'll be authoring new content as well, and at some point I'll make a more pronouced annoucement about the (re)launch of [jasongullickson.com](https://jasongullickson.com).

Until then I'll leave the existing [Gullickson Laboratories](https://jjg.2soc.net) site up, but eventually, once the content has been ingested here, I'll turn it into some kind of redirect to this site.
---
title: Software-Only Computer Hardware Company
date: Tue, 13 Oct 2015 22:45:37 -0500
author: jjg
draft: false
tags:
  - preposterous
---
It sound like a paradox, but I propose a software-only computer hardware
company.

  

I've wanted to start a computer company since I was a child, but it's become
something of a necessity for me as of late.

  

As a professional programmer, what I need from a computer turns out to be
something of an "edge case" and as such there's really no off-the-shelf
computer that's really made for me.  This is compounded by the fact that I
need a portable computer, which pretty much limits me to laptops, where
options in terms of displays, keyboards, etc. are fewer.  Unfortunately for me
these are the areas that matter the most.

  

I also want to run Linux.  While there's a lot of hardware that will run
Linux, there's not a lot of laptops that have great support for it, and the
ones that do don't provide the hardware features I want most (the ones that
come close are very expensive).

  

So I resolved to build my own computer.  This is easier than it sounds, due to
some recent technological advances.

  

The first is the rise of the single-board computer (SBC).  The most well-known
of these is the Raspberry Pi, a complete general-purpose computer the size of
a deck of cards that costs $35.00 USD and is capable of running a complete
Linux desktop environment.  Today there is an ever-growing number of these SBC
devices with a wide-range of prices, performance and features.

  

The second is desktop manufacturing and specifically, the 3D printer.  This
significantly reduces the complexity of fabricating custom components
necessary to create the computer's case as well as other various parts.

  

The availability of these two technologies makes it much more feasable for me
to produce a custom, personalized laptop computer.  So I set out to design my
ideal system.  The result is a design I call the "OffGRiD", which has the
following key elements:

  * "Clicky" keyboard with full-size, full-stroke keys 
  * Sunlight-visible screen 
  * Solar recharging 
  * Full Linux support 

  

Once I started designing OffGRiD it occurred to me that if I made the 3D
models parametric (which I always try to do) the printable parts could easily
be adapted to accept alternative components.  This became more important as I
discussed OffGRiD with others who thought it was cool but wanted to tweak the
design to their personal preferences.  Doing this on a case-by-case basis is a
lot of work, so I started thinking about how I could automate the process.

  

This is when I came up with the idea for a software-based computer hardware
company.

  

The idea is pretty simple.  A website where you can design a personalized
computer by selecting from a menu of components (SBC's, displays, keyboards,
etc.).  Once you've selected the components you want the site generates custom
printable 3D models for all the printed parts and a bill-of-materials (BOM)
for the components, connectors, fasteners, etc. you'll need to build the
computer.  These parts and the BOM can be downloaded from the site for free.

  

However not everyone has access to a 3D printer, and sourcing all these
components can be a lot of work, and this is where the business model lies.
Once the customization process is complete, all or some of the computer's
parts can be purchased directly via the website.  3D parts are sourced from 3D
printing services (more on this later) and the remaining components are
sourced from partner companies.  A 10% margin is applied to all parts,
although an effort is made to keep parts cost close to their retail price
through discount arrangements made with fulfillment partners.

  

But what if someone doesn't want to build (really, assemble) the computer
themselves?  The site doesn't provide any build-to-order options, however it
does facilitate connecting customers with local resources for help.  In fact,
the site will connect a potential customer with a local maker/hackerspace, or
other organization or individual in their area to help with assembly, print
parts, etc.  This might seem like it could undermine sales, but the primary
objective is happy users, so a few lost parts sales is worth it if it means a
better chance at providing a user with a pleasant experience.

  

Profits made by the site go back into R&D to produce new 3D models and
continuously add support for new components.  In addition to laptop models
obvious areas of expansion are tablet and desktop designs, but other less
common systems such as "media center" computers and automotive applications
could be added (these being particularly applicable to customized form-
factors, etc.)

  

In addition to new categories of machines additional user-driven customization
could incorporate user-supplied design files, art, etc. that could be applied
to the printable models to add even more personalization.  If the user
desires, these customizations could be provided as options to other users for
some sort of compensation (perhaps store credit or something more abstract).

  

In addition to minor customization, all models (as well as operating-system
source code, etc.) are open-source and avaliable on Github.  Ambitious
individuals are invited to fork these repositories and issue pull requests to
improve the existing models or provide support for new components or entirely
new model lines, if desired.

  

As hinted at in the last paragraph actual software plays a role as well.  A
standard Linux distro will be developed that will be included with each kit,
designed to work with the selcted hardware.  This includes hardware support
for the selected SBC as well as accessories where appropriate (for example, if
a solar panel is included some kind of charge gauge and other operating-level
support is expected).  Since some customizations will be geared toward
hackers, support for things like GPIO and other less common features will be
included in the distributed operating system.  Initial O/S choices are likely
to include an Enlightenment-oriented Linux desktop environment and a Sugar-
based system.  Of course owners can choose to install their own custom
operating systems and software as well.

  

  
// jjg

---
title: Solarhome 620 - First Impression 
date: 2019-01-22
author: Jason J. Gullickson
draft: false
tags:
  - offgrid
  - solar
  - biolite
  - solarhome-620 
---

Today I received a [Biolite SolarHome 620](https://www.bioliteenergy.com/products/solarhome-620) and my initial experience hasn't been great.  I'm a big fan of [Biolite](https://www.bioliteenergy.com) as a company, and I've been impressed with the other products I've purchased from them, so I may have had unrealistic expectations for the SolarHome.

The design of the device has the same level of quality I've experienced with other Biolite products, and the materials seem, at least superficially to be good, but it's hard to say more than that because so far I haven't had any luck getting it to work.

The system appears to ship with a (nearly) dead battery.  I was able to get the control module to turn on for a moment to display this information.  Not a big deal, in fact I expect things to have dead batteries when they show up this time of year but less expected is the fact that the unit won't charge.

![battery status screen](/battery_screen.jpeg)

I connected the included solar panel, placed the unit near a window and coaxed the display into showing the charge rate, which was zero.

![sun status screen](/sun_screen.jpeg)

I figured the unit was cold, and maybe the panel wasn't making much power so I left the system this way for a couple hours.  Checking again later the results were the same, so I grabbed a meter and tested the panel to see if there was a problem or if it was just not sunny enough to work.

The VOM shows between 11-12vdc at the solar panel terminals, and the lable on the back of the panel indicates that 12vdc is the ideal output voltage.  I did a continuity test between the panel's terminals and the plug, and that checked-out as well.

![panel specifications label](/panel_specs.jpeg)

So it appears that the panel and its cable are OK, and that there's enough sunlight to do some work, but the control module disagrees.  I'm tempted to find a suitable power supply and see if I can get the unit to indicate charging that way (perhaps the panel isn't producing enough amps?), but I'm a little worried about toasting the thing, and since I don't have a plug of the proper size I'd probably have to butcher the cable that came with the panel.

Instead I sent a request via Biolite's contact form to see what they think.  Hopefully they'll have some ideas because I think this is a very cool, very useful system and I'd love to be able to recommend it to others.
---
title: "Solarhome Inputs and Outputs"
date: 2019-02-06T14:31:47Z
draft: false 
tags:
    - solar
    - biolite
    - solarhome-620
---

Spent some time researching and documenting everything I could find about the on the [SolarHome 620](https://www.bioliteenergy.com/products/solarhome-620).

## Battery

BioLite publishes the following battery details on their website:

* 3300mAh
* 6.5 VDC
* 20 Wh
* LiFEPO4 chemistry

Based on this I believe that the battery is a pair of 26650 cells, perhaps simular to this: [https://www.batteryspace.com/custom-lifepo4-26650-battery-6-4v-3300mah-s-s-21wh-3-5a-rate.aspx](https://www.batteryspace.com/custom-lifepo4-26650-battery-6-4v-3300mah-s-s-21wh-3-5a-rate.aspx)

## Inputs & Outputs

I also spent some time probing the inputs and outputs.  Between the documentation and the lables on the hardware you can find a number of useful details:

Count | Description | Connector | Volts DC | Amps
----- | ----------- | --------- | ----- | ----
 1 | Solar panel input | 4.8mm barrel | 12 | 0.5
 2 | Lamp output | 5.5mm barrel | 12 | 0.5
 2 | USB charger | USB A-Type | 5 | 0.5
 1 | Expansion battery | Female Mini DIN-4 | ? | ? 

### Power In

![Solar panel specifications label](/solarhome_panel_specs.jpg)

Knowing the power input parameters is exciting because one of the most common complaints I see online about the SolarHome is that there's no way to charge the battery other than using the solar panel.  This isn't a big deal if you're installing the system somewhere for awhile where it can get charged-up, but if you're going camping or something more temporary, it would be nice to head-out with a full battery in case you don't have a full day of sunlight ahead of you.

It also opens up the possibility of using other off-grid power sources such as wind or micro-micro-hydro.  Without knowing how much regulation capacity the SolarHome provides, it would be prudent to have regulation on the supply-side of any alternative power source you come up with, but so long as you **don't push more than 12VDC @ 0.5A into the unit**, you shouldn't be in any more danger than you would be using the supplied solar panel.

###  Power Out

![Controller specifications label](/solarhome_controller_specs.jpg)

My guess is that the USB charging ports will be the best way to get power out of this unit.  Since they are designed to charge unknown devices they should have ample regulation to prevent you from harming the device.  

12VDC is avaliable via the jacks intended for the lamps which ship with the system, but it's less clear how the control unit interacts with devices connected to these outputs, so you may experience unexpected fluctuations in power from these ports.  In addition, since these ports are intended specifically for the lamps provided by BioLite it's possible that they don't have any regulation.  If you were to connect a load that draws more current than the ports are designed to deliver (0.5 amps) you may damage the system.

Unless you can be sure to only draw 0.5A or less, it's best to avoid using these for now.

### Mysterious Expansion Port

BioLite doesn't provide any additional information about the "Expansion battery" port other than that it's used to connect additional batteries and that I can't buy one.  I started poking-around at the port to see what it does and here's my best-guesses so far:

![Mini DIN-4 diagram](/1024px-MiniDIN-4_Diagram.svg.png)

*Pins are numbered counter-clockwise starting with the lower-right pin.*

Pin | Measurement | Purpose
--- | ----------- | -------
Skirt | 0 VDC | Ground
1 | 0 VDC | Serial RX
2 | 3.3 VDC, fluctuating | Serial TX
3 | 12 VDC | Power to charge external battery
4 | Floating | Not connected

The only connections I'm sure of are the skirt (ground) and pin 3 (12VDC).  At first I thought pin 4 was another ground but after seeing a signal on pin 2, I think it might be the receiving pin for a 3.3v serial connection (given the proximity to pin 2).

![Controller connected to oscilloscope](/solarhome_oscope.jpg)

When the control box is idle pin 2 is at 0 volts.  If a button is pressed, pin 2 goes high to 3.3v.  With the oscilloscope connected there appears to be a signal transmitted on pin 2 which appears as a series of voltage drops.  This sequence repeats about once ever 5 seconds as long as the LCD is displaying information.

It may be possible to learn more about this signal using only an oscilloscope, but if my scope can do it, I don't know how to use it that way.  I did a little searching on reverse-engineering serial signals and landed on using a [logic analyzer](https://en.wikipedia.org/wiki/Logic_analyzer) as the next step in decoding this signal.  I've never used a logic analyzer before, but I was able to find a [USB-based device for about $12](https://www.amazon.com/gp/product/B077LSG5P2) that is compatible with the open-source [Sigrok](https://sigrok.org) software, so I decided to give it a go.

Based on the above my guess as to how this works is that the external battery communicates with the control box using a serial command protocol to share state-of-charge information.  This data is then used by the control box to display capacity information to the user via the LCD.  I'm not sure why the battery charge voltage is 12VDC vs. something closer to typical [LiFEPO4](https://en.wikipedia.org/wiki/Lithium_iron_phosphate_battery) charging voltage (around 3.5VDC), my only guess is that they are using the same charging circuit design in the expansion battery that they are using in the control box (which uses a 12VDC supply as well).  Since there appears to be 12VDC on pin 3 all the time (even when there is no power coming from the solar panel and the  LCD is turned off), my guess is that it's up to the expansion battery to decide how much to draw from the control unit and that this is negotiated via serial communication.

That said these are all guesses.  I haven't done enough of this type of signal analysis to have much confidence in my findings so don't go out using these pins assuming I know what I'm talking about.  Hopefully I'll be able to confirm some of this after the logic analyzer arrives (and I learn how to use it).
---
title: "Solarhome Take 2"
date: 2019-01-31T15:06:36Z
draft: false 
tags:
  - solar
  - offgrid
  - biolite
  - solarhome-620
---

In my [previous post](/posts/solarhome-620-first-impression/) I introduced the [SolarHome 620](https://www.bioliteenergy.com/products/solarhome-620) and discussed my disappointment in receiving a malfunctioning unit.  I'm excited to say that shortly after contacting Biolite support, they cross-shipped a replacement to me and the new unit is working great (thanks Zach!).

It's winter here and unusually cold so my performance expectations for solar-powered equipment this time of year is low.  However, I was pleasantly surprised to find that I could get the BioHome's solar panel to output almost 50% capacity under these conditions.

![SolarHome screen displaying panel output of 48%](/solarhome_sun.jpg)

Throughout the day I moved the panel between several windows to keep it exposed to direct sunlight.  By the end of the day the battery was about 30% charged.  This might not sound like much, but this time of year the sun goes down before dinnertime and our neighbor's house blocks the sun for a good chunk of the day.

Having a charged battery gave me a chance to check-out some of the features of the system, but since I really wanted to test the capacity, I didn't play-around with it too much (I'll save that for a later post).

![SolarHome screen displaying battery 24% charged](/solarhome_battery.jpg)

What I do want is to see how far into the night that 30% charge would get us.  I connected two of the three included lamps (I didn't want to mess around with the motion-activated light yet) and set both to medium brightness as the sun went down (at this point the unit indicated 30% charge and estimated 4 hours remaining).

About an hour after the sun set we decided that the medium setting wasn't sufficient to illuminate the room and switched the lamps to high.  The control unit's estimate went from 3 hours to "battery almost empty", but the lights stayed on for another two hours at this setting.

I'm impressed with the accuracy of the estimations the unit makes in regard to discharge.  It will also estimate charging time, but this fluctuates wildly as the amount of light hitting the panel changes throughout the day.  I'm not sure if this is a simple point-in-time calculation, or if the unit keeps track of day-over-day patterns to produce more accurate measurements (not useful for ad-hoc setups but probably fairly accurate if the panel were located in the same place for a year or more).  Either way there seems to have been considerable thought put into the devices software as well as the hardware.

So far I'm impressed.  I've experimented with a number of DIY solar power setups and a few cheap off-the-shelf units and for the money I haven't seem something that is as "complete" as the SolarHome.  It will be interesting to see how it performs under "field" conditions (we'll bring it camping once the weather improves) and I plan on doing more extensive indoor testing to explore the edges of the device's performance envelope.

I'm also going to tear it down and see what makes it tick.  There's an undocumented DIN port (labeled "expansion battery" in the documentation) that I want to document, and I'm curious about the USB ports; do they connect only to USB power or is there perhaps a connection to the device's microcontroller for setup/debug?  I also want to see if there's any room inside the case where one might cram some additional functionality...?


---
title: "Son of OffGRiD"
date: 2019-04-09T14:04:48Z
draft: false 
tags:
  - hardware
  - offgrid
  - solar
---

I've spent a lot of time thinking about what I want to work on next, and in particular what project has the potential to become a "product" as well.

Since I was a kid I've wanted to have a computer company, and over the years I've imagined this in many forms.  For the most part the idea doesn't go much further than that, because I've never seen myself as the kind of person who could enjoy running a company, and most of the computers I want to build don't have an obvious audience.

However over the last few years I've received positive feedback for a number of computer designs or prototypes I've built, and the one that seems to strike a chord with others most often is [OffGRiD](https://gitlab.com/jgullickson/offgrid).  I've never built a working prototype of this machine because there's a number of tricky problems to solve, and it seems like each time I solve one problem, the solution to another problem becomes undone.  I think I can solve all the problems, but it's hard to justify the time, energy and resources required to build a computer for myself.  However, if the design could be re-used to benefit others, then it's easier to invest what's needed to get it done.

The other barrier has been the scope of the project.  There's a number of "unknowns" that have to be answered before the machine as-designed will be usable, and it's possible that I could spend a year (or more) building a prototype only to find out that it doesn't work the way I expected.  I personally don't have the resources to absorb a miss like this, which is another reason I haven't pursued the work aggressively.

The solution to this of course is to break-down the development process into smaller efforts that have testable results, but how to do this wasn't clear to me until very recently.  Spring rolled around and once again I wished that I had spent some of the winter months working on a computer I could use outdoors.  This time however I decided I'm just going to do something about it, even if I can't realize all of the design goals of OffGRiD. So I tracked-down a [Pixel Qi](https://en.wikipedia.org/wiki/Pixel_Qi) display (the screen originally selected for OffGRiD, but which has since gone out of production) and did some research to see what off-the-shelf computer I could cram it into (the results of this effort will be described in detail in a future post). 

Once I accepted this compromise it became clear to me how I might break-down the development of OffGRiD into a series of experimental machines, each yielding a useful computer with capabilities increasingly like OffGRiD. Each experiment will give me a chance to test the features I've planned for OffGRiD in the field, and determine whether or not they are as valuable as I expect them to be.  This process will also let me learn what I need to learn *incrementally* and there's less chance of spending a couple years building something that in the end isn't as valuable as I thought it would be.

There will certainly be "elbow" points along the way that will require significant investments of time or other resources to move forward, but hopefully the preceding experiments will have demonstrated sufficient value to justify these investments.  In addition to my personal experience with each prototype, I will be sharing the results with others (potentially building & distributing additional machines if there is demand), which could provide further justification for increased investment.

I'm also going to use this process to experiment with the idea of "non-scalable systems": systems and devices designed deliberately to work well at small scales and avoiding compromises that are typically made in the name of scalability.
---
title: Spares
date: Fri, 04 Sep 2015 16:11:13 -0500
author: jjg
draft: false
tags:
  - preposterous
---
One of the coolest applications for 3D printing in my book is the ability to
produce spares on-site.

![IMG_0048.JPG](/preposterous/assets/69-img_0048.jpg)

This isn't one of the most common consumables (and it's untested at this point
so who knows what will happen), but I thought it was a good example because it
took all of 15 minutes to design, 20 minutes to print and saved the time,
trouble and cost of having spares shipped around the world.

Replacement bolt for crossbow ( [ Amazon link ](http://www.amazon.com/K-8025
-Cocking-Tactical-
Crossbow-80-Pound/dp/B00NZ8O1AY/ref=sr_1_1?ie=UTF8&qid=1441401027&sr=8-1&keywords=cobra+crossbow)
).  Design files on Github: [ https://github.com/jjg/xbow_bolts
](https://github.com/jjg/xbow_bolts)

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Still alive test
date: Sat, 14 Dec 2013 17:08:20 -0800 (PST)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Just had a video post fail to show up, so testing to make sure we're not
borked.

—  
Sent from [ Mailbox ](https://www.dropbox.com/mailbox) for iPad

---
title: Tantillus Reprap Build Part 1
date: Sun, 16 Feb 2014 09:40:40 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Ever since I finished building my first Reprap from a kit I've been wanting to
build one "from scratch", using my first printer to make as much of the second
printer as possible (in the Reprap tradition).  Of the designs that were
floating around at the time, the one that captured my interest the most was
the [ Tantillus ](http://www.tantillus.org/) .  I've made several attempts to
print the parts needed to build the Tantillus, most times resulting in either
parts that were not good enough, or breaking my original printer, so I would
get frustrated, forget about it for a few months and then try again.

  

Over this time a lot of [ cool ](http://reprap.org/wiki/Rostock) [ new
](http://reprap.org/wiki/RepRap_Morgan) [ designs
](http://reprap.org/wiki/Simpson) have emerged, and each time I get inspired
to print another printer, I consider what's out there as well as designing
something new from scratch.  Each time I find myself back at the Tantillus.
There are plenty of practical reasons why (mostly because it addresses most of
the problems I've had with my original printer) but I think it also strikes a
balance between functional and aesthetic beauty that I find compelling.

  

So yesterday I decided to take another go at it after fixing a Y-axis issue on
my current printer.  The results speak for themselves:

  

![Inline image 1](/preposterous/assets/208-jason's _img_0314.jpg)  

  

  

On the success of this I printed the next big part of the Tantillus and again,
_success_ !

  

  

![Inline image 2](/preposterous/assets/208-jason's _img_0315.jpg)  

  

This was inspiration enough to take another run at building the Tantillus
printer.  While much of the printer is "printable", there are of course plenty
of "minerals" that will need to be tracked down, and it's a little tempting to
simply order all these bits online, but one of my main interests in building a
second printer this way is to avoid the pitfalls of the "kit" approach, and
really experience the ultimate goal of the Reprap project, which is to create
printers that can be reproduced "in-the-field", using only printed and
readily-available parts.

  

So instead of placing a few online orders, first I'm going to scavenge as much
as I can out of my own "inventory" (i.e., the buckets of scrap in the
laboratory).  I'll also hit-up my hardware-hacker co-horts for surplus that I
don't have in my own reserves.  Next I'll try to find any remaining components
within "bicycle distance" from home, and consider fabricating anything that I
can't find locally.  If after all that there's still things I need (I expect
some of the electronics, etc.) then I'll resort to ordering online, but I
really want to see how close I can get to finishing the project using the
things I print, recycle or source around town.

  

I'll be posting updates here as the project progresses, and I'll be keeping
track of the parts hunt in this [ Google spreadsheet
](https://docs.google.com/spreadsheet/ccc?key=0AkrbMuh-
gaxPdDRqWGV4NFJHSm1wRFBDQW1yb0pGQ2c&usp=sharing) for those of you who want to
follow along.  If you have any recommendations for parts sources or feel free
to pass them along or share your own build stories on Preposterous (yes I'm
planning to mention Preposterous in every post, part of the dogfood effort :).

  

  

\- Jason

---
title: Test human-readable blog names
date: Wed, 25 Dec 2013 01:19:23 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is test 1 of the humanization of blog names (as opposed to simply using
the raw hash).

  

  

\- Jason

---
title: Testing A thing from iOS
date: Sun, 07 Dec 2014 22:16:14 -0800 (PST)
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Here is a quick.  Test from iOS.

  

This isn't using the native mail app.  Well try that next.

  

  

\- Jason

  
\- Jason

---
title: Testing the autopilot
date: Fri, 13 Dec 2013 01:20:50 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Let's see if the auto update is working on skylab....  

  

\--  
  

---
title: The 30 Minute Button
date: Mon, 13 Jan 2014 07:10:34 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
  

This button is an exercise in seeing how quickly I could produce something
useful with my Reprap. I could have printed one of the countless useful things
on [ Youmagine ](https://www.youmagine.com) , [ Cubehero
](https://cubehero.com) or [ Thingiverse.com ](http://www.thingiverse.com) ,
but I wanted to include the design step in my experiment to accurately measure
the end-to-end (or mind-to-matter, if you will) process.

  

There's certainly faster ways to produce larger amounts of buttons, but all-
told I don't think I could have acquired a single button like this faster if I
had purchased one (travel time, etc.), and it's very unlikely I could have
done it less expensively.

  

  

\- Jason

[ ![](/preposterous/assets/198-img_0061.jpeg) ](assets/198-img_0061.jpeg)

---
title: The best programming language is the one you have with you
date: Sun, 2 Nov 2014 09:32:45 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I still get a surprising amount of resistance from programmers (and others)
when I talk about using Javascript.

  

My first computer was a Commodore VIC=20.  I'd used one or two computers
before that (my father's Gimix Ghost and my grandfather's hand-made SWTP) but
the VIC=20 was mine.  When you turned this machine on you were placed into the
BASIC programming language, just like almost every other personal computer
made at the time.  This meant that in order to do anything with these
computers, you had to write code, and if you used it for any amount of time,
you learned how to program.

  

Some people simply wrote down a "cheat-sheet" of the BASIC commands they
needed to load a program off tape (or less often, disk) but if you had an
ounce of curiosity or you were on a budget you learned how to write programs
because it was a way to make the machine do more than it could do out-of-the
box, and the only cost was your time.  It was also relatively easy.  Most
periodicals and books that discussed these machines included program listings
(source code) for all kinds of games and applications, written right in the
text of the pages that you'd simply type in to your BASIC prompt, use and
modify them to suit your own needs and save on tape for later use.  It never
occurred to me at the time (I didn't think about software licensing much
before age 10) but this was not unlike how open-source software development
works today, largely because the only way to transfer software this way was in
source-code form.

  

But what does this have to do with Javascript?  Compared to other programming
languages of the time (C, Pascal, etc.) BASIC, in a number of ways, sucked.
It was interpreted which made it slow, it was weakly-typed, the flow-control
structures almost encouraged the use of GOTO and most implementations required
using short, unintelligible variable names.  Compared to something like C,
BASIC was not the best programming language, but it was what we had.  On some
machines there were compilers for other languages available, but most of them
were expensive and only worked on the higher-end machines, and the code they
produced was binary, so it only ran on the type of computer it that they were
compiled on.  Bear in mind that 80's microcomputers used an assortment of
processors and some varied wildly in their hardware architecture, making
binary-distributed programs incompatible between machines (even sometimes
machines of the same product line).  However programs written in BASIC and
distributed in source-code form often ran unmodified on machines from
different companies, or if not they required small modifications that most
owners were capable of.

  

Regardless of its limitations when compared to other languages, some very cool
things were built with BASIC, but probably the most important thing created
this was was a generation who saw the personal computer not as a thing to run
applications or a game console but as a blank slate, capable of becoming
anything you wanted, without having to buy anything else, limited only by your
imagination, time and gumption.  The pervasive access to the BASIC language
provided a way for these programmers to write software with confidence that it
could be shared, unencumbered by technical limitations, and work on the
inexpensive computers of their friends and neighbors.

  

As microcomputers evolved there were efforts to address the limits of BASIC or
to replace it altogether, and the result of those efforts reduced the number
of computer owners who learned to program, and made the programs writing by
the remaining programmers harder to write and share-able and compatible.
Eventually personal computers began shipping with no programming languages at
all, and the era of the computer "user" began.

  

Today we live in a world where computers are everywhere but programmers are
rare.  Many people could benefit from the ability to make their computers do
more than they were designed to do, but there is an impression that this is
hard, and should be left to professionals (I'm not going to say the software
development industry is completely responsible for this impression, but it's
advantageous for them to maintain it).  Given the knowledge that the tools are
available to them, and the encouragement I think that most "users" have the
potential to become programmers and not only do I think it would help them
solve specific problems, it would change the way they look at the relationship
between their computers, the companies that make them and themselves.  The
personal computer is a magical thing that can create enormously valuable stuff
out of almost nothing, but we've constructed a world where most people who use
them are oblivious to this potential and this is exploited in ever increasing
ways.

  

Javascript has flaws, and is not perfect for every application, but it is
available on almost every personal computer made today, and I've seen some
very amazing things built with it.  The computers we have don't make it as
obvious as the microcomputers of yore, and the culture that separates "users"
from "programmers" is strong, but with the right tools, culture and attitude I
think we could turn the tide.  In a world that is increasingly built out of
software this is as important as it ever has been.

  

  

\- Jason

---
title: The Black Proposal
date: Fri, 14 Aug 2015 09:15:09 -0500
author: jjg
draft: false
tags:
  - preposterous
---
This document is what created the [ National Center for Supercomputing
Applications
](https://en.wikipedia.org/wiki/National_Center_for_Supercomputing_Applications)
, among other things:

http://www.ncsa.illinois.edu/20years/timeline/documents/blackproposal.pdf

It's a good read, and it's compelling that it was successful in garnering
investment.  I feel like a lot of the potential has gone unfulfilled, and a
generation or so later, I feel like we've lost a lot of ground in terms of
training programmers to make the most out of these supercomputer-style
architectures (which are now part of most consumer computing devices).

Reading this has inspired me to evaluate what a "sequel" to this proposal
might look like today.  Something that would inspire the current and future
generations of programmers to lay down the hoe and learn how to realize the
true potential of modern computing machinery, and turn those results over to
not only University scientists, but to everyone who has an idea that can be
explored more effectively through high-performance computing.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: The Cannondrone Run
date: Tue, 25 Aug 2015 11:23:15 -0500
author: jjg
draft: false
tags:
  - preposterous
---
The idea is simple: A cross-country race between drone aircraft, inspired by
the infamous [ Cannonball Run ](https://en.wikipedia.org/wiki
/Cannonball_Baker_Sea-To-Shining-Sea_Memorial_Trophy_Dash) .  Explaining the
purpose is a bit more complicated.

Drone technology has evolved quickly over the last decade and is at a point
where off-the-shelf quadcopters capable of doing useful things (flying cameras
around, etc.) can be had for under $100USD.  This is pretty cool, and it also
has had some negative side-effects.

I think this technology could have some significant life-improving
applications, but I believe that it is in danger of the same kind of " [
satisficience ](https://en.wikipedia.org/wiki/Satisficing) " that has been
reached in computers, mobile phones, etc.

The proposed race will push both the technological and the sociological limits
of what is being done with consumer drone technology today, and the
development of technology necessary to compete (or even complete) the race
will dramatically broaden the application possibilities for drones.

Let me give you an example: One pro-social use of drones would be to deliver
library books to rural communities, however the most common quad-copter-style
drones lack the range to do this on their own.  In order to compete in this
race, advances in battery capacity, en-route charging or in-flight charging
would need to be developed, which would then allow drones to be used for this
application.

Another important aspect to the race is that it pushes the boundaries of the
law.  Based on current FAA requirements a drone can only be operated within
line-of-sight of the operator.  This means that in order to participate in the
race, the operator will either need to find a way to meet this requirement, or
develop technologies that can evade detection and capture.  There are a number
of creative solutions to these problems that I can imagine (and countless more
that I can't) and both have very practical application in positive drone use
scenarios.  A technology that allows an operator to preserve line-of-sight
operation increases safety and potentially social interaction with drone work.
A technology that avoids detection will also avoid collisions with other
aircraft, and be capable of delivering packages in a more secure fashion.

One final advantage to the somewhat "outlaw" nature of the race is that it
will discourage corporations from direct involvement.  The have too much to
loose if they get caught, and there's no "branding" opportunity in the
traditional sense.  This will encourage independent participation and foster
technological development at the grassroots level where it is less likely to
become encumbered with intellectual property constraints or compromised in
order to be "monitizable".

There are countless other advantages that I won't attempt to enumerate here.
What I will share is what I see as the essential elements of how the race
should be held:

1\.  The first race should be coast-to-coast in the United States as a nod to
the event that it's based on.  Races elsewhere are very much encouraged, but
key is that there is sufficient geographical distance to force anyone
participating to come up with technology beyond what's available off-the-
shelf.

2\.  The start point of the race is announced in advance, but the end point is
not announced until the race has begun, ideally in a way that only
participants are aware of it.

3\.  Cooperation will be rewarded

There may be other constraints, but I really don't want to make it
complicated.  The idea being that any kind of "cheating" serve the higher goal
of encouraging open, independent research & development to take drone
technology to the next level, and open a new frontier of applications.  The
secondary goal is to push the social and political boundaries of how drones
can be used and derive intelligent ways to address the fears that result in
restrictive legislation.

If this piques your interest I've setup a room to discuss the pros, cons,
risks and rewards of such a race (should it ever take place) at [
https://hack.chat/?cannondronerun ](https://hack.chat/?cannondronerun)

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: The Computing Middle Class
date: Tue, 3 Jun 2014 11:22:19 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
A middle-class of computers users has developed (not to be confused with a
financial middle class) between users and programmers.  I'd like to come up
with a clever name for them, but I'll leave that as an exercise for the
reader.

  

Unlike users, they expect to be able to program.  Unlike programmers, they
expect user-level affordances.

  

To some degree these were the original computer users, back when computers
booted to BASIC, before the emergence of the User class and the separation of
Users from Programmers.  The last effort to preserve this original user-
programmer class was Hypercard, and some some degree Lotus Notes, Microsoft
Access, etc., but users of these programs rarely consider themselves capital-P
Programmers.

  

I think the world is a better place when experts outside of computer science
can create their own software, but I think that since the end of Hypercard,
the tools available to ambitious computer users have taken the wrong path, and
created a class of "programmers" who are uninterested (or at least not
passionate about) programming itself, and only see it as a means to an end.
There would be nothing wrong with this if it were not for the fact that these
programmers are writing software for unsuspecting Users.

  

I'd like to remedy this by creating a computing platform where motivated users
are able to use existing software as well as modify existing software and
create new software entirely themselves, in which the system treats this
software as first-class applications, just like software written by
professional Programmers.  This requires a fresh look at creating software
development tools that provide User-level affordances and don't get in the way
of what is trying to be done.  It cannot allow software written by big-P
Programmers to appear superior in meaningful ways, and should not encourage or
demand that User-generated software be marketable or sold in order to be
valuable.

  

I have some ideas for this, but that's another post.

  

  

\- Jason

  

  

  

---
title: The first application 
date: Fri, 11 Jul 2014 19:29:00 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I want to build an application that runs on top of JSFS because it's hard to
describe just how powerful it is abstractly; but I struggled for a long time
deciding what to write first. Then the obvious answer finally struck me: an
editor. An editor (and more specifically, a "programmer") would allow further
development to occur within JSFS itself, simultaneously serving as a
demonstration as well as an demanding application to drive the system forward.
I feel really dumb for not realizing this earlier, it's kind of the go-to
starting point for every general-purpose operating system. \- Jason

---
title: The Flobots
date: Thu, 15 Oct 2015 19:08:39 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I first heard the Flobots at _Target_ .  
  
I thought maybe it was Eminem, because that seemed like something that might
be playing at Target.  Listening closer thought it became obvious that there
was something more intelligent going on.  
  
The song was "Handlebars".  I don't claim to know the artists intent, but to
me it is a song about the duality of power in the form of intelligence.  How
it can be good or evil and how sometimes it's hard to differentiate, even when
it's used the same way.  It struck a chord with me.  
  
Once I figured out the name of the band I hunted down a copy of the album
containing "Handlebars"; "Fight With Tools".  I was so moved by "Handlebars"
that I had high expectations for the rest of the album.  I was a little
surprised by the varied style of the music, it reminded me a little of the
rap-metal-funk we saw in the 1990's but with something new as well, and
lyrically, it took me back to my punk-rock days.  It was like having Dead
Kennedy's music that discussed events I was old enough to remember happening.  
  
I anxiously awaited their next release and have to admit I was underwhelmed.
I wasn't disappointed, it was still a good record, but it just didn't resonate
with me the way FWT did.  
  
Then "The Circle in the Square" came out.  For me this was an huge leap over
Survival Story and cemented for me that Flobots would be a band I'd be into
for a long time.  
  
It's hard to quantify what I love about this band, but that's probably because
they are more than just a band.  Like a lot of my other favorite musicians,
music is just a way to get their thoughts and observations out there, and each
song turns into research and study, and in the case of the Flobots, connecting
with active movements in the world working on the kind of changes I want to
see happen.  Their latest effort underscores this aspect of the group, and
they are directly working to raise themselves (and I think other musicians as
well) above the world of entertainment and into something new entirely.  
  
The #noenemies project ( [ https://www.kickstarter.com/projects/974127639
/flobots-two-new-albums-for-2016-noenemies
](https://www.kickstarter.com/projects/974127639/flobots-two-new-albums-
for-2016-noenemies) ) stands to be a prototype for other artists to rise above
simply being entertainers and revenue generators for an industry that has long
exploited them.  Using music as a tool to enable, not enslave.  It's very
exciting and the kind of thing I've wanted to happen for a long time.  
  
Take a few minutes to check out the Kickstarter project video and if you share
my enthusiasm back the project.  I'd love for the success of this project to
inspire other artists to go this route and eventually, those of us working for
"the machine" in other lines of work as well.  
  
  
// jjg

---
title: The future is written in Javascript
date: Fri, 27 Dec 2013 23:37:58 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
There are a lot of programming languages, but there is only one that runs in
your web browser, and that's  [ Javascript
](http://en.wikipedia.org/wiki/Javascript) .  [ Ruby
](http://en.wikipedia.org/wiki/Ruby_\(programming_language\)) ,  [ Java
](http://en.wikipedia.org/wiki/Java_\(programming_language\)) ,  [ C#
](http://en.wikipedia.org/wiki/C_Sharp_\(programming_language\)) ,  [ Python
](http://en.wikipedia.org/wiki/Python_\(programming_language\)) ,  [ PHP
](http://en.wikipedia.org/wiki/Php) and  [ dozens more
](http://en.wikipedia.org/wiki/Web_development#Server_side_coding) are used to
write software for the web but in the end they all produce  [ HTML
](http://en.wikipedia.org/wiki/Html) and Javascript, and HTML is not a
programming language. That means that there's only one programming language
for the web, and that's  **Javascript** .  
  
If you're writing software for the web, you're programming in Javascript, it's
just a matter of how many layers of "comfort zone" there is between you and
your application.  
  
**But what about the servers?!?**  
Fine, great, the future of  _user interface_ might be written in Javscript,
but what about the server code?  
  
If you're a web developer you have of course heard of  [ Node.js
](http://en.wikipedia.org/wiki/Node.js) (if you haven't, you can get the scoop
[ here ](http://nodejs.org/) ). The short of it is that you can now write your
server code in Javascript as well as your client-side code, and Node.js has
been around long enough to have proven that it has the necissary ingredients (
[ performance ](http://zgadzaj.com/benchmarking-nodejs-basic-performance-
tests-against-apache-php) ,  [ scalability
](http://lanyrd.com/2012/dibi/sqtyb/) and  [ security
](http://wegnerdesign.com/blog/why-node-js-security/) ) to function in a
production environment.  
  
That being the case, why would you spread your focus accross multiple
languages to write a web application? Some will argue that Javascript isn't
appropriate for writing server-side code, that there is something in the
nature of the language or the runtime that just isn't right. I argue that this
is patently incorrect, and the only way to arrive at such a conclusion is to
have an inaccurate or incomplete understanding of the Javascript language.  
  
Javascript has been designed from the beginning to work in the event-driven
environment of the browser and excels at asyncronous processing. There was a
time where this was not a useful feature for server-side web software, but
since the proliferation of  [ AJAX
](http://en.wikipedia.org/wiki/Ajax_\(programming\)) techniques the old "page
at a time" method of server-side processing has gone by the wayside.  
  
Beyond the technical or implementation-level featurs that make Javascript
suitable for these tasks is the philosophy of the language and its constructs
that lead the developer into an event-driven way of thinking. Once embraced,
this mode of thought changes the way a developer designs applications and
opens them up to new ways of providing better, more responsive and more
engaging experiences to users.  
  
**But the future isn't all on the web right? What about mobile?**  
The future of mobile is Javascript as well.  
  
This isn't a radical idea, in fact Apple knew this when the first iPhone was
introduced back in 2007. Until 2008, the only way to write software for the
iPhone (unless you were an internal Apple developer) was to write it in
Javascript in the form of "  [ Web Apps ](http://www.apple.com/webapps/) ".
Early on, there was no indication that Apple would ever open up the iPhone to
allow third-party developers to write "native" applications for the iPhone,
and the fact that Apple provided Javascript API's to access device features in
these early versions of iOS is evidence that they saw the potential of
delivering fully-functional Javascript-based applications to mobile devices
(why did Apple change directions? That's  [ another story
](http://9to5mac.com/2011/10/21/jobs-original-vision-for-the-iphone-no-third-
party-native-apps/) ).  
  
Since then, the number of device features that have been made accessible via
Javascript API's has grown, and technologies like  [ Websockets
](http://en.wikipedia.org/wiki/Websockets) and  [ WebRTC
](http://en.wikipedia.org/wiki/WebRTC) make writing Web Apps that rival native
applications even easier, and that's just the tip of the iceberg. With the
introduction of  [ FirefoxOS ](https://developer.mozilla.org/en-
US/docs/Mozilla/Firefox_OS) (also known as "Boot to Gecko" or B2G), there is
now a platform where Javascript and native code applications are on equal
footing (a predecessor with a similar philosophy is  [ WebOS
](http://en.wikipedia.org/wiki/Webos) ), and by equal I mean feature parity,
however Javascript has several distinct advantages.  
  
**Developers, Developers, Developers**  
There is no other language with more developers using it than Javascript
(remember the first few paragraphs where we talked about how all web
developers are writing Javascript?). This means that, all else being equal,
the potential of Javascript applications is orders of magnitude higher than
"native" applications based on the sheer number of developers with decades of
experience behind them. What's more is that these same developers have been on
the cutting-edge of creating compelling user experiences (much of the original
use of Javscript was to provide realtime feedback and event-driven user
interface elements). These developers have been sidelined on mobile platforms
by restricted subsets of device features or second-class integration into the
user experience. With B2G, a whole new army of developers will be able to
build great applications for mobile, and as the iPhone has proven, great user
experiences drive the platform.  
  
**Developer productivity and other excuses**  
There is a case to be made that other languages, frameworks, etc. are more
than up-to-the-task of creating compelling experiences on the web (and
beyond), and that these tools and platforms lend themselves to increased
developer productivity. I could dispute these points as well but instead I'll
agree, that any language can be used to generate HTML and Javascript and
return it to a browser for processing, and that frameworks and toolkits and
the like can make creating applications more convinient for programmers
unfamiliar with raw Javascript and it proper use. I'll even go so far to say
that anything you can find on the web today could probably be created using
any of these toolsets, and if you have those tools (or those trained in those
tools) at your disposal then you can probably get more done faster by using
what you know.  
  
However, this essay is about the future, and if there's anything certain about
the future it is that you can't build it within the restrictions of models
from the past. If you're using a toolset it is a fact that creating something
that wasn't considered when the toolset was designed will be difficult if not
impossible, and in either case will doing so will compromise your vision.  
  
Javascript is the most complete programmatic interface avaliable to web-based
applications. Instead of mastering analogies and metaphores that insulate the
developer from the platform (and often mask its most exciting features), time
spent embracing this single language allows a developer to not only create
what is known, but to invent future applications and techniques that have not
yet been imagined by the authors and architects of today's convinience
frameworks.  
  
This, coupled with the ever expanding list of  [ exciting new technology
](https://github.com/leapmotion/leapjs) that is programmable using javascript
is what I mean by "the future is written in Javascript".  
  
**Easier said that done (or, "how do we get there from here?")**  
Understanding and even accepting this philosophy is one thing, but how to
transition from an existing application, or from previous experience, to
Javascript-based development? The path will varry for every developer and each
application, but what I can do is share the path that I have personally
embraced.  
  
_Learn the code_  
Most web developers have used Javascript code directly, and many have even
written it, but few have taken the time to learn Javascript as a language unto
itself. Read a good book and learn Javascript as if from scratch. Take care to
learn the  [ good parts ](http://amzn.to/108GJJg) , and learn how to avoid the
[ bad parts ](http://amzn.to/10xIwri) .  
  
Write Javascript programs in an environment where you won't get hung-up on
dealing with the browser and its DOM or third-party Javascript libraries
(Node.js is a great environment for this).  
  
_Draw a line in the sand_  
Javascript fits more naturally on the front-end than on the back end, and the
proliferation of REST and JSON-based API's makes implementing user interfaces
in Javascript much easier than it was in the past. Begin here by building
directly against these interfaces using client-side Javascript code, and
resist the urge to leverage frameworks and libraries whenever possible.  
  
Partition your existing applications by implementing a  [ Hypermedia API
](http://amzn.to/13a4eWC) . Use this as a "firewall" to push existing code
behind a clean surface to attach a Javascript client to. Refactor the code
behind the API over time and eventually replace it with service calls built on
Node.js.  
  
_Learn a new platform_  
Start developing for FirefoxOS, or develop Web Apps for other platforms. iOS
provides decent support for making Web Apps behave like native apps (including
launcher icons and full-screen modes that hide browser chrome). Android
provides a rich set of Javascript-accessible device API's as well, but for the
full experience FirefoxOS is really the way to go (in fact all of the built-in
software is written this way).  
  
There are as many paths over the mountain as there are developers, but the
first step is to look at Javascript not as an inconvinience or a compromise,
but as an elegant solution to a complex problem. A language which is
encumbered with demons from its past, but which are only harmful to those who
know not how to avoid them.  
  
Any of the steps above can be the beginning of this understanding, and all of
them can lead to building the applications of the future, with Javascript.  

---
title: The Network Defect
date: Sun, 20 Sep 2015 10:57:55 -0500
author: jjg
draft: false
tags:
  - preposterous
---
The hard part about privacy is that it's only as secure as the weakest link.
The technology is well established, but it only works if the people you want
to talk to are using the same thing you are.

Getting people to switch, especially if there's a financial cost involved
(essentially a requirement to ensure privacy) is notoriously difficult due to
[ the network effect ](https://en.wikipedia.org/wiki/Network_effect) .
However I have an idea that is nothing more than a re-framing of the
relationship between a product and it's customer that might help break this
effect.

Imagine a secure private message service akin to [ Silent Circle
](https://silentcircle.com/) , etc., but instead of charging each user a
subscription fee to access the system, you sell a subscription that allows
each customer to add some number of other users for no additional cost.  This
allows someone with a strong personal interest in the product to start using
it without having to convince others to purchase additional subscriptions.

There is a secondary effect at work here as well.  When someone asks you to
sign up for something that is free, it can feel like a burden, but when
someone gives you a gift, one that would otherwise come with a financial cost,
people tend to look at that differently.  I imagine there is a formal
psychological term for this phenomenon.  What I've observed directly is that
people tend to be willing to invest more time in something proportional to
it's cost.  So when you ask your friend to join a free service to talk to you
it may seem like a burden, but when you present them with a gift that would
otherwise cost them money, it's both more compelling, and also carries an
obligation to at least try it out (to appreciate the gift).

The counter argument to a system like this is that it can be "gamed" or
otherwise exploited.  A group of people who want to get a better deal could
simply sign one person up and then split the monthly fee, effectively getting
away with something.  It's probably obvious to you that is simply an socially-
orchestrated fall-back to the old one-subscription-per-user model.  If the
pricing for the service has been established correctly this has no negative
impact on the sustainability of the system, provides the subscribers with a
feeling of satisfaction and actually _reduces_ operational cost compared to
the original one-bill-per-subscriber model by reducing the administrative
overhead of billing each account.

The key is to establish a system whose operational cost scales in-step with
paid-subscriber revenue, and a paid-subscriber to unpaid-subscriber ratio that
fits into this equation.  This might sound difficult but here again it's
actually no different than the same service where each subscriber pays, except
that you multiply the subscription cost by the number of non-paying
subscribers each paying subscriber is allowed to grant free subscriptions to.
An interesting side-effect of this is that it is unlikely that every paid
subscriber will fill every unpaid subscriber "slot" they are granted, so the
revenue-per-paid-subscriber will actually be higher than a service which
requires every subscriber to be a paying subscriber.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: The New Blog
date: Thu, 10 Dec 2015 22:32:41 -0600
author: jjg
draft: false
tags:
  - preposterous
---
Closing out this account, _but the blog rolls on at:_

  

[ https://preposter.us/beryllium-stairway-nuts-tennessee/the-new-blog.html
](https://preposter.us/beryllium-stairway-nuts-tennessee/the-new-blog.html)

  

Catch you on the flip-side.

  
  
// jjg

---
title: The new thing
date: Wed, 21 May 2014 14:24:10 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I'm going to start a podcast. First episode recorded next Tuesday. Stay
tuned... \- Jason

---
title: The Next Film
date: Wed, 12 Feb 2014 23:11:28 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Here's a little sneak-peek at what's in store for our next film...

  

![Inline image 1](/preposterous/assets/207-next.png)

---
title: The personal touch
date: Tue, 18 Feb 2014 12:51:27 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/209-image.jpeg) ](assets/209-image.jpeg)

---
title: The Programming Den
date: Sun, 22 Jun 2014 12:17:27 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
In a quiet backstreet of San Franscisco, in the basement of an unassuming
laundry, one can find the dim, quiet retreat of those seeking [ the
programmer's high
](http://www.slate.com/articles/technology/technology/2014/06/coder_s_high_the_intense_feeling_of_absorption_exclusive_to_programmers.html)
.

  

Quiet and cool, these private clubs provide a safe place where programmers can
indulge themselves, surrounded only by others of their kind, for hours, days
or perhaps weeks on-end, engulfed in the bliss and serenity of uninterrupted
code.

  

Not open to the general public, membership requires passing an initiation that
is elementary for the initiated programmer, but sufficient to ward off
tourists, gawkers and pointy-haired bosses.  The only rules are a vow of
silence, and a "programmers only" constituency.  All who violate these tenets
are quietly asked to leave.

  

No talking, no phone calls and certainly no Google Hangouts, The Programming
Den is strictly for programming only.  Amenities vary from location to
location, some spartan in furnishment while others luxurious, with retail and
refreshments catered specifically to the hackerish clientele.  Staff work
quietly isolated from the programmers, and accommodations are provided for
members who need to make noise (sometimes phone booths and private rooms,
other times an alley behind the building).  The key affordance is room to
concentrate and ample Internet bandwidth.

  

As dens form like pools around the world where concentrations of programmers
grow, the only franchise agreements are that the rules above are adhered to,
so that travelling programmers can expect a welcoming environment anywhere
they find the SSID "the_programming_den".

  

  

  

  

_Inspired by my personal experience of The Programmer's High,[ David
Auerbach's ](http://davidauerba.ch) recent Slate article entitled " [ The
Coder's High
](http://www.slate.com/articles/technology/technology/2014/06/coder_s_high_the_intense_feeling_of_absorption_exclusive_to_programmers.html)
" and the [ opium den ](http://en.wikipedia.org/wiki/Opium_den) culture of the
early 20th century - [ Jason ](http://twitter.com/jasonatmurfie) _

  

  

  

---
title: The value of recalibration
date: Thu, 19 Dec 2013 00:32:55 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Today was a lot of trying and failing.  Among other things, I really wanted to
print some things on the Reprap but each time it went wrong and the whole
thing was very gumption depleting.

  

It wasn't hard to realize that the machine was out of adjustment, and no
amount of luck or perseverance was going to change that.  But I "just wanted
it to work", and I didn't want to be bothered with calibrating it, so I just
got frustrated and turned to working on something else (there's always
something else to work on after all).

  

A few hours later Jamie made some really cool crafts and shared them with me.
Seeing them, and seeing the work that went into them, I suddenly had the
motivation to make the printer work again, and I was even excited about the
process of recalibrating it.

  

I think it's easy to get frustrated when focus on an end goal creates an
illusion that the steps between where you are and where you want to be are
unimportant or nonexistent.  To think that the goal even exists without the
steps leading to it is absurd if you think about it, but it's easy to loose
sight of this in the heat of the moment.

  

Even though reigning from a pursuit can feel like failure or giving up, often
this is one of these invisible steps, and in my case a necessary one to put me
on the path to mustering the ambition necessary to complete the entire course.

  

  

\- Jason

---
title: The weird A thing.
date: Fri, 6 Jun 2014 18:16:33 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Is the weird "A"-like character that's been showing up in my recent posts
specific to posts I make from my Chromebook?  I'm writing this from iOS to see
if it still shows up.

  

I see in old posts the character isn't there, so if it's _not_ chromeOS or
maybe gmail **web** client specific, it is a true mystery.

  

Let's see!

  

  

\- Jason  
  
Sent with my thumbs

---
title: This is the second post
date: Thu, 12 Dec 2013 22:44:17 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
The second post to the awesome new old blogging platform.

  

I'll add a few linefeeds and other things to see if we can make the posts look
_more_ like real **blogging** !  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

---
title: Thoughts on an FPGA Cloud
date: Tue, 17 Jun 2014 08:14:30 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
We were talking about applying [ FPGA ](http://en.wikipedia.org/wiki/Fpga)
technology to a specific engineering problem at Murfie when [ Preston
](https://twitter.com/gl33p) asked "has anyone built a cloud of FPGA's?".

  

It was one of those questions that as soon as you hear it, it seems like an
obvious thing to do, but I'd never heard of it being done, and it's certainly
in the center of the [ Venn Diagram
](http://en.wikipedia.org/wiki/Venn_diagram) of my interests.  Some casual
research doesn't reveal the existence of such a thing, but it does show why it
might not yet exist.  FPGA products lean heavily in the direction of embedded
electronics, aimed at being incorporated into a piece of hardware and
interfaced with various other physical bits (sensors, actuators, etc.).  This
application doesn't really lend itself to the "... As A Service" model, but
it's certainly not the only application for FPGA's.

  

Pure software systems can benefit from FPGA implementations if they contain
the right kind of problems.  An easy example of this is the [ Bitcoin miner
](http://en.wikipedia.org/wiki/Bitcoin_miner#Bitcoin_mining) , an extremely
compute-bound problem which can be executed much faster (and with dramatically
lower power consumption) using an FPGA implementation of its core algorithm.

  

While it's true that most software lack such a clear need for hardware-based
acceleration (and the associated engineering & design), I think that a lot of
modern software has "pockets" of this kind of processing, and especially as we
build systems that rely more and more on hashing algorithms, encryption and
also become more savvy about using asynchronous and parallel-processing
architectures.

  

So to me the missing piece of a valuable "FPGA [ PAAS
](http://en.wikipedia.org/wiki/Platform_as_a_service) " is an efficient means
of expressing existing or new software in ways that can leverage the
performance, efficiency and flexibility of FPGA hardware.  To some degree this
is a self-solving problem, history shows that making tools available to
developers grows the base of developers who know how to use the tools.  In
addition to expanding the knowledge of leveraging FPGA tech in software, I
think that a properly designed platform could automate the process as well
(runtimes and compilers capable of identifying candidates for optimization,
potentially generating the optimized implementation on-the-fly).  A " [ soft
core ](http://en.wikipedia.org/wiki/Soft_microprocessor) " ARM processor,
implemented in an commodity FPGA will run at about 2/3 the speed of its
traditional silicon implementation, but the soft processor could also monitor
the code running on it and adapt itself to run that specific code much more
efficiently than a general-purpose processor.

  

Having given this some thought, I think there is potential for a cloud of
FPGA's, even if nobody knows it yet.  Projects like [ HP's "Moonshot"
](http://h17007.www1.hp.com/us/en/enterprise/servers/products/moonshot/index.aspx)
and the broader push toward modular, configurable computing point this
direction as well, and it's hard to imagine a more dynamic platform than one
which can change it's fundamental hardware implementation at will.

  

  

  

\- Jason

---
title: Thoughts on JSFS 4
date: Sat, 14 Nov 2015 08:25:04 -0600
author: jjg
draft: false
tags:
  - preposterous
---
On Friday I pushed the first  (and second) release of the 4.x series of [ JSFS
](https://github.com/jjg/jsfs) to Github.

I've been thinking about JSFS for a couple years and have been actively
working on it for the last year or so.  The 4.x series is the result of some
pretty extensive field-testing which lead to a complete re-evaluation of what
is most important to the system.

I started working on JSFS with two features in mind:

  * Storing and retrieving data via an HTTP API 
  * Efficient storage of large amounts of data 

These needs were met by implementing an HTTP server capable of handling GET,
POST, PUT & DELETE verbs, and performs block-level deduplication on incoming
data before storing it to disk.  Along the way I experimented with a number of
additional features that seemed useful to some of the specific applications,
or related ideas I wanted to explore.

The zenith of the JSFS featureset came during the 3.x series, by which point I
had added directory indexes, encryption, federation, appendable objects,
websocket support, versioning and more.  These features worked great for small
storage pools (<1TB), but production systems with larger pools began to
experience performance degradation based on the implementation of some of
these features.

In some ways this wasn't a surprise.  Design choices made in early
implementations of JSFS were starting to show their limitations in ways that I
expected (memory consumption) and in ways I didn't anticipate (ever increasing
start-up times).  Steps were taken to mitigate these issues within the current
implementation, but ultimately it became clear that there were aspects of the
current design that had intrinsic limitations or weaknesses that would need to
go away if JSFS was to scale beyond managing a few terabytes.  Additionally,
the code had grown considerably as features were added, which made testing,
debugging and improving the it much more complicated and time consuming.  As
JSFS was now a production system, I needed to take aggressive steps to resolve
these issues.

Over a weekend I came up with a re-implementation of the storage core that
would eliminate the relationship between pool size and memory.  This approach
eliminated the "memory-resident superblock" which was the source of numerous
problems, but it was also central to the implementation of several features.
I thought I could re-implement some of the features in the new design, but it
would be a "back to the drawingboard" operation so it wouldn't happen soon.
As I took time to mull this over, production issues mounted and I had to make
a move.

Depreciating features is never fun.  Removing functionality from software is
guaranteed to make users unhappy, even when it's something that's only used by
a handful of people and yourself.  But I think it's important, because I think
a lot of great software gets ruined by pursuing new features instead of
improving it's core ones.  In my case this happened because there was "spare
time" between the point when JSFS's core features were implemented and when
the limitations of this implementation were found.  Other causes of this
problem are product or marketing related.  Regardless of the cause, when it
become apparent that a product is letting users down because it is failing to
deliver on it's original functionality, something must be done, and often
cutting features is the fastest and most reliable way to get there.

I didn't intend for this to become a philosophical post, but that's OK.  The
tech world is probably more in need of improved philosophy than technical
documentation.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Thoughts on JS/OS 1.0
date: Thu, 19 Mar 2015 08:31:44 -0500
author: jjg
draft: false
tags:
  - preposterous
---
So my thought is to get a node.js runtime operational directly on top of the
Xen Hypervisor and use this as the foundation for JS/OS.  With nothing more
than this, I should be able to support everything that JS/OS itself needs to
do from a server perspective.

  

To add a user interface layer is a matter of providing enough support to
render a web browser, which means support for a display and the various
underpinnings necessary to render HTML visually (I imagine handling typefaces,
vector and bit mapped graphics, etc.)

  

This stage, the visual user interface, is where the complexity skyrockets.  On
one hand there are plenty of existing open-source codebases to build on, but
they have an extraordinary amount of dependencies, many of which exist to
support functionality that will go unused in JS/OS.  On the other hand writing
something like this from scratch (as well as maintaining it as HTML
specifications change) is a daunting task.

  

I'm tempted to skip it altogether and move on to the next generation of visual
user interfaces: _virtual reality_ .  Perhaps surprisingly implementing a VR
UI from "scratch" is far less intimidating and actually familiar territory for
me.  This does mean giving up a potentially large range of applications that
JS/OS could be used for during the transition from existing GUI's to
commonplace immersive UI's, but reservations about that might be more of a
cognitive hurdle than a real one (after all, consider how quickly we've moved
from mouse-based desktop metaphors to tactile interfaces).

  

Of course if you're giving up the browser and HTML then you really have to
reconsider the value of JavaScript as the system programming language.  I've
said that the future of the web is written in Javascript, but without the
browser and HTML we're not really talking about the web anymore are we?
Furthermore JS/OS is a solution for the problem of developing web software,
and I've already discussed elsewhere that I feel the tipping point for the
widespread application of VR technologies will be when VR development becomes
self-hosted and self-sustaining, and it isn't clear that a VR platform build
on top of Javascript would fit that bill.  I'd argue that it would be an
incredible coincidence if that was the case.  More likely the answer will come
in a form that won't be obvious until we've spent enough time in virtual
environments to get in touch with their underlying patterns and natural
methods of interaction.

  

So where does this leave JS/OS?  From one perspective it is a dead-end, if you
believe that I'm right about VR.  However there is probably a lot of road
between where we are and the end of that road (perhaps a decade or even a
generation).  I also think there are architectural lessons yet to be learned
from JS/OS that will pertain to any distributed system which can be applied to
a future VR-based system even if none of the direct implementation is shared.

  

The question for me then comes back to the right way to provide a UI for JS/OS
applications, and I'm leaning toward building something from scratch, but I
plan to investigate the existing options more deeply before committing to
something that ambitious.

---
title: Thoughts on Preposterous 1.0
date: Fri, 13 Dec 2013 11:31:10 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Things are moving rather quickly.  I'd like to say that the code is almost
ready for an initial useful release, but there's a few important things that I
think need to be in place first (most of these have an entry in the [ Github
Issues list ](https://github.com/jjg/preposterous/issues?state=open) already).

  

Probably the most important is handling embedded images.  Right now not only
do emails with embedded images not work right, they cause an error that makes
them get ignored completely.  A good implementation would handle these
seamlessly and preserve their formatting as well as any other HTML formatting
in the email (I think most others are being handled OK right now).

  

Along these lines, handling plain-text emails needs to be reviewed.  I really
want Preposterous to be useable by simple devices that don't necessarily deal
in HTML email so I want to test and review the code to make sure plain text
emails are handled as nicely as possible.

  

Another are to noodle on is the rendered html itself.  I don't want to go too
far down the rabbit hole with this, but right now the html generated is
straight-up invalid.  I think the right way to do this is to add enough logic
to generate valid HTML with as little formatting as possible.  I also have
some tricks in mind that would allow users to edit the templates used for the
index and posts views, which if I can make these work w/o undermining the
simplicity and stability of the app I'll put those in, but I don't consider
them necessary for a v1.0 release.

  

I'm not big on freaking out about security, but I would like to verify that
we're not doing something _really stupid_ , like providing a vector for
running arbitrary code on other people's servers, etc.  To be clear, I'm not
sure that I want to prevent the ability to run "sandboxed" code on
Preposterous (code that can only affect contents within a single blog), but I
want to give the code a once-over from a security perspective to make sure
people can deploy it without lying awake at night.

  

Configuration needs to be separated from the main script.  Not sure the best
way to do this without making it overly complex, but giving that some thought,
and if it's going to be easy to update (by simply pulling the git repo) it's
something that will need to be addressed.

  

Considering the sort of the index pages.  Seems like these should be sorted in
reverse-chronological order (instead of straight chrono as they are now) but
it seems like there's a better way to address that, along with things like
search, that don't violate the simplicity, statelessness and ability to
function as static html.  That said maybe I'm over-thinking it and should just
render the static indexes the way the rest of the world thinks blogs should be
sorted.

  

There is some sort of internal housekeeping stuff to address as well.  The
slugs generated for filenames need to not have things in them that can't be
accessed correctly by the web server, and there are other formatting
considerations to consider.  There's also maintenance things like re-
generating the entire server from IMAP data (pretty easy, but probably should
avoid doing things like re-sending notifications for every post, etc.) and of
course there are larger scalability considerations like how long to keep the
emails in the IMAP box, etc.  In this vein, the idea of providing some sort of
"bundled" http server is a consideration.  I'm pretty sure that if you're
interested in running a Preposterous server, and you don't already have an
http server that can serve static HTML, you're probably OK using Python's
SimpleHTTPServer until you outgrow it.

  

That's pretty much what's on my mind for a 1.0 release.  There could also be
improvements to notification/welcome messages, better documentation, other
stuff I'm not thinking of, but I think with the above in place it would be
useful enough to consider a production release, and good enough that content
created with it would be stable and upgradeable as features and such are
implemented in later releases.

---
title: Thoughts on Preposterous 2.0
date: Fri, 27 Dec 2013 23:29:35 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
As bugs fall and new features are rolled out, I'm working out what's left to
be done before Preposterous 2.0 can be considered complete.  Personally, I
want Preposterous 2.0 to be something I can replace my existing Wordpress blog
with so with that in mind, here's what I think needs to be done:

  

**Delete support**

Right now you can effectively delete a post by sending an empty email with the
same title, but this doesn't remove the post from the post index.  If I can
make this happen, then Preposterous will have complete CRUD support.

  

**Custom domains**

Since I'll be replacing the blog currently at [
http://www.gullicksonlaboratories.com ](http://www.gullicksonlaboratories.com)
I'll need to sort out the best way to go about implementing custom domains.
Since Preposterous generates static files this should be pretty
straightforward, but there's a few ways to do it and I'd like to see if I can
automate the process so I can make it available to other users as well without
having to manually manage configurations, etc.  It also brings up the issue of
customizing things like the blog templates, etc. which I have ideas for but
haven't completely tested yet.

  

**Comments**

I'm not sure if comment support is going to be a requirement of Preposterous
2.0.  I know how I _want_ to add support for comments, but I haven't figured
out exactly how to go about it yet.  I'm also not convinced that comments are
necessary, and they certainly are not without consequences.  If I can get the
implementation I have in mind working I might include it in 2.0 but I'm not
going to hold back the release for it.

  

In addition to the things I need out of 2.0, there's a number of things I'd
like to include to make Preposterous better for all users, and especially for
new ones:

  

  * Better documentation for users 
  * Better notification emails 
  * Better support for attachments (inline and otherwise) 
  * Acknowledgements (give credit where credit is due) 

  

I'm also looking for feedback and suggestions from existing or potential
users.  If there's something you'd like to see in Preposterous 2.0 or future
releases, reach out to [ @preposterous_me
](https://twitter.com/preposterous_me) or [ @jasonbot2000
](https://twitter.com/jasonbot2000) on twitter or post an Issue to the [
Github repository ](https://github.com/jjg/preposterous/issues?state=open) .

  

  

\- Jason

---
title: "Thrashing"
date: 2019-01-30T16:36:03Z
draft: false 
tags:
    - k40
    - lasers
---

I've been feeling overwhelmed by my project backlog lately.  I have so many in-progress projects that are stuck in [wait states](https://en.wikipedia.org/wiki/Wait_state) that I'm having a hard time making any progress on anything.  When I look at the list (and especially when I look at the lab) it's a bit crippling and hard to imagine how to wrangle it all.

I get myself into this situation because of two interlocked problems.  The first problem is that I take-on some projects that are too ambitious to finish in a short period of time.  Often these have external or internal dependencies that can take a long time to resolve, and often leave the project "blocked" until I'm able to muster the resources to move-on to the next step.

The second problem is that I get bored during these "blocked" periods and take-on smaller projects to fill the gaps.

When the secont type become the first type, I get *really* stuck.

Right now I have a list of projects that are in this state, enough that when I try to enumerate the dependencies in my mind it exceeds the capacity of my [call stack](https://en.wikipedia.org/wiki/Call_stack).

After some thought and feedback from friends on Mastodon I decided the best way to tackle this is to serialize the work and commit to working on only one project at a time until I can find a way to avoid these [deadlocks](https://en.wikipedia.org/wiki/Deadlock).  Then it's just a matter of ordering the list and for this I'm choosing to order the list by dependency, with the objective of prioritizing work that will unblock the most other projects.

That being the case, the highest-priority project is getting the laser cutter online.

![k40 laser unboxed](/k40_unboxed.JPG)

Several of my other projects either rely on or could be greatly-accelerated by lasercut parts, so getting the cutter into production should provide the most leverage in terms of getting other things done.  Unfortunately this is also one of the more complex projects, primarily due to the infrastructure involved.

So it's tempting to choose something else, but I'm going to try and be disciplined about this and not take the easy way out.


---
title: Threshold (Makibox Part 1)
date: Tue, 6 May 2014 13:08:32 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I should have been better about posting my [ Makibox LT
](https://store.makibox.com/#/product-detail?type=1&&option=31) build, but
honestly it went so fast I barely had enough time to take photos.

  

![](/preposterous/assets/227-img_0903.jpg)  
  

That said, it wasn't the smoothest of assemblies, and I'm not alone in that
experience.  Having put the machine together and made it through the first
print, here's my impressions:

  

The parts of the machine do not fit together well.  The tabs on the plastic
parts of the case are too large for their corresponding holes and require
force to join.  Some people have altered the tabs with a file to compensate
for this, but I was able to overcome the resistance of my parts with brute
force (for better or worse).

  

The extensive use of non-self-tapping screws in non-threaded plastic mounting
points makes assembly much more difficult than it has to be, and easily
results in misaligned parts and cock-eyed screws.  It's also hard to know how
much pressure can be applied without stripping the screw "holes".

  

There is much play in the X and Y axis.  Adjustment here did improve things
somewhat, but there's still flex when the axis changes directions.

  

Taking one step at a time, once the machine was together I tried extruding
some filament by hand.  The first failure here was getting the hot end to
temp.  After initial assembly, my host software ( [ Octoprint
](http://octoprint.org/) ) was reporting a nozzle temp of -40c, which
indicates an open circuit between the controller and thermistor.  Based on
what I had read I feared this meant a dead hot-end, but after some basic
continuity testing it turned out to be a bad connection in the wiring harness
between the controller and hot-end, easily worked around with a little tape
and a connector from the lab.

  

  

![](/preposterous/assets/227-img_0912.jpg)  
  
  

With that behind me things didn't get much better, and as expected it wasn't
possible to extrude the PLA plastic that came with the printer without
jamming.  The source of the jam is one I'm familiar with, because it's what
happens when heat creeps up the barrel and causes the filament to expand
before it reaches the melt zone, creating a "cork".  I've had precisely the
same problem with the QU-BD hot end, and occasionally even with J-Heads that
are not actively cooled.

  

  

![](/preposterous/assets/227-img_0916.jpg)  
  

  

Since I'd seen this before I figured it could be solved with some active
cooling, but I didn't want to invest a lot of time unless I was sure, so I
took the Makibox down to the laboratory and grabbed the biggest fan I could
find.  Using a little gaffer tape, I shaped the outlet of the fan so it would
blow only on the barrel of the hot end, and with this I was able to extrude
PLA reliably.  Of course this setup wasn't going to work long-term, but it was
good enough to attempt a print, and after two failed attempts (described
later) I was able to get a test piece to complete successfully.

  

The results were as expected.  There is a "lean" to the print that is likely
caused by the slop in the X & Y axis as they change direction, but other than
that most of the defects could be chalked-up to a lack of tuning.  I won't
know for sure until I take the time to tune temperature, extrusion, etc.

  

![](/preposterous/assets/227-img_0935.jpg)  
  

  

At this point it would be easy to assess the Makibox as a disappointment.
Clearly it's not capable of printing PLA without modification, and it's likely
that overcoming the errors in the X & Y axis will require modifications as
well (it's unclear at this point if Z errors are contributing to lower print
quality).  That said, at $200.00, it's also easy to see the Makibox as a
slightly over-priced set of Reprap electronics.

  

My next step is to determine at what point is time spent troubleshooting the
Makibox better spent recycling it's parts to complete another Reprap project
(namely my [ Tantillus ](http://www.tantillus.org/Home.html) ).  At a minimum
I plan to spend a few more hours performing basic tuning on the Makibox to see
if I can improve the print quality, but what's less clear is if it will be
worth the time and effort to address the hot end cooling issues, since that
could involve significant time testing and additional financial cost as well.

  

It's also a bit sad because it seems like the overall printer _could_ work,
and there is some clever design work in there, but there are just so many
compromises in areas where it seems obvious (at least from my perspective)
that better choices could have been made that would make the Makibox easier to
build and more likely to print successfully without modification.  Perhaps
these choices are just the reality of delivering a printer for about $200, but
I'm not sure I believe that yet.

---
title: Tiny GENA update
date: Tue, 18 Aug 2015 22:50:07 -0500
author: jjg
draft: false
tags:
  - preposterous
---
The Mixtile folks posted a video of the GENA paired to an iPhone and that
encourages me enough to to start designing a case for it.

  

![image1.JPG](/preposterous/assets/54-image1.jpg)  

  

It's not much to _look_ at yet, but I'm iterating on a design.  Mixtile says
they will release 3D design files but I have no idea when, and it's more fun
to make your own anyway.

  
  
//  jjg

---
title: Tools
date: Wed, 10 Dec 2014 17:04:37 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I have no interest in tools that make better programs, only better
programmers.

  

I wanted to write more about this but it's all coming out wrong.

  

  

\- Jason

---
title: Traffic Violations
date: Sat, 22 Feb 2014 09:16:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
A common subject of discussion among programmers (and those who work with
them) is the need to preserve concentration and minimize interruption.  This
isn't unique to programmers, in fact anyone who's tried to read a book can
appreciate the frustration of someone walking up and talking to you while
you're deep in a world that only exists inside your head.

  

The default assumption is that people are jerks, and should learn to not
interrupt others while they are concentrating, but from the outside it's not
always easy to tell when someone is "in the zone" vs. more interruptible
activities.

  

Jamie and I were discussing the problem this morning and came up with the idea
of having a visual indicator that reflected the state of someone so that it
was unambiguous as to whether or not it was OK to interrupt them.  At her
office she had instituted a traffic light-style system of red light/green
light signs on office doors to communicate interruptability to other workers.
This worked somewhat, but since the only cost to "running the light" was paid
by the person who was trying to work, it wasn't always effective at deterring
others from interrupting.  Additionally, this relied on making sure the state
of the sign was updated by the person in the office, which isn't always an
option in modern open-design (or public) workspaces.

  

So we came up with two ideas to improve on this system.  The first is to use a
more portable and convenient indicator.  Since much of our time is spent on
mobile devices, the red light/green light indicator could be incorporated in a
case for the device, or even as an accessory (something that slips into the
headphone port, etc.).  The same sort of indicator could be applied to a
laptop or other device as well.

  

Secondly, just like a real traffic violation, institute a "ticket" system for
blowing a light.  The simplest implementation could be based on salary
(although any value system could be used).  Let's say an interruption takes 30
minutes to recover from.  Add this to the duration of the interruption and
multiply times the salary of the interrupted employee; the interruptor is then
fined this amount.

  

There are of course times when interrupting someone's concentration is
important enough to justify the cost, and by implementing a system that
associates a hard cost to these interruptions, the interruptor can make an
educated decision as to if what they need is important enough to incur the
expense.

  

There are of course cases where a system like this would be impractical, but
we believe that implementing a system like this in the workplace and in the
wider world could at least improve the situation and help preserve both the
productivity of knowledge workers as well as reduce animosity of those
(including their peers) who work with and around them.

  

  

  

\- Jason

---
title: Try Duck Duck Go
date: Sat, 22 Nov 2014 14:25:15 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
It's come a _long_ way, baby!  
  
[ https://duckduckgo.com ](https://duckduckgo.com)  
  
  
\- Jason  

---
title: Try multimedia
date: Fri, 13 Dec 2013 00:24:59 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Here's something new:

  

  

  

\--  
  
Jason J. Gullickson  
Producer  
[ jason.gullickson@gmail.com ](mailto:jason.gullickson@gmail.com)

[ ![](/preposterous/assets/11-jason2.jpg) ](assets/11-jason2.jpg)

---
title: Typical pic 2
date: Mon, 8 Dec 2014 00:22:34 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This time we'll force an **htmlId** email.  
  
![](/preposterous/assets/291-img_0598.jpg)

  
  
Sent from my iPhone

---
title: Typical pic 
date: Mon, 8 Dec 2014 00:21:24 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This is typical of the pictures I post via email. Hopefully it works right.
We'll see. Sent from my iPhone

[ ![](/preposterous/assets/290-img_0598.jpg) ](assets/290-img_0598.jpg)

---
title: Unhosted Twitter built on RemoteStorage
date: Thu, 06 Aug 2015 09:35:16 -0500
author: jjg
draft: false
tags:
  - preposterous
---
These are some notes on a Twitter-like microblogging service built on top of [
RemoteStorage ](https://remotestorage.io/) .  Besides duplicating Twitter
functionality on a distributed platform, this also allows the control and
ownership of content that RemoteStorage applications provide.

The first time someone uses it, the app (a simple HTML page + Javascript)
creates a directory under  /public  that stores status updates (  There may
even be an existing Category that is appropriate)  perhaps under
/public/status  .  Additionally, there is another, private, object stored that
holds a list of people you follow.

After initial setup the app polls all the  /public/status  directories of all
the people in the "following" list (perhaps within a certain timeframe) and
merges them into a timeline for display purposes.

Mentions could work just like Twitter by mentioning the full RS (webfinger?)
name of someone (ie jjg@5apps.com).

Discovery of other users is a bit trickier (following requires knowledge of
other users public address).  This may not necessarily be an issue though
since intrinsic discovery mechanisms (especially the semi-forced kind) are one
of the downsides of Twitter as far as I'm concerned.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Usenet and JSFS
date: Mon, 19 Oct 2015 09:04:54 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I was reading about [ Usenet ](https://en.wikipedia.org/wiki/Usenet) and
realized how much it has in common with [ JSFS ](https://github.com/jjg/jsfs)
.

I'm generally familiar with Usenet but I've never been a deliberate user of
it.  In it's heyday I was too disconnected to interact with it (it wasn't
carried by the BBS's I could dial-in to locally), and by the time that I had
regular, reliable Internet access it was the dawn of the Blog Era and I was
using web-based systems to meet similar needs.

For whatever reason I started digging into the implementation details of
Usenet and found some similarities to JSFS.  The use of hierarchical
namespaces to organize things and the idea of federating content by simply
"broadcasting" data from node-to-node in a peer group.  Also the idea that any
node in the network can access data stored on another node using a forwarding
mechanism, and then caching the results.

I didn't intentionally design JSFS to overlap with Usenet, and of course the
things they have in common are shared with other systems that did influence
JSFS's design, but what's curious is how features specific to JSFS could
address one of the key problems with Usenet: the storage requirements.

If think you could copy Usenet's hierarchy directly into a JSFS instance by
simply replacing "/" characters used in JSFS paths with "." used in Usenet
paths.  If you did this JSFS deduplication would kick-in and presumably reduce
the physical storage requirements for the archive.  Providing support for this
sort of "namespace translation" could be intrinsic in JSFS or some sort of
add-on module, but it's not a lot of work.  In fact JSFS uses a dot-oriented
naming scheme to establish it's own internal fully-qualified paths.

If implemented as a module, the module could also provide an [ NNTP
](https://en.wikipedia.org/wiki/Network_News_Transfer_Protocol) protocol
interface to the archive, essentially becoming an NNTP server that is simply
back-ended to a JSFS store.  Interestingly this would result in the creation
of a web-accessible archive (using the standard JSFS URL's that would result)
and when JSFS indexing is implemented, a searchable archive as well.

I don't have a specific point to make here, just that I find it interesting.
I find the idea that Usenet essentially proved that the federation mechanism
I've been considering for JSFS could actually work, and at a scale that dwarfs
any other similar network that I know of.  I also like the idea of configuring
federation not only by adding nodes to a peer group, but by offering the
possibility of selecting subsets of the storage by namespace, making access to
content via a peer fast for preferred bits, but still possible for namespaces
that are less of a priority to the node operator.

I'm also very interested in exploring "store-and-forward" architectures for
JSFS and distributed computing in general.  We've largely abandoned these
designs in light of the availability of "always-on" Internet, but we need to
remember that there is a cost to making the assumption that everyone can be
connected all the time.  Not just the financial cost of the connection, but
the social cost of cutting-off most of the world who do not have such access
to the Internet.

There is also an intellectual cost that prevents us from seeing and
implementing systems that can function over slow or intermittent connections.
While high-speed, fairly reliable Internet has become more common and less
expensive, systems that can function over slower and less reliable connections
have significantly more freedom, and the hardware necessary to implement them
is within reach for a much larger part of the world.  Even in wealthy
countries, monopolistic business practices mean that in many communities there
is only once choice for Internet access.  Network software architectures that
can work on lower-tech network hardware create opportunities for community-
oriented Internet access that can usurp these monopoly providers.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Various offGrID Updates
date: Thu, 10 Sep 2015 08:13:44 -0500
author: jjg
draft: false
tags:
  - preposterous
---
A [ friend ](http://n-o-d-e.net/) asked me how [ OffGRiD
](https://github.com/jjg/offgrid) is coming along; here's where things are at:

I've designed a lot of the case and started printing off some parts to see how
they fit together and get a feel for the real-world size of the thing.

![assembly_only.png](/preposterous/assets/71-assembly_only.png)

I'm pretty happy with the way things fit and I'm honestly surprised that
things matched-up when I laid them out together with placeholders for the off-
the-shelf parts.

At this point I think I'm running out of things I can do without getting my
hands on the components, especially the solar panel and display.  I've been
working from the manufacturer's specs, but they don't tell me everything I
need (or are completely missing for some of the parts).  It's also hard for me
to really get a feel for the stuff without having it on-hand, and stuff like
cable routing, etc. is hard (at least for me) to model in CAD.

The big barrier to getting the parts is the cost.  All-in it's about $500.00
in components and I'd like to have some spares/alternatives just in case.  I'm
considering doing a little crowfunding campaign to raise some R&D funds but I
don't want to get distracted from the project itself to run the campaign, so
I'm still looking for alternatives.

Longer-term something I've been noodling on is a way to help people who are
interested in a computer like OffGRiD get their hands on one.  I don't want to
get into mass-producing them (it goes against some of the design goals to do
so) but I did think of a way to make it more accessible for a broader range of
people.

Once I have a working prototype or two, I'm thinking about creating a website
where you can build-our your own personalized OffGRiD-style computer by
selecting the off-the-shelf components (display, processor, battery, etc.) and
have the site generate a Bill of Materials (BOM) for the off-the-shelf parts
and then a collection of custom printable files for the case, keyboard, etc.

From there you can download the BOM & printables and source them yourself, or
place an order for the components directly on the website (sourced from
companies we like to support) and also send the printable files to a 3D
printing company like 3D Hubs, Shapeways, etc.

This leaves assembly to the user, which is OK but I'd also like to partner
with regional maker/hackerspaces so a person has options on finding local
help, or setting up "build nights", etc.

It's a fairly ambitious project in it's own right, but I don't think too much
so.  It's something I don't want to dive into until I know that I can actually
design a machine that works (and is pleasant to work with) so the prototypes
come first, but I think it's good to have an idea of what's next for a
project, especially one that requires a substantial up-front investment in
time & resources.

\--  

// jjg

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: Video from iOS test
date: Fri, 20 Dec 2013 17:05:07 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Testing sending video directly from iOS. \- Jason

---
title: Video Test 1
date: Sat, 14 Dec 2013 13:37:39 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
First test of video embedding support.

  

  

---
title: Video test 2
date: Sat, 14 Dec 2013 13:39:16 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Second test of posting video

---
title: Voicebox
date: Sun, 23 Feb 2014 12:44:02 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
This morning I was cleaning up the workbench and decided to finish up a little
(so far) meaningless project that was taking up some space.  
  
I found this awesome old speaker (I think it was intended for **HAM** radio)
at Goodwill a few years back and knew I'd have some use for it eventually.  
  

![](/preposterous/assets/213-image.jpeg)

  
  
A few months back I ran across a Speakjet chip ( [
https://www.sparkfun.com/products/9578
](https://www.sparkfun.com/products/9578) ) I had picked up somewhere years
ago.  I've experimented with the chip but it was always a hassle to work with
because it needs an amplifier and a speaker, etc.  I figured I should do
something about that so I dug up an LM386 audio amp chip and tied them
together on a breadboard.

  

![](/preposterous/assets/213-image.jpeg)

  
  
I played with that a bit, but got frustrated trying to generate phonemes by
hand and let it all just sit on the bench for another month or so.  
  
Today I decided to clean things up and stuffed the whole breadboard inside the
speaker.  I hesitated a bit because I didn't want to cut-up the nice cord on
the speaker but in the end decides that this thing had been left unfinished
long enough.  
  
I was able to save the cable grommet and re-use it to protect the USB cable
attached to the Arduino inside. The result is a nice clean job, you'd hardly
believe it was my handiwork.  
  
I still need to find an easier way to generate speech codes for the chip, but
at least it all fits nicely on top of my workstation now and doesn't take up
space on the benchtop.  
  

![](/preposterous/assets/213-image.jpeg)

  
  
  
\- Jason

---
title: Volume down
date: Mon, 8 Dec 2014 17:52:23 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/294-img_0610.jpg) ](assets/294-img_0610.jpg)

---
title: Volume up
date: Mon, 8 Dec 2014 17:52:52 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/295-img_0611.jpg) ](assets/295-img_0611.jpg)

---
title: Webos Phone Test 2
date: Tue, 17 Dec 2013 09:16:35 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Posts from my phone don't seem to be making it through, so this is a test from
gmail to make sure things are still working period :)

---
title: We're building a new kind of Internet
date: Wed, 4 Jun 2014 08:29:02 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
I think it's hard to understand the logic behind the systems I've been
designing lately if you don't understand that _we're trying to build a new
kind of Internet_ .

  

To be sure, it's not completely new, at least not yet.  We're still relying on
the underlying TCP/IP network protocol (at least for now), but even there
we're focused on IPv6 and not letting the baggage of IPv4 hold back anything
interesting.  Above the protocol layer though, things are much different from
how most people currently understand the Internet, and in particular, the Web.

  

To most people the Web _is_ the Internet.  I'm frequently frustrated by this
but I understand why it's the case.  It's not unlike how email was once known
to millions as "AOL", and how, if we allow the trend to continue, most people
will start calling Facebook the "internet".

  

Given this it's easy to see why even programmers minds are a bit melted when
they try to consume the type of services and architectures that we're
proposing, because they're designed for a future network that doesn't
completely exist today.  We're designing for a network that in some ways looks
a lot like the ancient Internet, even before the existence of Internet Service
Providers (ISP's).  Our vision is a worldwide network of independent systems
which can cooperate voluntarily or operate completely on their own, capable of
seamless and dynamic resource sharing and unencumbered by non-technical
baggage.  A network accessible by every intelligence on Earth equally, without
financial or intellectual tariffs and surcharges, and without gatekeepers or
profiteers simply operating turnstiles that exist only to extract value from
users and contribute no actual processing capacity.

  

This might sound impossible or idealistic, perhaps utopian (unless you make a
living shaking down Internet users, in which case perhaps "nightmarish" comes
to mind) and to be fair it's not a small undertaking, but it breaks down into
smaller parts somewhat nicely, some which we don't yet have the technology or
influence necessary, but others which we can start building _right now_ .

  

One of the steps we can take today is to stop building systems incapable of
existing in such a model.  In some ways this means systems become more
independent, capable of stand-alone operation, but in other ways it means that
systems become more open, capable of integration with known and unknown
systems in seamless and dynamic ways.  In our current view of what an
"application" is, this seems paradoxical, but part of the new way is
increasing the granularity of applications, not unlike previous efforts in
separating data from logic and display in the Model-View-Controller
application architecture, or the emergence of REST/Web API's.

  

However this division not only goes the direction of application _code_ , but
also in _data_ as well.  By using data structures that represent information
completely, independent of relationship to other documents, we can eliminate
dependencies that constrain existing applications to "always online"
operation, and lay the foundation for networks that can change much more
dramatically without disrupting continuity.  This is a significant change from
the silo design currently popular for Internet applications, and even the idea
of servers themselves.

  

I'll elaborate more in a later post.  In the meantime if you're working with
us on a project, and something we've designed seems weird or over complicated,
keep in mind that we're not designing a new application, but a new kind of
network as well.

  

  

\- Jason

---
title: What is next
date: Fri, 13 Dec 2013 23:13:43 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/39-jason1.jpg) ](assets/39-jason1.jpg)

---
title: What's Next
date: Fri, 13 Dec 2013 23:08:58 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
[ ![](/preposterous/assets/38-jason1.jpg) ](assets/38-jason1.jpg)

---
title: What's =?UTF-8?Q?Next=3F?=
date: Sat, 01 Aug 2015 20:22:21 -0500
author: jjg
draft: false
tags:
  - preposterous
---
I'm not sure what the future holds for Preposter.us.

It's awesome, don't get me wrong, and personally, I love it.  I had thoughts
of trying to do something big with it, make it self-sustaining and possibly
underwriting upcoming projects, but I don't know, I don't think the time is
right.

It's not that I think it ever will be, we may just be past the point where a
simple end elegant blog isn't a thing.  This sounds dumb to most people I'm
sure, clearly blogs are far out of fashion but you have to remember that I was
"blogging" before there was a word for it, so to me Preposter.us is called a
blog only out of convenience.  To me it's just another journaling system, one
in a long line of systems I've designed for my own use in the past, and
certainly not the last.

On the flip side there's essentially no cost to keeping it going, at least not
at any predictable scale.  The software is tight and reliable, and by design
uses very little resources.  I only maintain the code when I want a new
feature (or get frustrated with a bug), but without a demanding audience it's
very little effort to keep development at pace.

It would be encouraging if it was valuable to others, but when you think about
it pursuing that sort of satisfaction is kind of dumb.  If cost isn't a factor
(both financial and otherwise) then why should I care if I'm the only one
using it?  Maybe there is something subconscious that nags me when it seems
like I'm only doing this to indulge myself.

Of course every time I bother to sit down and use it, I start to remember why
I wrote Presposter.us in the first place.  The simplicity of producing a
journal via email is hard to beat, especially since doing so is one of the few
ways left to function outside of the "mainframification" of the "web".  Maybe
I just need to eat my own dogfood more to remember why I cooked it in the
first place?

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

---
title: When the heat is on...
date: Mon, 16 Dec 2013 12:37:24 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
...I tend to think a lot about heating.  A few weeks back I was thinking about
networking space heaters to make them capable of out-performing central
heating systems (or at least work more cooperatively with them) and today I
ran across a post on [ Hackaday ](http://hackaday.com) that had an interesting
take on the same line of thinking: [ heat people not buildings
](http://hackaday.com/2013/12/16/a-new-way-to-heat-people/) .

  

Once I saw this the thought occurred to me: could [ beamforming
](http://en.wikipedia.org/wiki/Beamforming) be used to improve upon this
design?  If you're not familiar with beamforming it's pretty fascinating
stuff, the first I heard of it was from Mike White describing how it's using
in wireless access points to improve reception without violating signal
strength limits.

  

I haven't thought through the physics of it yet, so there may be some
fundamental problem with the approach, but if I'm right a beamforming
technique could address a number of concerns that have been raised about
Leigh's current design.  A beamforming transmitter shouldn't require the
physical apparatus that is required by Leigh's system (in fact, it might not
require any moving parts at all).

  

Without moving parts, it may be able to overcome another concern with Leigh's
design, that is how do you use this technique to provide heat for multiple
people or objects.  Since a non-mechanical beamforming device can change
"focus" orders of magnitude faster than a mechanical armature, heating
multiple targets simply means moving quickly between them.  It's also possible
that simultaneously focusing on multiple targets is possible, but I'd have to
think that through some more (it might still be round-robin, but so fast that
it seems instantaneous).

  

The other area in which I'd improve this design is by using a simpler motion
capture system.  Watching the demo, I wondered how the Kinect functioned with
all the infrared being pumped out of the heating unit.  This made me realize
that the targets for this system are (or should be) warmer than their
surroundings, and furthermore their IR signature could be used to measure
their local temperature.  With these two bits of information it should be
possible to design a tracking system based on measuring radiation instead of
movement that would likely be simpler, less expensive and just as (if not more
than) effective as the Kinect motion capture system.

  

This doesn't itself allow for personal selection of temperature per se
(although I don't know that the Kinect-only system would allow for this either
without some sort of visual cue), but preferred local heating could be
accomplished using some sort of device or fob held by the user, which would
eliminate the need for the visual/radiant targeting system altogether (the
signal from the device could be used, in this case it could use existing
beamforming data used to target the device's wifi radio :)

  

  

\- Jason

---
title: Why I let GullicksonLaboratories.com die
date: Sat, 14 Jun 2014 23:15:23 -0500
author: jason.gullickson
draft: false
tags:
  - preposterous
---
Tomorrow the domain [ gullicksonlaboratories.com
](http://gullicksonlaboratories.com) will expire, and the online home of my
projects and research since 2009 will go dark...for awhile.

  

I'm letting the domain expire as a way to motivate my work on distributed and
decentralized networking.  Name resolution, and the domain name registration
system is a prime example of the kind of artificial scarcity whose elimination
is at the heart of my most important projects.  By allowing [
gullicksonlaboratories.com ](http://gullicksonlaboratories.com) to expire, I'm
directly affected by my procrastination in finding and promoting alternatives,
which should provide the additional motivation necessary to get the job done.

  

In the meantime I'll be slowly redirecting links that point to [
gullicksonlaboratories.com ](http://gullicksonlaboratories.com) to this blog,
and I'll be resurrecting parts of the old site here that may be of interest to
others as I work on turning the lights back on again.

  

  

  

\- Jason

---
title: Why they call it Internet Exploder
date: Tue, 28 Jan 2014 09:07:54 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
![](/preposterous/assets/202-screen shot 2014-01-28 at 9.03.35 am.png)

  

![](/preposterous/assets/202-screen shot 2014-01-28 at 9.06.19 am.png)

---
title: Will slug test 6 work?
date: Fri, 13 Dec 2013 23:47:11 -0600
author: jason.gullickson
draft: false
tags:
  - preposterous
---
If this goes through I think we'll call it fixed for now.

---
title: Wrist Reader for Pebble
date: Sat, 07 Mar 2015 12:23:15 -0600
author: jjg
draft: false
tags:
  - preposterous
---
WristReader is a Pebble app version of an idea for a minimalist e-reader that
came out of some research I had read on how constrained reading devices can
make reading _easier_ for people who struggle with reading and a number of
e-reader user interface conversations Preston  & I have had.

![wristreader_title.jpg](/preposterous/assets/32-wristreader_title.jpg)

The original idea was to design a device that displayed one line of text at a
time instead of an entire page, the principle being that you can only read one
line at a time*, so why carry around something the size of a page?  I even
went as far as to begin designing a piece of hardware along these lines called
SlimReader ( [ https://github.com/jjg/slimreader
](https://github.com/jjg/slimreader) ), but due to the limitations of off-the-
shelf parts, it was a bit of a compromise and ended up being less convenient
that desired.

When I started wearing a [ Pebble watch ](https://getpebble.com/#/AToOEW:H7t)
the topic came up again and it seemed like the Pebble had all the ingredients
necessary for the reader, so I took a whack at writing it as a Pebble app.

It didn't take long to realize that fitting an entire line of text on the
Pebble screen was out of the question, so instead I broke it down to one word
at a time.  This was a frustrating limitation but after thinking about it the
same logic applies to one word as does to one sentence (you only read one at a
time) so I ran with it.

As it turns out one word at a time might even be better.  After loading up
some test text and using the reader as designed I noticed that with a little
tuning of the display speed the story starts to play in your head almost like
listening to an audiobook.  I added manual control to adjust the display speed
while reading and with that I was able to make it through an entire chapter of
the book without strain and with comprehension.

I added a couple other details for demonstration purposes (showing the title
screen when paused, indicating progress, etc.), just enough to get a feel for
what it feels like to use it, but stopped there to review some of the ideas
and questions that came to mind once it worked.

(For the curious and ambitious, the work-in-progress code is available on
Github here: [ https://github.com/jjg/wrist-reader ](https://github.com/jjg
/wrist-reader) )

There's a couple of routes to go forward from here.  I could develop
WristReader as a stand-alone e-reader application, capable of being loaded
with text from various sources, or I could develop the app as a sort of
"container" that could be used to release books as independent, stand-alone
apps (perhaps with unique functionality).  I think that the platform may be
too constrained to create a general-purpose reader (especially in terms of
managing a library, etc.) but on the other hand those chores could be
"outsourced" to more robust ebook management tools, which use WristReader as
an output device (like iTunes and an old-school iPod).

It's important to me that the book be self-contained, not dependent on
connectivity to a phone or the Internet for reading.  The convinced of having
a book on your wrist coupled with the Pebbles excellent battery life would
make it a great companion for getting far away from it all and reading as long
as these dependencies are avoided.

What do you think about a water-resistant ebook you can always have with you?
Reading a book this way might seem awkward, I felt that way myself while I was
developing it, but it's really something you have to experience to judge.  I'd
love to get some feedback on the idea to shape the direction it goes or to
decide if it warrants further development.  Free free to comment or message me
on the medium of your choice via the links below

*There are speed-reading techniques that involve reading a page-at-a-time. 

\--  

\- Jason

[ Preposter.us ](http://jjg.preposter.us/) | [ Github
](https://github.com/jjg) | [ Twitter ](https://twitter.com/jasonbot2000) | [
Ello ](https://ello.co/jasonbot) | [ Google+
](https://plus.google.com/u/0/+JasonGullickson/posts) | [ Facebook
](https://www.facebook.com/jasonjgullickson)

